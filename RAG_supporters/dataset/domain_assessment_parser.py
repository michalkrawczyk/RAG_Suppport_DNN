"""CSV parser for domain assessment data.

Parses CSV files generated by domain_assessment.py agent with:
- source_text, question_text, chroma_ids
- suggestions (JSON formatted list of DomainSuggestion objects)
- cluster_probabilities (JSON formatted probability vector)
"""

import json
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, TypedDict

import pandas as pd

logger = logging.getLogger(__name__)


class DomainAssessmentRecord(TypedDict):
    """TypedDict representing a single record from domain assessment CSV."""

    source_text: str
    question_text: str
    chroma_source_id: str
    chroma_question_id: str
    suggestions: List[Dict[str, Any]]  # List of DomainSuggestion dicts
    cluster_probabilities: Optional[List[float]]


def create_domain_assessment_record(row: pd.Series) -> DomainAssessmentRecord:
    """
    Create DomainAssessmentRecord from a pandas Series (CSV row).
    
    Args:
        row: Pandas Series from CSV
        
    Returns:
        DomainAssessmentRecord dictionary
        
    Raises:
        ValueError: If required fields missing or invalid format
    """
    # Required fields with aliases
    source_text = row.get("source_text") or row.get("source", "")
    question_text = row.get("question_text") or row.get("question", "")
    chroma_source_id = row.get("chroma_source_id") or row.get("source_id", "")
    chroma_question_id = row.get("chroma_question_id") or row.get("question_id", "")

    # Validate required fields
    if not source_text or not question_text:
        raise ValueError("Missing source_text or question_text")
    
    if not chroma_source_id or not chroma_question_id:
        raise ValueError("Missing chroma_source_id or chroma_question_id")

    # Parse suggestions (JSON formatted)
    suggestions_str = row.get("suggestions", "[]")
    try:
        suggestions = json.loads(suggestions_str) if isinstance(suggestions_str, str) else []
    except json.JSONDecodeError as e:
        logger.warning(f"Failed to parse suggestions: {e}")
        suggestions = []

    # Parse cluster probabilities (JSON formatted)
    cluster_probabilities: Optional[List[float]] = None
    cluster_probs_str = row.get("cluster_probabilities")
    if cluster_probs_str is not None and isinstance(cluster_probs_str, str):
        try:
            cluster_probabilities = json.loads(cluster_probs_str)
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse cluster_probabilities: {e}")

    # Return plain dict, typed as DomainAssessmentRecord
    return {
        "source_text": source_text,
        "question_text": question_text,
        "chroma_source_id": chroma_source_id,
        "chroma_question_id": chroma_question_id,
        "suggestions": suggestions,
        "cluster_probabilities": cluster_probabilities,
    }


class DomainAssessmentParser:
    """
    Parser for domain assessment CSV files.

    Handles:
    - Multiple CSV files (concatenates and deduplicates)
    - Chunked reading for large files
    - Validation of required fields
    """

    def __init__(self, chunksize: Optional[int] = None):
        """
        Initialize parser.

        Args:
            chunksize: If set, reads CSV in chunks to reduce memory usage
        """
        self.chunksize = chunksize

    def parse_csv(self, csv_path: Union[str, Path]) -> List[DomainAssessmentRecord]:
        """
        Parse single CSV file.

        Args:
            csv_path: Path to CSV file

        Returns:
            List of parsed records

        Raises:
            FileNotFoundError: If CSV not found
            ValueError: If CSV invalid or missing required fields
        """
        csv_path = Path(csv_path)
        if not csv_path.exists():
            raise FileNotFoundError(f"CSV file not found: {csv_path}")

        logger.info(f"Parsing CSV: {csv_path}")
        records = []

        if self.chunksize:
            # Chunked reading for large files
            chunks = pd.read_csv(csv_path, chunksize=self.chunksize)
            for i, chunk in enumerate(chunks):
                logger.info(f"Processing chunk {i+1} ({len(chunk)} rows)")
                records.extend(self._parse_dataframe(chunk))
        else:
            # Read entire file
            df = pd.read_csv(csv_path)
            records = self._parse_dataframe(df)

        logger.info(f"Parsed {len(records)} records from {csv_path}")
        return records

    def parse_multiple_csvs(
        self, csv_paths: List[Union[str, Path]], deduplicate: bool = True
    ) -> List[DomainAssessmentRecord]:
        """
        Parse multiple CSV files and optionally deduplicate.

        Args:
            csv_paths: List of CSV file paths
            deduplicate: If True, removes duplicate records based on chroma IDs

        Returns:
            Combined list of parsed records
        """
        all_records = []

        for csv_path in csv_paths:
            try:
                records = self.parse_csv(csv_path)
                all_records.extend(records)
            except Exception as e:
                logger.error(f"Failed to parse {csv_path}: {e}")
                raise

        if deduplicate:
            all_records = self._deduplicate_records(all_records)

        logger.info(
            f"Total records after processing {len(csv_paths)} CSV files: {len(all_records)}"
        )
        return all_records

    def _parse_dataframe(self, df: pd.DataFrame) -> List[DomainAssessmentRecord]:
        """
        Parse DataFrame into records.

        Args:
            df: Pandas DataFrame

        Returns:
            List of parsed records
        """
        records = []

        for idx, row in df.iterrows():
            try:
                record = create_domain_assessment_record(row)
                records.append(record)
            except ValueError as e:
                logger.warning(f"Skipping row {idx}: {e}")
            except Exception as e:
                logger.error(f"Unexpected error parsing row {idx}: {e}")

        return records

    def _deduplicate_records(
        self, records: List[DomainAssessmentRecord]
    ) -> List[DomainAssessmentRecord]:
        """
        Remove duplicate records based on chroma IDs.

        Args:
            records: List of records

        Returns:
            Deduplicated list
        """
        seen_pairs = set()
        unique_records = []

        for record in records:
            pair_id = (record["chroma_source_id"], record["chroma_question_id"])
            if pair_id not in seen_pairs:
                seen_pairs.add(pair_id)
                unique_records.append(record)

        removed = len(records) - len(unique_records)
        if removed > 0:
            logger.info(f"Removed {removed} duplicate records")

        return unique_records
