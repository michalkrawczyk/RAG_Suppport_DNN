[
  {
    "path": "RAG_supporters/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "RAG_supporters",
    "package": "RAG_supporters",
    "module_docstring": "RAG Supporters - Tools and agents for RAG dataset creation, curation, and enhancement.\n\nThis package provides specialized LLM-powered agents and utilities for working with\nRetrieval-Augmented Generation (RAG) datasets, including:\n\n- Agents: Question augmentation, text augmentation, dataset checking, domain analysis,\n  and source evaluation\n- Clustering: Keyword clustering and topic distance calculation\n- Dataset utilities: Splitting, labeling, and storage\n- Embeddings: Embedding generation and I/O",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/agents/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "agents",
    "package": "RAG_supporters",
    "module_docstring": "Agents package for LLM-powered dataset operations.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/agents/dataset_check.py",
    "file": "dataset_check.py",
    "module": "dataset_check",
    "parent_module": "agents",
    "package": "RAG_supporters",
    "module_docstring": "Dataset checking agent for comparing text sources.",
    "classes": {
      "CheckAgentState": {
        "docstring": "State object for the agent.\n\nAttributes\n----------\nmessages : List[BaseMessage]\n    List of messages exchanged during the checking process.\nquestion : str\n    The question being analyzed.\nsource1 : str\n    First source text to compare.\nsource2 : str\n    Second source text to compare.\nanalysis : str\n    Analysis result from source comparison.\nfinal_choice : int\n    Final choice label (0 for source1, 1 for source2, -1 for error).",
        "methods": {}
      },
      "DatasetCheckAgent": {
        "docstring": "Agent for checking and comparing text sources in datasets.\n\nThis agent uses LLM-based analysis to compare two text sources for a given\nquestion and determine which source is better or if they are duplicates.\n\nParameters\n----------\nllm : BaseChatModel\n    Language model instance for performing text analysis.\n\nAttributes\n----------\n_llm : BaseChatModel\n    The language model used for analysis.\n_executor : object\n    Compiled workflow executor for the checking process.",
        "methods": {
          "__init__": "Initialize the DatasetCheckAgent.",
          "_build_graph": "Build the workflow graph for source checking process.",
          "compare_text_sources": "Compare two text sources for a given question.\n\nParameters\n----------\nquestion : str\n    The question to analyze the sources against.\nsource1 : str\n    First source text to compare.\nsource2 : str\n    Second source text to compare.\nreturn_analysis : bool, optional\n    Whether to return the analysis text, by default False.\nreturn_messages : bool, optional\n    Whether to return the message history, by default False.\n\nReturns\n-------\ndict\n    Dictionary containing:\n    - 'label': int (0 for source1, 1 for source2, -1 for error)\n    - 'analysis': str or None (if return_analysis=True)\n    - 'messages': list or empty list (if return_messages=True)",
          "process_dataframe": "Process the dataframe to check for duplicates and other issues.\n\nParameters\n----------\ndf : pandas.DataFrame\n    DataFrame containing columns: 'question_text', 'answer_text_1',\n    'answer_text_2', and optionally 'label'.\nsave_path : str, optional\n    Path to save the processed DataFrame as CSV, by default None.\nskip_labeled : bool, optional\n    Whether to skip rows that already have labels (!= -1), by default True.\nstart_index : int, optional\n    Index to start processing from, by default 0.\n\nReturns\n-------\npandas.DataFrame\n    DataFrame with updated 'label' column containing comparison results.",
          "process_csv": "Perform dataset check on a CSV file (Overwriting the file).\n\nParameters\n----------\ncsv_path : str\n    Path to the CSV file to process. File will be overwritten with results.\nskip_labeled : bool, optional\n    Whether to skip rows that already have labels (!= -1), by default True.\nstart_index : int, optional\n    Index to start processing from, by default 0."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/agents/domain_assesment.py",
    "file": "domain_assesment.py",
    "module": "domain_assesment",
    "parent_module": "agents",
    "package": "RAG_supporters",
    "module_docstring": "Domain analysis agent for domain extraction, guessing, and assessment tasks.\n\nAgent for domain extraction, guessing, and assessment tasks.",
    "classes": {
      "OperationMode": {
        "docstring": "Operation modes for domain analysis.",
        "methods": {}
      },
      "DomainSuggestion": {
        "docstring": "Model for a single domain/subdomain/keyword suggestion.",
        "methods": {}
      },
      "DomainExtractionResult": {
        "docstring": "Result for source domain extraction.",
        "methods": {
          "validate_suggestions_length": "Ensure total_suggestions matches the length of suggestions list."
        }
      },
      "DomainGuessResult": {
        "docstring": "Result for question domain guessing.",
        "methods": {
          "validate_suggestions_length": "Ensure total_suggestions matches the length of suggestions list."
        }
      },
      "SelectedTerm": {
        "docstring": "Model for a selected term with relevance score.",
        "methods": {}
      },
      "DomainAssessmentResult": {
        "docstring": "Result for domain assessment against available terms.",
        "methods": {
          "validate_total_selected_length": "Ensure total_selected matches the length of selected_terms list."
        }
      },
      "TopicRelevanceScore": {
        "docstring": "Model for a single topic descriptor with relevance probability.\n\nNote: Typically used with cluster descriptors from KeywordClusterer output.",
        "methods": {}
      },
      "QuestionTopicRelevanceResult": {
        "docstring": "Result for question-topic relevance assessment.\n\nNote: Designed to work with topic descriptors from KeywordClusterer\n(RAG_supporters/clustering/keyword_clustering.py). When using KeywordClusterer,\nthe topic_descriptors represent the descriptors of the created clusters.",
        "methods": {
          "validate_topic_scores_length": "Ensure total_topics matches the length of topic_scores list."
        }
      },
      "AgentState": {
        "docstring": "State for the LangGraph domain analysis agent.",
        "methods": {}
      },
      "DomainAnalysisAgent": {
        "docstring": "Unified LangGraph agent for domain analysis tasks.\n\n- Extract domains from text sources\n- Guess domains needed to answer questions\n- Assess questions against available domain terms",
        "methods": {
          "__init__": "Initialize the domain analysis agent.\n\nParameters\n----------\nllm : BaseChatModel\n    Language model to use for analysis\nmax_retries : int, optional\n    Maximum number of retries for parsing. Default is 3.\nbatch_size : int, optional\n    Default batch size for batch processing. Default is 10.\ninclude_reason : bool, optional\n    If True, include reasoning explanations in topic relevance assessments.\n    (For now) Only applies to assess_topic_relevance_prob operations (TOPIC_RELEVANCE_PROB mode).\n    Default is False.",
          "_check_openai_llm": "Check if the LLM is from OpenAI for batch processing.",
          "_create_prompt_template": "Create a prompt template with format instructions.",
          "_get_parser_for_mode": "Get the appropriate parser and fixing parser for the operation mode.",
          "_get_template_for_mode": "Get the appropriate prompt template for the operation mode.",
          "_get_column_prefix": "Get column prefix for mode to avoid overwrites.",
          "_build_graph": "Build the LangGraph workflow.",
          "_analyze": "Analyze based on the operation mode.",
          "_validate_response": "Validate the response.",
          "_should_retry": "Determine if we should retry or end.",
          "_handle_retry": "Handle retry logic.",
          "_extract_result_dict": "Extract result as dictionary.\n\nParameters\n----------\nresult : Any\n    Result object to extract\n\nReturns\n-------\nOptional[Dict[str, Any]]\n    Result as dictionary or None",
          "_create_relevance_json_mapping": "Create JSON mapping of topic descriptors to probabilities.\n\nParameters\n----------\ntopic_scores : List[Dict[str, Any]]\n    List of topic score dictionaries\n\nReturns\n-------\nstr\n    JSON string mapping topic_descriptor to probability",
          "_parse_topic_descriptors": "Parse and extract topic descriptors from various input formats.\n\nHandles:\n- List of strings: [\"topic1\", \"topic2\"]\n- JSON string: '[\"topic1\", \"topic2\"]'\n- File path: \"path/to/clusters.json\"\n- KeywordClusterer dict: {\"cluster_stats\": {\"0\": {\"topic_descriptors\": [...]}, ...}}\n\nParameters\n----------\ntopic_descriptors : Union[List[str], List[Dict], str, Dict]\n    Topic descriptors in any supported format\n\nReturns\n-------\nList[str]\n    Flat list of topic descriptor strings\n\nRaises\n------\nValueError\n    If the input format is invalid or file not found",
          "extract_domains": "Extract domains, subdomains, and keywords from text source.\n\nParameters\n----------\ntext_source : str\n    The text to analyze for domain extraction\n\nReturns\n-------\nOptional[Dict[str, Any]]\n    Dictionary with suggestions, total_suggestions, and primary_theme\n\nExamples\n--------\n>>> result = agent.extract_domains(\"Machine learning is a subset of AI...\")\n>>> print(result['primary_theme'])\n'Artificial Intelligence'",
          "guess_domains": "Guess which domains are needed to answer a question.\n\nParameters\n----------\nquestion : str\n    The question to analyze\n\nReturns\n-------\nOptional[Dict[str, Any]]\n    Dictionary with suggestions, total_suggestions, and question_category\n\nExamples\n--------\n>>> result = agent.guess_domains(\"What is the capital of France?\")\n>>> print(result['question_category'])\n'Geography'",
          "assess_domains": "Assess which available terms are most relevant to a question.\n\nParameters\n----------\nquestion : str\n    The question to analyze\navailable_terms : Union[List[str], List[Dict], str]\n    List of available terms (as strings, dicts, or JSON string)\n\nReturns\n-------\nOptional[Dict[str, Any]]\n    Dictionary with selected_terms, total_selected, question_intent, and primary_topics\n\nExamples\n--------\n>>> terms = [\"physics\", \"chemistry\", \"biology\"]\n>>> result = agent.assess_domains(\"What is photosynthesis?\", terms)\n>>> print(result['primary_topics'])\n['biology']",
          "assess_topic_relevance_prob": "Assess relevance probabilities between a question and topic descriptors.\n\nThis method is designed to work with cluster descriptors from KeywordClusterer\n(RAG_supporters/clustering/keyword_clustering.py). It automatically handles:\n- List of strings: [\"topic1\", \"topic2\"]\n- JSON string: '[\"topic1\", \"topic2\"]'\n- File path to JSON: \"path/to/clusters.json\"\n- KeywordClusterer dict format: {\"cluster_stats\": {\"0\": {\"topic_descriptors\": [...]}, ...}}\n\nNote: Topic descriptors are typically shared across all questions for\nperformance and memory efficiency.\n\nParameters\n----------\nquestion : str\n    The question to analyze\ntopic_descriptors : Union[List[str], List[Dict], str, Dict]\n    Topic descriptors in any supported format:\n    - List of strings (direct input)\n    - JSON string (will be parsed)\n    - File path to JSON file (will be loaded and parsed)\n    - KeywordClusterer dict with 'cluster_stats' containing 'topic_descriptors'\n\nReturns\n-------\nOptional[Dict[str, Any]]\n    Dictionary with topic_scores, total_topics, and question_summary (optional).\n    Each topic_score contains topic_descriptor, probability, and optionally\n    reason (if the agent was initialized with ``include_reason=True``).\n\nExamples\n--------\n>>> # Example 1: List of strings\n>>> descriptors = [\"machine learning\", \"databases\", \"web development\"]\n>>> result = agent.assess_topic_relevance_prob(\n...     \"What is gradient descent?\",\n...     descriptors\n... )\n\n>>> # Example 2: File path\n>>> result = agent.assess_topic_relevance_prob(\n...     \"What is gradient descent?\",\n...     \"path/to/keyword_clusters.json\"\n... )\n\n>>> # Example 3: KeywordClusterer dict\n>>> clustering_data = {\n...     \"cluster_stats\": {\n...         \"0\": {\"topic_descriptors\": [\"machine learning\", \"AI\"], \"size\": 50},\n...         \"1\": {\"topic_descriptors\": [\"databases\", \"SQL\"], \"size\": 30}\n...     }\n... }\n>>> result = agent.assess_topic_relevance_prob(\n...     \"What is gradient descent?\",\n...     clustering_data\n... )",
          "extract_domains_batch": "Extract domains from multiple text sources in batch.\n\nParameters\n----------\ntext_sources : List[str]\n    List of text sources to analyze\nbatch_size : int, optional\n    Batch size for chunking. If None, uses self.batch_size\nshow_progress : bool, optional\n    Show progress bar. Default is True.\n\nReturns\n-------\nList[Optional[Dict[str, Any]]]\n    List of extraction results",
          "guess_domains_batch": "Guess domains for multiple questions in batch.\n\nParameters\n----------\nquestions : List[str]\n    List of questions to analyze\nbatch_size : int, optional\n    Batch size for chunking. If None, uses self.batch_size\nshow_progress : bool, optional\n    Show progress bar. Default is True.\n\nReturns\n-------\nList[Optional[Dict[str, Any]]]\n    List of guess results",
          "assess_domains_batch": "Assess domains for multiple questions against the same available terms.\n\nParameters\n----------\nquestions : List[str]\n    List of questions to analyze\navailable_terms : Union[List[str], List[Dict], str]\n    Available terms (same for all questions)\nbatch_size : int, optional\n    Batch size for chunking. If None, uses self.batch_size\nshow_progress : bool, optional\n    Show progress bar. Default is True.\n\nReturns\n-------\nList[Optional[Dict[str, Any]]]\n    List of assessment results",
          "assess_topic_relevance_prob_batch": "Assess topic relevance for multiple questions against the same topic descriptors.\n\nPerformance Note: Topic descriptors are shared across all questions for memory\nefficiency. This is the typical use case, as topic descriptors from\nKeywordClusterer are usually consistent across a dataset.\n\nAutomatically handles:\n- List of strings\n- JSON string\n- File path to JSON\n- KeywordClusterer dict with cluster_stats containing topic_descriptors\n\nParameters\n----------\nquestions : List[str]\n    List of questions to analyze\ntopic_descriptors : Union[List[str], List[Dict], str, Dict]\n    Topic descriptors (same for all questions). Supports:\n    - List of strings\n    - JSON string\n    - File path to KeywordClusterer JSON\n    - KeywordClusterer dict with 'cluster_stats' containing 'topic_descriptors'\nbatch_size : int, optional\n    Batch size for chunking. If None, uses self.batch_size\nshow_progress : bool, optional\n    Show progress bar. Default is True.\n\nReturns\n-------\nList[Optional[Dict[str, Any]]]\n    List of topic relevance results",
          "_batch_process": "Process inputs in batches for efficiency.\n\nThis method properly splits input into chunks and processes each chunk\nseparately using the LLM's batch API.\n\nHandles KeyboardInterrupt gracefully by returning partial results.\n\nParameters\n----------\nmode : OperationMode\n    Operation mode (extract, guess, assess, or topic_relevance_prob)\ntext_sources : Optional[List[str]]\n    List of text sources for extraction mode\nquestions : Optional[List[str]]\n    List of questions for guess/assess/topic_relevance_prob modes\navailable_terms : Optional[List[str]]\n    Available terms for assess mode (one per question)\ntopic_descriptors : Optional[str]\n    Topic descriptors JSON string for topic_relevance_prob mode (shared across all questions)\nbatch_size : Optional[int]\n    Number of items to process in each batch\nshow_progress : bool\n    Whether to show progress bar\n\nReturns\n-------\nList[Optional[Dict[str, Any]]]\n    List of results, one per input",
          "process_dataframe": "Process a DataFrame based on the specified operation mode.\n\nIn EXTRACT mode, if 'source_id' column exists, the method will:\n- Group rows by source_id\n- Process each unique source only once (using text from first occurrence)\n- Apply the result to all rows with the same source_id\nThis optimization reduces redundant processing when multiple rows reference the same source.\n\nParameters\n----------\ndf : pd.DataFrame\n    DataFrame to process\nmode : OperationMode\n    Operation mode (EXTRACT, GUESS, ASSESS, or TOPIC_RELEVANCE_PROB)\ntext_source_col : str, optional\n    Column name for text sources (required for EXTRACT mode)\nquestion_col : str, optional\n    Column name for questions (required for GUESS, ASSESS, and TOPIC_RELEVANCE_PROB modes)\navailable_terms : Union[List[str], List[Dict], str], optional\n    Available terms for ASSESS mode\ntopic_descriptors : Union[List[str], List[Dict], str, Dict], optional\n    Topic descriptors for TOPIC_RELEVANCE_PROB mode. Supports:\n    - List of strings\n    - JSON string\n    - File path to KeywordClusterer JSON\n    - KeywordClusterer dict with 'clusters' or 'cluster_stats'\nprogress_bar : bool, optional\n    Show progress bar. Default is True.\nsave_path : str, optional\n    Path to save results\nskip_existing : bool, optional\n    Skip rows with existing results. Default is True.\ncheckpoint_batch_size : int, optional\n    Save checkpoint every N rows\nuse_batch_processing : bool, optional\n    Use batch processing if available. Default is True.\nbatch_size : int, optional\n    Batch size for processing\n\nReturns\n-------\npd.DataFrame\n    DataFrame with added result columns",
          "_process_dataframe_batch": "Process DataFrame using batch API.\n\nNote: Simplified since _batch_process() handles chunking and progress internally.",
          "_process_dataframe_sequential": "Process DataFrame sequentially.",
          "_process_dataframe_with_source_grouping": "Process DataFrame by grouping rows with same source_id.\n\nOnly processes each unique source once and applies results to all rows with that source_id.\nHandles interrupts gracefully at any stage.\n\nParameters\n----------\nresult_df : pd.DataFrame\n    DataFrame to process (will be modified in place)\ntext_source_col : str\n    Column name containing source text\nprefix : str\n    Column prefix for results (e.g., 'extract')\nskip_existing : bool\n    If True, skip sources that already have results\nprogress_bar : bool\n    Show progress bars\nsave_path : str, optional\n    Path to save checkpoints and final results\ncheckpoint_batch_size : int, optional\n    Save checkpoint every N rows\nshould_batch : bool\n    Use batch processing if True\nbatch_size : int\n    Batch size for processing\n\nReturns\n-------\npd.DataFrame\n    Processed DataFrame with results",
          "_process_dataframe_with_question_grouping": "Process DataFrame by grouping rows with identical question text.\n\nOnly processes each unique question once and applies results to all rows with that question.\nHandles interrupts gracefully at any stage.\n\nParameters\n----------\nresult_df : pd.DataFrame\n    DataFrame to process (will be modified in place)\nquestion_col : str\n    Column name containing question text\ntopic_descriptors : Union[List[str], List[Dict], str, Dict]\n    Topic descriptors for relevance assessment\nprefix : str\n    Column prefix for results (e.g., 'topic_relevance')\nskip_existing : bool\n    If True, skip questions that already have results\nprogress_bar : bool\n    Show progress bars\nsave_path : str, optional\n    Path to save checkpoints and final results\ncheckpoint_batch_size : int, optional\n    Save checkpoint every N rows\nshould_batch : bool\n    Use batch processing if True\nbatch_size : int\n    Batch size for processing\n\nReturns\n-------\npd.DataFrame\n    Processed DataFrame with results",
          "_save_and_log_stats": "Save results and log statistics.\n\nParameters\n----------\nresult_df : pd.DataFrame\n    DataFrame to save\nsave_path : str, optional\n    Path to save results\nstats : dict\n    Statistics dictionary with processing metrics\nmode_name : str, optional\n    Name of the processing mode for logging. Default is 'Source grouping'."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/agents/question_augmentation_agent.py",
    "file": "question_augmentation_agent.py",
    "module": "question_augmentation_agent",
    "parent_module": "agents",
    "package": "RAG_supporters",
    "module_docstring": "Question Augmentation Agent for question rephrasing and alternative question generation.\n\nThis agent provides functionality to:\n1. Rephrase questions to align with source context and domain terminology\n2. Generate alternative questions based on source content",
    "classes": {
      "QuestionAugmentationAgent": {
        "docstring": "Agent for question generation and rephrasing in CSV/DataFrame contexts.\n\nThis agent can rephrase questions to align with source content and domain,\nand generate alternative questions based on provided sources. It's designed\nto work with CSV datasets containing questions and sources for RAG training.\n\nParameters\n----------\nllm : BaseChatModel\n    Language model instance for performing question operations.\nmax_retries : int, optional\n    Maximum number of retries for LLM calls. Default is 3.\nbatch_size : int, optional\n    Default batch size for batch processing. Default is 10.\n\nAttributes\n----------\n_llm : BaseChatModel\n    The language model used for question operations.\n_max_retries : int\n    Maximum retries for LLM operations.\nbatch_size : int\n    Default batch size for processing.",
        "methods": {
          "__init__": "Initialize the QuestionAugmentationAgent.\n\nParameters\n----------\nllm : BaseChatModel\n    Language model instance for performing question operations.\nmax_retries : int, optional\n    Maximum number of retries for LLM calls. Default is 3.\nbatch_size : int, optional\n    Default batch size for batch processing. Default is 10.",
          "_check_openai_llm": "Check if the LLM is from OpenAI for batch processing.",
          "_invoke_llm_with_retry": "Invoke the LLM with retry logic.\n\nParameters\n----------\nprompt : str\n    The prompt to send to the LLM.\n\nReturns\n-------\nOptional[str]\n    The LLM's response, or None if all retries failed.",
          "rephrase_question_with_source": "Rephrase a question to align with the terminology and context of a source.\n\nThis method takes a question and a source text, and rephrases the question\nto better fit the domain, terminology, and context found in the source\nwhile preserving the original question's meaning.\n\nParameters\n----------\nquestion : str\n    The original question to rephrase.\nsource : str\n    The source text providing context and domain terminology.\nallow_vague : bool, optional\n    If True, allows the rephrased question to use less precise language.\n    If False (default), ensures the question is clear and specific.\n\nReturns\n-------\nOptional[str]\n    The rephrased question, or None if rephrasing failed.\n\nExamples\n--------\n>>> agent = QuestionAugmentationAgent(llm=my_llm)\n>>> question = \"What does it do?\"\n>>> source = \"Mitochondria are organelles that generate ATP...\"\n>>> rephrased = agent.rephrase_question_with_source(question, source)\n>>> print(rephrased)\n\"What is the primary function of mitochondria?\"",
          "rephrase_question_with_domain": "Rephrase a question to align with a specific domain or context.\n\nThis method takes a question and a domain description, and rephrases\nthe question to better fit the terminology and conventions of that domain.\n\nParameters\n----------\nquestion : str\n    The original question to rephrase.\ndomain : str\n    The domain or context description (e.g., \"biology\", \"machine learning\").\nallow_vague : bool, optional\n    If True, allows the rephrased question to use less precise language.\n    If False (default), ensures the question is clear and specific.\n\nReturns\n-------\nOptional[str]\n    The rephrased question, or None if rephrasing failed.\n\nExamples\n--------\n>>> agent = QuestionAugmentationAgent(llm=my_llm)\n>>> question = \"How do you make it learn?\"\n>>> domain = \"machine learning\"\n>>> rephrased = agent.rephrase_question_with_domain(question, domain)\n>>> print(rephrased)\n\"How do you train a machine learning model?\"",
          "generate_alternative_questions": "Generate n alternative questions that can be answered by the source.\n\nThis method analyzes a source text and generates multiple diverse\nquestions that can be answered using the information in the source.\n\nParameters\n----------\nsource : str\n    The source text to generate questions from.\nn : int, optional\n    Number of questions to generate. Default is 5.\nallow_vague : bool, optional\n    If True, allows generated questions to use less precise language.\n    If False (default), ensures questions are specific and focused.\n\nReturns\n-------\nOptional[List[str]]\n    List of generated questions, or None if generation failed.\n\nExamples\n--------\n>>> agent = QuestionAugmentationAgent(llm=my_llm)\n>>> source = \"Photosynthesis is the process by which plants...\"\n>>> questions = agent.generate_alternative_questions(source, n=3)\n>>> for q in questions:\n...     print(q)\n\"What is photosynthesis?\"\n\"How do plants produce energy?\"\n\"What role does chlorophyll play in photosynthesis?\"",
          "rephrase_question_with_source_batch": "Rephrase multiple questions with their corresponding sources in batch.\n\nParameters\n----------\nquestions : List[str]\n    List of questions to rephrase.\nsources : List[str]\n    List of source texts (must match length of questions).\nallow_vague : bool, optional\n    If True, allows rephrased questions to use less precise language.\n\nReturns\n-------\nList[Optional[str]]\n    List of rephrased questions (None for failures).",
          "rephrase_question_with_domain_batch": "Rephrase multiple questions with a domain in batch.\n\nParameters\n----------\nquestions : List[str]\n    List of questions to rephrase.\ndomain : str\n    Domain context (same for all questions).\nallow_vague : bool, optional\n    If True, allows rephrased questions to use less precise language.\n\nReturns\n-------\nList[Optional[str]]\n    List of rephrased questions (None for failures).",
          "generate_alternative_questions_batch": "Generate alternative questions for multiple sources in batch.\n\nParameters\n----------\nsources : List[str]\n    List of source texts.\nn : int, optional\n    Number of questions to generate per source. Default is 5.\nallow_vague : bool, optional\n    If True, allows generated questions to use less precise language.\n\nReturns\n-------\nList[Optional[List[str]]]\n    List of question lists (None for failures).",
          "_batch_process_rephrase_source": "Batch rephrase questions with sources.",
          "_batch_process_rephrase_domain": "Batch rephrase questions with domain.",
          "_batch_process_generate": "Batch generate questions from sources.",
          "process_dataframe_rephrasing": "Process a DataFrame by rephrasing questions based on source or domain.\n\nParameters\n----------\ndf : pd.DataFrame\n    DataFrame containing question and optionally source columns.\nrephrase_mode : str, optional\n    Mode of rephrasing: 'source' (use source text) or 'domain' (use domain).\n    Default is 'source'.\ndomain : Optional[str], optional\n    Domain context for rephrasing when mode is 'domain'.\nallow_vague : bool, optional\n    If True, allows rephrased questions to use less precise language.\n    If False (default), ensures questions are clear and specific.\ncolumns_mapping : Optional[dict], optional\n    Mapping of expected column names. Should contain:\n    'question_text' (default: 'question_text'),\n    'source_text' (default: 'source_text' - required for 'source' mode),\n    'rephrased_question' (default: 'rephrased_question' - output column).\nuse_batch_processing : bool, optional\n    Use batch processing if available. Default is True.\nbatch_size : int, optional\n    Batch size for processing. If None, uses self.batch_size.\ncheckpoint_batch_size : int, optional\n    Save checkpoint every N rows.\nsave_path : str, optional\n    Path to save results and checkpoints.\n\nReturns\n-------\npd.DataFrame\n    DataFrame with added column for rephrased questions.\n\nRaises\n------\nValueError\n    If required columns are missing or invalid mode is specified.",
          "_process_dataframe_rephrasing_batch": "Process DataFrame rephrasing using batch API.",
          "_process_dataframe_rephrasing_sequential": "Process DataFrame rephrasing sequentially.",
          "process_dataframe_generation": "Process a DataFrame by generating alternative questions for each source.\n\nThis method creates new rows in the DataFrame, with each row containing\none of the generated alternative questions paired with its source.\n\nParameters\n----------\ndf : pd.DataFrame\n    DataFrame containing source text column.\nn_questions : int, optional\n    Number of alternative questions to generate per source. Default is 5.\nallow_vague : bool, optional\n    If True, allows generated questions to use less precise language.\n    If False (default), ensures questions are specific and focused.\ncolumns_mapping : Optional[dict], optional\n    Mapping of expected column names. Should contain:\n    'source_text' (default: 'source_text'),\n    'question_text' (default: 'question_text' - output column).\nuse_batch_processing : bool, optional\n    Use batch processing if available. Default is True.\nbatch_size : int, optional\n    Batch size for processing. If None, uses self.batch_size.\n\nReturns\n-------\npd.DataFrame\n    New DataFrame with generated question-source pairs.\n\nRaises\n------\nValueError\n    If required columns are missing.",
          "_process_dataframe_generation_batch": "Process DataFrame generation using batch API.",
          "_process_dataframe_generation_sequential": "Process DataFrame generation sequentially.",
          "process_csv_rephrasing": "Process a CSV file by rephrasing questions.\n\nParameters\n----------\ninput_csv_path : str\n    Path to the input CSV file.\noutput_csv_path : str\n    Path to save the output CSV with rephrased questions.\nrephrase_mode : str, optional\n    Mode of rephrasing: 'source' or 'domain'. Default is 'source'.\ndomain : Optional[str], optional\n    Domain context for rephrasing when mode is 'domain'.\nallow_vague : bool, optional\n    If True, allows rephrased questions to use less precise language.\n    If False (default), ensures questions are clear and specific.\ncolumns_mapping : Optional[dict], optional\n    Mapping of expected column names.\nuse_batch_processing : bool, optional\n    Use batch processing if available. Default is True.\nbatch_size : int, optional\n    Batch size for processing.\ncheckpoint_batch_size : int, optional\n    Save checkpoint every N rows.\n\nReturns\n-------\npd.DataFrame\n    The processed DataFrame with rephrased questions.",
          "process_csv_generation": "Process a CSV file by generating alternative questions.\n\nParameters\n----------\ninput_csv_path : str\n    Path to the input CSV file.\noutput_csv_path : str\n    Path to save the output CSV with generated questions.\nn_questions : int, optional\n    Number of questions to generate per source. Default is 5.\nallow_vague : bool, optional\n    If True, allows generated questions to use less precise language.\n    If False (default), ensures questions are specific and focused.\ncolumns_mapping : Optional[dict], optional\n    Mapping of expected column names.\nuse_batch_processing : bool, optional\n    Use batch processing if available. Default is True.\nbatch_size : int, optional\n    Batch size for processing.\n\nReturns\n-------\npd.DataFrame\n    The generated DataFrame with question-source pairs."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/agents/source_assesment.py",
    "file": "source_assesment.py",
    "module": "source_assesment",
    "parent_module": "agents",
    "package": "RAG_supporters",
    "module_docstring": "Source evaluation agent for assessing text source quality.",
    "classes": {
      "ScoreRange": {
        "docstring": "Validates score is within 0-10 range.",
        "methods": {}
      },
      "SourceEvaluation": {
        "docstring": "Model for source evaluation scores and reasoning.",
        "methods": {
          "validate_all_scores_present": "Ensure all required scores are present and valid.\n\nValidates that all six evaluation dimensions have scores assigned\nand that none of the score values are None.\n\nRaises\n------\nValueError\n    If any required field is missing or if any score is None"
        }
      },
      "AgentState": {
        "docstring": "State for the LangGraph agent.",
        "methods": {}
      },
      "SourceEvaluationAgent": {
        "docstring": "LangGraph agent for evaluating sources with retry logic.",
        "methods": {
          "__init__": "Initialize the agent with an LLM and retry configuration.\n\nSets up the evaluation pipeline with output parsers, prompt templates,\nand the LangGraph workflow for reliable source evaluation.\n\nParameters\n----------\nllm : BaseChatModel, optional\n    Language model to use for evaluation. Default is None.\nmax_retries : int, optional\n    Maximum number of retries for getting correct format. Default is 3.\nevaluation_prompt : str, optional\n    Prompt template for evaluation. Default is SINGLE_SRC_SCORE_PROMPT.\nbatch_size : int, optional\n    Default batch size for batch processing. Default is 10.",
          "_check_openai_llm": "Check if the LLM is from OpenAI (ChatOpenAI).\n\nReturns\n-------\nbool\n    True if LLM is from OpenAI, False otherwise",
          "_create_prompt_template": "Create the prompt template with format instructions from parser.",
          "_build_graph": "Build the LangGraph workflow.",
          "_evaluate_source": "Evaluate the source using the LLM with output parser.",
          "_validate_response": "Validate the response using Pydantic.",
          "_should_retry": "Determine if we should retry or end.",
          "_handle_retry": "Handle retry logic.",
          "_format_output": "Format the output to match the expected structure.",
          "_generate_score_summary": "Generate a formatted score summary.",
          "evaluate": "Evaluate a source for a given question using the LangGraph workflow.\n\nMain interface method that orchestrates the complete evaluation process\nincluding retry logic and output formatting.\n\nParameters\n----------\nquestion : str\n    The question to evaluate the source against\nsource_content : str\n    The source content to evaluate for relevance and quality\n\nReturns\n-------\nOptional[Dict[str, Any]]\n    Dictionary with evaluation scores and reasoning, or None if evaluation failed",
          "evaluate_batch": "Evaluate multiple question-source pairs in a batch using LangChain's batch processing.\n\nOnly available for OpenAI LLMs. Falls back to sequential processing for other LLMs.\n\nParameters\n----------\nquestions : List[str]\n    List of questions to evaluate\nsource_contents : List[str]\n    List of source contents corresponding to each question\n\nReturns\n-------\nList[Optional[Dict[str, Any]]]\n    List of evaluation results, None for failed evaluations",
          "process_dataframe": "Process a pandas DataFrame with question-source pairs and add evaluation scores.\n\nBatch processes multiple question-source pairs, adding comprehensive\nevaluation scores across all dimensions with optional reasoning text.\n\nParameters\n----------\ndf : pd.DataFrame\n    DataFrame containing question and source columns to evaluate\nquestion_col : str, optional\n    Name of the column containing questions. Default is \"question_text\".\nsource_col : str, optional\n    Name of the column containing source content. Default is \"source_text\".\ninclude_reasoning : bool, optional\n    Whether to include reasoning text for each score. Default is False.\nprogress_bar : bool, optional\n    Whether to display a progress bar during processing. Default is True.\nsave_path : str, optional\n    Path to save the results as CSV. Default is None.\nskip_existing : bool, optional\n    Whether to skip rows that already have evaluation scores. Default is True.\ncheckpoint_batch_size : int, optional\n    Batch size for saving intermediate checkpoints. Default is None.\nuse_batch_processing : bool, optional\n    Whether to use batch processing for OpenAI LLMs. Default is True.\nbatch_size : int, optional\n    Size of batches for batch processing. If None, uses default from init. Default is None.\n\nReturns\n-------\npd.DataFrame\n    DataFrame with added evaluation score columns and optional reasoning",
          "_process_dataframe_batch": "Process DataFrame using batch processing for OpenAI LLMs.\n\nInternal method that handles the batch processing logic.",
          "_process_dataframe_sequential": "Original sequential processing method.\n\nThis is the existing process_dataframe logic moved to a separate method."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/agents/text_augmentation.py",
    "file": "text_augmentation.py",
    "module": "text_augmentation",
    "parent_module": "agents",
    "package": "RAG_supporters",
    "module_docstring": "Text Augmentation Agent for generating alternative questions and sources.\n\nThis agent provides functionality to rephrase questions and source texts while\npreserving their original meaning, useful for data augmentation in RAG datasets.",
    "classes": {
      "TextAugmentationAgent": {
        "docstring": "Agent for augmenting text data through rephrasing.\n\nThis agent can rephrase entire texts or specific sentences within texts\nwhile preserving the original meaning. It's designed to work with CSV\ndatasets containing questions and sources for RAG training.\n\nParameters\n----------\nllm : BaseChatModel\n    Language model instance for performing text rephrasing.\nverify_meaning : bool, optional\n    Whether to verify that rephrased text preserves meaning. Default is False.\nmax_retries : int, optional\n    Maximum number of retries for LLM calls. Default is 3.\nbatch_size : int, optional\n    Default batch size for batch processing. Default is 10.\n\nAttributes\n----------\n_llm : BaseChatModel\n    The language model used for rephrasing.\n_verify_meaning : bool\n    Whether to verify meaning preservation.\n_max_retries : int\n    Maximum retries for LLM operations.\nbatch_size : int\n    Default batch size for processing.",
        "methods": {
          "__init__": "Initialize the TextAugmentationAgent.\n\nParameters\n----------\nllm : BaseChatModel\n    Language model instance for performing text rephrasing.\nverify_meaning : bool, optional\n    Whether to verify that rephrased text preserves meaning. Default is False.\nmax_retries : int, optional\n    Maximum number of retries for LLM calls. Default is 3.\nbatch_size : int, optional\n    Default batch size for batch processing. Default is 10.",
          "_check_openai_llm": "Check if the LLM is from OpenAI for batch processing.",
          "_invoke_llm_with_retry": "Invoke the LLM with retry logic.\n\nParameters\n----------\nprompt : str\n    The prompt to send to the LLM.\n\nReturns\n-------\nOptional[str]\n    The LLM's response, or None if all retries failed.",
          "_invoke_llm_batch": "Invoke the LLM with a batch of prompts.\n\nParameters\n----------\nprompts : List[str]\n    List of prompts to send to the LLM.\n\nReturns\n-------\nList[Optional[str]]\n    List of responses, or None for failed items.",
          "_verify_text_equivalence": "Verify that the rephrased text preserves the original meaning.\n\nParameters\n----------\noriginal_text : str\n    The original text.\nrephrased_text : str\n    The rephrased text to verify.\n\nReturns\n-------\nbool\n    True if texts are semantically equivalent, False otherwise.",
          "_verify_text_equivalence_batch": "Verify meaning preservation for batch of text pairs.\n\nParameters\n----------\noriginal_texts : List[str]\n    List of original texts.\nrephrased_texts : List[str]\n    List of rephrased texts.\n\nReturns\n-------\nList[bool]\n    List of verification results.",
          "rephrase_text": "Rephrase text using the specified mode.\n\nThis is a unified interface that dispatches to the appropriate\nrephrasing method based on the mode parameter.\n\nParameters\n----------\ntext : str\n    The text to rephrase.\nmode : str, optional\n    Rephrasing mode: \"full\" (rephrase entire text),\n    \"sentence\" or \"random\" (rephrase one random sentence).\n    Default is \"full\".\nverify : Optional[bool], optional\n    Whether to verify meaning preservation. If None, uses instance default.\n\nReturns\n-------\nOptional[str]\n    The rephrased text, or None if rephrasing failed.",
          "rephrase_full_text": "Rephrase an entire text while preserving its meaning.\n\nParameters\n----------\ntext : str\n    The text to rephrase.\nverify : Optional[bool], optional\n    Whether to verify meaning preservation. If None, uses instance default.\n\nReturns\n-------\nOptional[str]\n    The rephrased text, or None if rephrasing failed.",
          "rephrase_full_text_batch": "Rephrase multiple texts while preserving their meanings.\n\nParameters\n----------\ntexts : List[str]\n    List of texts to rephrase.\nverify : Optional[bool], optional\n    Whether to verify meaning preservation. If None, uses instance default.\n\nReturns\n-------\nList[Optional[str]]\n    List of rephrased texts, or None for failed items.",
          "rephrase_random_sentence": "Rephrase a randomly selected sentence within the text.\n\nParameters\n----------\ntext : str\n    The text containing sentences to rephrase.\nverify : Optional[bool], optional\n    Whether to verify meaning preservation. If None, uses instance default.\n\nReturns\n-------\nOptional[str]\n    The text with one sentence rephrased, or None if rephrasing failed.",
          "rephrase_random_sentence_batch": "Rephrase a random sentence in multiple texts.\n\nParameters\n----------\ntexts : List[str]\n    List of texts to process.\nverify : Optional[bool], optional\n    Whether to verify meaning preservation. If None, uses instance default.\n\nReturns\n-------\nList[Optional[str]]\n    List of texts with one sentence rephrased, or None for failed items.",
          "augment_dataframe": "Augment a DataFrame by adding rephrased versions of questions and/or sources.\n\nParameters\n----------\ndf : pd.DataFrame\n    DataFrame containing question and source columns.\nrephrase_question : bool, optional\n    Whether to rephrase questions. Default is True.\nrephrase_source : bool, optional\n    Whether to rephrase sources. Default is True.\nrephrase_mode : str, optional\n    Rephrasing mode: 'full' (entire text), 'sentence' (random sentence),\n    or 'random' (randomly choose between full and sentence). Default is 'random'.\nprobability : float, optional\n    Probability of applying rephrasing to each row. Default is 0.5.\ncolumns_mapping : Optional[dict], optional\n    Mapping of expected column names. Should contain:\n    'question_text' (default: 'question_text'),\n    'source_text' (default: 'source_text').\nuse_batch_processing : bool, optional\n    Use batch processing if available. Default is True.\nbatch_size : int, optional\n    Batch size for processing. If None, uses instance default.\n\nReturns\n-------\npd.DataFrame\n    New DataFrame with augmented rows added to the original data.",
          "_augment_dataframe_sequential": "Process DataFrame sequentially (original implementation).",
          "_augment_dataframe_batch": "Process DataFrame using batch processing.",
          "process_csv": "Process a CSV file by augmenting it with rephrased versions.\n\nParameters\n----------\ninput_csv_path : str\n    Path to the input CSV file.\noutput_csv_path : Optional[str], optional\n    Path to save the augmented CSV. If None, overwrites input file.\nrephrase_question : bool, optional\n    Whether to rephrase questions. Default is True.\nrephrase_source : bool, optional\n    Whether to rephrase sources. Default is True.\nrephrase_mode : str, optional\n    Rephrasing mode: 'full', 'sentence', or 'random'. Default is 'random'.\nprobability : float, optional\n    Probability of applying rephrasing to each row. Default is 0.5.\ncolumns_mapping : Optional[dict], optional\n    Mapping of expected column names.\nuse_batch_processing : bool, optional\n    Use batch processing if available. Default is True.\nbatch_size : int, optional\n    Batch size for processing.\n\nReturns\n-------\npd.DataFrame\n    The augmented DataFrame."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/augmentations/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "augmentations",
    "package": "RAG_supporters",
    "module_docstring": "Augmentation package for embedding and data augmentation utilities.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/augmentations/embedding.py",
    "file": "embedding.py",
    "module": "embedding",
    "parent_module": "augmentations",
    "package": "RAG_supporters",
    "module_docstring": "Embedding augmentation utilities for data augmentation.",
    "classes": {},
    "functions": {
      "random_noise_embedding": "Add random Gaussian noise to the embedding.\n\nArgs:\n    embedding (np.ndarray): The original embedding vector.\n    noise_level (float): The standard deviation of the Gaussian noise to be added.\n    probability (float): The probability of applying the noise augmentation.\n\nReturns:\n    np.ndarray: The augmented embedding with added noise.",
      "random_zero_embedding": "Randomly set whole embedding as zero with a given probability.\n\nThis would be used for steering embeddings to 'no information' state.\n\nArgs:\n    embedding (np.ndarray or torch.Tensor): The original embedding vector.\n    probability (float): The probability of applying the zeroing augmentation.\n\nReturns:\n    np.ndarray or torch.Tensor: The augmented embedding with possible zeroing."
    }
  },
  {
    "path": "RAG_supporters/clustering/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "clustering",
    "package": "RAG_supporters",
    "module_docstring": "Clustering utilities for keyword and suggestion embeddings.\n\nThis module provides clustering tools for working with embeddings:\n\n- ClusteringData: Data container for cluster information with JSON loader\n- KeywordClusterer: Unified clustering class with KMeans or Bisecting KMeans,\n  integrated centroid comparison methods, similarity search, topic descriptor\n  extraction (Phase 1), and source-to-cluster assignment (Phase 2)\n- Convenience functions for complete clustering workflows\n\nThese tools support the subspace/cluster steering roadmap by enabling\norganized grouping of embeddings and efficient similarity-based retrieval.\n\nThe KeywordClusterer class combines clustering and assignment functionality,\nsupporting both hard (one-hot) and soft (multi-subspace) assignment modes\nwith configurable temperature scaling and thresholds.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/clustering/clustering_data.py",
    "file": "clustering_data.py",
    "module": "clustering_data",
    "parent_module": "clustering",
    "package": "RAG_supporters",
    "module_docstring": "Clustering data handling component.",
    "classes": {
      "ClusteringData": {
        "docstring": "Container for cluster-related data.\n\nIntegrates with keyword_clustering.py JSON format for seamless Phase 3 dataset creation.\n\nAttributes:\n    n_clusters: Number of clusters\n    labels: Mapping of sample indices to cluster labels (hard or soft)\n    descriptors: Mapping of cluster IDs to topic descriptors\n    centroids: Cluster centroids as numpy array\n    metadata: Additional clustering metadata",
        "methods": {
          "from_json": "Load clustering data from JSON file generated by keyword_clustering.py.\n\nExpected JSON format:\n{\n    \"metadata\": {\n        \"algorithm\": \"kmeans\",\n        \"n_clusters\": 5,\n        \"n_keywords\": 100,\n        \"random_state\": 42,\n        \"embedding_dim\": 384,\n        \"assignment_config\": {...}\n    },\n    \"cluster_assignments\": {...},\n    \"clusters\": {...},\n    \"cluster_stats\": {\n        \"0\": {\n            \"size\": 20,\n            \"keywords_sample\": [...],\n            \"topic_descriptors\": [\"keyword1\", \"keyword2\", ...]\n        },\n        ...\n    },\n    \"centroids\": [[...], [...], ...],\n    \"embeddings\": {...}  # optional\n}\n\nArgs:\n    path: Path to clustering results JSON file\n    exclude_keys: Set of top-level keys to exclude from loading (e.g., {'embeddings'})\n\nReturns:\n    ClusteringData instance\n\nRaises:\n    ValueError: If file not found or invalid format",
          "_extract_descriptors_from_cluster_stats": "Extract cluster descriptors from cluster_stats in keyword_clustering.py format.\n\nArgs:\n    data: Full JSON data from keyword_clustering.py\n\nReturns:\n    Dictionary mapping cluster IDs to topic descriptor lists",
          "get_label": "Get cluster label for a sample.\n\nArgs:\n    idx: Sample index\n\nReturns:\n    Cluster label (int for hard, List[float] for soft) or None",
          "get_primary_cluster": "Get primary cluster ID for a sample.\n\nFor hard labels, returns the label directly.\nFor soft labels, returns the cluster with highest probability.\n\nArgs:\n    idx: Sample index\n\nReturns:\n    Primary cluster ID or None",
          "get_descriptors": "Get topic descriptors for a cluster.\n\nArgs:\n    cluster_id: Cluster ID\n\nReturns:\n    List of descriptor strings or None",
          "get_centroid": "Get centroid for a cluster.\n\nArgs:\n    cluster_id: Cluster ID\n\nReturns:\n    Centroid vector or None",
          "set_labels": "Set cluster labels from assignment results.\n\nArgs:\n    labels: Mapping of sample indices to cluster labels\n           (int for hard, List[float] for soft)",
          "infer_n_clusters_from_labels": "Infer number of clusters from labels.\n\nReturns:\n    Number of clusters\n\nRaises:\n    ValueError: If labels are not set or invalid",
          "validate": "Validate clustering data consistency.\n\nRaises:\n    ValueError: If data is inconsistent"
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/clustering/keyword_clustering.py",
    "file": "keyword_clustering.py",
    "module": "keyword_clustering",
    "parent_module": "clustering",
    "package": "RAG_supporters",
    "module_docstring": "Keyword clustering using KMeans and Bisecting KMeans algorithms.\n\nThis module provides a unified clustering solution for keyword/suggestion embeddings\nwith integrated Phase 1 (clustering) and Phase 2 (assignment) functionality for the\nsubspace/cluster steering roadmap.\n\nMain Components\n---------------\nKeywordClusterer : class\n    Unified clustering class that handles:\n    - Clustering keyword/suggestion embeddings (Phase 1)\n    - Extracting topic descriptors from clusters\n    - Assigning new items to clusters (Phase 2)\n    - Saving and loading clustering configurations\n\ncluster_keywords_from_embeddings : function\n    Convenience function for the complete clustering workflow\n\nFeatures\n--------\nPhase 1: Clustering Foundation\n    - KMeans and Bisecting KMeans algorithms\n    - Topic descriptor extraction (n closest keywords to centroids)\n    - Cluster quality metrics and statistics\n    - Persistent storage of results with metadata\n\nPhase 2: Source Assignment\n    - Hard (one-hot) and soft (multi-subspace) assignment modes\n    - Temperature-scaled softmax probability distributions\n    - Configurable threshold filtering\n    - Batch processing support\n    - State management and fitted model validation\n\nDistance Metrics\n    - Euclidean distance\n    - Cosine similarity/distance\n\nExamples\n--------\nBasic clustering workflow:\n\n>>> from RAG_supporters.embeddings import KeywordEmbedder\n>>> from RAG_supporters.clustering import cluster_keywords_from_embeddings\n>>>\n>>> # Create embeddings\n>>> embedder = KeywordEmbedder()\n>>> keywords = [\"machine learning\", \"deep learning\", \"AI\"]\n>>> embeddings = embedder.create_embeddings(keywords)\n>>>\n>>> # Cluster and extract topics\n>>> clusterer, topics = cluster_keywords_from_embeddings(\n...     embeddings,\n...     n_clusters=2,\n...     n_descriptors=5,\n...     output_path=\"clusters.json\"\n... )\n\nAssignment workflow:\n\n>>> # Load saved clustering\n>>> from RAG_supporters.clustering import KeywordClusterer\n>>> clusterer = KeywordClusterer.from_results(\"clusters.json\")\n>>>\n>>> # Configure assignment\n>>> clusterer.configure_assignment(\n...     assignment_mode=\"soft\",\n...     threshold=0.15,\n...     temperature=1.0,\n...     metric=\"cosine\"\n... )\n>>>\n>>> # Assign new items\n>>> new_embedding = embedder.create_embeddings([\"neural networks\"])\n>>> result = clusterer.assign(new_embedding[\"neural networks\"])\n>>> print(result['primary_cluster'])\n0\n\nNotes\n-----\n- Models loaded via `from_results()` are in a partially initialized state\n  suitable for assignment operations but should not be re-fitted\n- Assignment configuration (mode, threshold, temperature, metric) is\n  persisted in save/load operations\n- All assignment methods validate fitted state before operating\n\nSee Also\n--------\nRAG_supporters.embeddings.KeywordEmbedder : For creating embeddings\ndocs/CLUSTERING_AND_ASSIGNMENT.md : Complete usage guide\n\nTODO: Future extensions\n-----------------------\n- Additional clustering algorithms (DBSCAN, hierarchical, Agglomerative)\n- Automatic optimal cluster number detection (elbow method, silhouette score)\n- Cluster quality metrics and visualization tools\n- Additional distance metrics (Manhattan, Mahalanobis)\n- Online/incremental clustering support",
    "classes": {
      "KeywordClusterer": {
        "docstring": "Cluster keyword embeddings and perform centroid-based comparisons.\n\nThis class combines clustering functionality with centroid comparison,\nallowing both cluster creation and similarity search operations.",
        "methods": {
          "__init__": "Initialize the clusterer.\n\nParameters\n----------\nalgorithm : str\n    Clustering algorithm: 'kmeans' or 'bisecting_kmeans'\nn_clusters : int\n    Number of clusters\nrandom_state : int\n    Random state for reproducibility\n**kwargs\n    Additional arguments for the clustering algorithm",
          "_create_model": "Create the clustering model.",
          "fit": "Fit the clustering model.\n\nParameters\n----------\nkeyword_embeddings : Dict[str, np.ndarray]\n    Dictionary mapping keywords to embeddings\n\nReturns\n-------\nKeywordClusterer\n    Self for chaining",
          "get_cluster_assignments": "Get cluster assignments for each keyword.\n\nReturns\n-------\nDict[str, int]\n    Dictionary mapping keywords to cluster labels",
          "get_clusters": "Get keywords grouped by cluster.\n\nReturns\n-------\nDict[int, List[str]]\n    Dictionary mapping cluster labels to lists of keywords",
          "get_centroids": "Get cluster centroids.\n\nReturns\n-------\nnp.ndarray\n    Array of cluster centroids, shape (n_clusters, embedding_dim)",
          "extract_topic_descriptors": "Extract topic descriptors (n closest keywords) for each cluster.\n\nThis method identifies the most representative keywords for each cluster\nby finding the n keywords closest to each cluster centroid.\n\nParameters\n----------\nn_descriptors : int\n    Number of closest keywords to use as descriptors per topic\nmetric : str\n    Distance metric: 'euclidean' or 'cosine'\nignore_n_closest_keywords : int\n    Number of additional closest keywords to skip after applying distance filter.\n    This skips keywords beyond those already filtered by min_descriptor_distance.\n    Useful to reduce influence of dominant topics and improve diversity.\n    Default: 0 (no skipping)\nmin_descriptor_distance : Optional[float]\n    Minimum distance threshold for including keywords in descriptors.\n    Keywords with distance <= this threshold are excluded first, then\n    ignore_n_closest_keywords additional keywords are skipped from the remaining.\n    Default: None (no filtering)\n\nReturns\n-------\nDict[int, List[str]]\n    Dictionary mapping cluster/topic IDs to lists of descriptor keywords.\n    Note: If filtering parameters are too restrictive, some topics may have\n    fewer descriptors than requested (or even empty lists).\n\nExamples\n--------\n>>> clusterer = KeywordClusterer(n_clusters=3)\n>>> clusterer.fit(keyword_embeddings)\n>>> topics = clusterer.extract_topic_descriptors(n_descriptors=5)\n>>> print(topics[0])  # Top 5 keywords for cluster 0\n['machine learning', 'deep learning', 'neural networks', ...]\n\n>>> # Skip the closest keyword and require minimum distance\n>>> topics = clusterer.extract_topic_descriptors(\n...     n_descriptors=5,\n...     ignore_n_closest_keywords=1,\n...     min_descriptor_distance=0.1\n... )",
          "configure_assignment": "Configure default assignment parameters.\n\nParameters\n----------\nassignment_mode : str\n    Assignment mode:\n    - 'hard': Assigns to single best cluster only\n    - 'soft': Assigns to all clusters above threshold probability\nthreshold : float\n    Minimum probability for cluster assignment (0.0 to 1.0)\n    - For hard mode: minimum confidence to assign (else returns no clusters)\n    - For soft mode: minimum probability to include cluster in assignment\n    Recommended ranges:\n    - Soft mode: 0.05-0.2 (include clusters with modest relevance)\n    - Hard mode: 0.3-0.5 (require strong confidence)\nmetric : str\n    Distance metric: 'euclidean' or 'cosine'\n    Recommended: 'cosine' for text embeddings\n\nReturns\n-------\nKeywordClusterer\n    Self for chaining\n\nExamples\n--------\n>>> clusterer.configure_assignment(\n...     mode=\"soft\",\n...     threshold=0.1,\n...     metric=\"cosine\"\n... )",
          "_check_fitted": "Check if the model has been fitted.",
          "_compute_probabilities": "Compute probability distribution over clusters.\n\nConverts distances to probabilities using similarity normalization:\n- For cosine: similarity = 1 - distance, normalized\n- For euclidean: similarity = 1 / (1 + distance), normalized\n\nParameters\n----------\nembedding : np.ndarray\n    Query embedding vector\nmetric : str\n    Distance metric\n\nReturns\n-------\nnp.ndarray\n    Probability distribution over clusters (sums to 1.0)",
          "_hard_assignment": "Perform hard (single-cluster) assignment.\n\nParameters\n----------\nprobabilities : np.ndarray\n    Probability distribution over clusters\nthreshold : float\n    Minimum probability required for assignment\n\nReturns\n-------\nTuple[List[int], Optional[int]]\n    (assigned_clusters, primary_cluster)\n    If max probability < threshold: ([], None)\n    Otherwise: ([best_cluster], best_cluster)",
          "_soft_assignment": "Perform soft (multi-cluster) assignment.\n\nParameters\n----------\nprobabilities : np.ndarray\n    Probability distribution over clusters\nthreshold : float\n    Minimum probability required for cluster inclusion\n\nReturns\n-------\nTuple[List[int], int]\n    (assigned_clusters, primary_cluster)\n    assigned_clusters: All clusters with probability >= threshold\n    primary_cluster: Cluster with highest probability",
          "assign": "Assign an embedding to cluster(s).\n\nThis method computes the probability distribution over clusters and\nassigns the embedding based on the specified mode and threshold.\n\nParameters\n----------\nembedding : np.ndarray\n    Query embedding vector\nmode : Optional[str]\n    Assignment mode ('hard' or 'soft'). Uses configured default if None.\n    - 'hard': Single cluster assignment (if above threshold)\n    - 'soft': Multi-cluster assignment (all clusters above threshold)\nthreshold : Optional[float]\n    Minimum probability for assignment. Uses configured default if None.\n    Must be between 0.0 and 1.0.\nmetric : Optional[str]\n    Distance metric ('euclidean' or 'cosine'). Uses configured default if None.\n\nReturns\n-------\nDict[str, Any]\n    Assignment results with keys:\n    - mode: Assignment mode used\n    - probabilities: Dict mapping cluster IDs to probabilities (always included)\n    - assigned_clusters: List of assigned cluster IDs (may be empty in hard mode)\n    - primary_cluster: ID of most likely cluster (None if no assignment in hard mode)\n    - threshold_used: Threshold value that was applied\n    - metric: Distance metric used\n\nExamples\n--------\n>>> # Hard assignment (single cluster)\n>>> result = clusterer.assign(embedding, mode=\"hard\", threshold=0.3)\n>>> print(result['assigned_clusters'])  # [2] or [] if below threshold\n\n>>> # Soft assignment (multi-cluster)\n>>> result = clusterer.assign(embedding, mode=\"soft\", threshold=0.1)\n>>> print(result['assigned_clusters'])  # [0, 2, 5] - all clusters > 10%\n>>> print(result['probabilities'])      # {0: 0.45, 1: 0.05, 2: 0.25, ...}",
          "assign_batch": "Assign multiple embeddings to clusters.\n\nParameters\n----------\nembeddings : Dict[str, np.ndarray]\n    Dictionary mapping IDs to embedding vectors\nmode : Optional[str]\n    Assignment mode. Uses default if None.\nthreshold : Optional[float]\n    Probability threshold. Uses default if None.\nmetric : Optional[str]\n    Distance metric. Uses default if None.\n\nReturns\n-------\nDict[str, Dict[str, Any]]\n    Dictionary mapping IDs to assignment results\n\nExamples\n--------\n>>> clusterer = KeywordClusterer(n_clusters=3)\n>>> clusterer.fit(keyword_embeddings)\n>>> source_embeddings = {\"src1\": emb1, \"src2\": emb2}\n>>> results = clusterer.assign_batch(source_embeddings, mode=\"soft\")",
          "compute_distances": "Compute distances from embedding to all centroids.\n\nParameters\n----------\nembedding : np.ndarray\n    Query embedding vector\nmetric : str\n    Distance metric: 'euclidean' or 'cosine'\n\nReturns\n-------\nnp.ndarray\n    Array of distances to each centroid",
          "find_nearest_cluster": "Find the nearest cluster(s) for an embedding.\n\nParameters\n----------\nembedding : np.ndarray\n    Query embedding vector\nmetric : str\n    Distance metric\ntop_k : int\n    Number of nearest clusters to return\n\nReturns\n-------\nUnion[Tuple[int, float], List[Tuple[int, float]]]\n    If top_k=1: (cluster_label, distance)\n    If top_k>1: List of (cluster_label, distance) tuples",
          "get_all_distances": "Get distances to all clusters.\n\nParameters\n----------\nembedding : np.ndarray\n    Query embedding vector\nmetric : str\n    Distance metric\nsort_by_dist : bool\n    Whether to sort by distance (ascending)\n\nReturns\n-------\nDict[int, float]\n    Dictionary mapping cluster labels to distances",
          "compare_keyword": "Compare a keyword against centroids.\n\nParameters\n----------\nkeyword : str\n    Keyword to compare\nkeyword_embeddings : Dict[str, np.ndarray]\n    Dictionary of keyword embeddings\nmetric : str\n    Distance metric\ntop_k : int\n    Number of nearest clusters to return\n\nReturns\n-------\nDict[str, Any]\n    Comparison results with nearest clusters and all distances",
          "compare_text": "Compare arbitrary text against centroids.\n\nParameters\n----------\ntext : str\n    Text to compare\nembedding_model : Any\n    Model to generate text embedding (must have encode() method)\nmetric : str\n    Distance metric\ntop_k : int\n    Number of nearest clusters to return\n\nReturns\n-------\nDict[str, Any]\n    Comparison results",
          "save_results": "Save clustering results to JSON.\n\nParameters\n----------\noutput_path : str\n    Path to save results\ninclude_embeddings : bool\n    Whether to include embeddings in output\ninclude_topics : bool\n    Whether to include topic descriptors in output",
          "load_results": "Load clustering results from JSON.\n\nParameters\n----------\ninput_path : str\n    Path to results file\n\nReturns\n-------\nDict[str, Any]\n    Clustering results",
          "from_results": "Create clusterer from saved clustering results.\n\nThis creates a clusterer with centroids loaded for assignment and\ncomparison operations. The underlying sklearn clustering model is\nnot restored, but all necessary components for assignment operations\nare available.\n\nParameters\n----------\nclustering_results_path : str\n    Path to clustering results JSON\n\nReturns\n-------\nKeywordClusterer\n    Initialized clusterer with loaded centroids and config\n\nExamples\n--------\n>>> # Save results\n>>> clusterer.fit(embeddings)\n>>> clusterer.save_results(\"clusters.json\")\n\n>>> # Load and use for assignment\n>>> loaded = KeywordClusterer.from_results(\"clusters.json\")\n>>> result = loaded.assign(new_embedding)"
        }
      }
    },
    "functions": {
      "cluster_keywords_from_embeddings": "Complete pipeline to cluster keywords and extract topics.\n\nThis is a convenience function that performs the full clustering workflow:\n1. Cluster keyword embeddings\n2. Extract topic descriptors\n3. Optionally save results\n\nParameters\n----------\nkeyword_embeddings : Dict[str, np.ndarray]\n    Dictionary mapping keywords to embeddings\nn_clusters : int\n    Number of clusters/topics to discover\nalgorithm : str\n    Clustering algorithm: 'kmeans' or 'bisecting_kmeans'\nn_descriptors : int\n    Number of descriptors per topic\noutput_path : Optional[str]\n    Path to save results (if None, results are not saved)\nrandom_state : int\n    Random state for reproducibility\nignore_n_closest_keywords : int\n    Number of additional closest keywords to skip after applying distance filter.\n    This skips keywords beyond those already filtered by min_descriptor_distance.\n    Default: 0 (no skipping)\nmin_descriptor_distance : Optional[float]\n    Minimum distance threshold for including keywords in descriptors.\n    Keywords with distance <= this threshold are excluded first, then\n    ignore_n_closest_keywords additional keywords are skipped from the remaining.\n    Default: None (no filtering)\n**kwargs\n    Additional arguments for the clustering algorithm\n\nReturns\n-------\nTuple[KeywordClusterer, Dict[int, List[str]]]\n    Tuple of (fitted clusterer, topic descriptors)\n\nExamples\n--------\n>>> from RAG_supporters.embeddings import KeywordEmbedder\n>>> embedder = KeywordEmbedder()\n>>> keywords = [\"machine learning\", \"deep learning\", \"neural networks\"]\n>>> embeddings = embedder.create_embeddings(keywords)\n>>> clusterer, topics = cluster_keywords_from_embeddings(\n...     embeddings,\n...     n_clusters=2,\n...     n_descriptors=5,\n...     output_path=\"results/keyword_clusters.json\"\n... )\n>>> print(f\"Found {len(topics)} topics\")\nFound 2 topics\n\n>>> # With filtering options\n>>> clusterer, topics = cluster_keywords_from_embeddings(\n...     embeddings,\n...     n_clusters=2,\n...     n_descriptors=5,\n...     ignore_n_closest_keywords=1,\n...     min_descriptor_distance=0.1\n... )"
    }
  },
  {
    "path": "RAG_supporters/clustering/topic_distance_calculator.py",
    "file": "topic_distance_calculator.py",
    "module": "topic_distance_calculator",
    "parent_module": "clustering",
    "package": "RAG_supporters",
    "module_docstring": "Utility function for calculating embedding distances to topic keywords.\n\nThis module provides functionality to process CSV files with question_text and source_text fields,\ncalculating embedding distances to topic keywords from KeywordClusterer result JSON.\nUnlike TOPIC_RELEVANCE_PROB, this does not use LLM but directly computes embedding distances.",
    "classes": {
      "TopicDistanceCalculator": {
        "docstring": "Calculate embedding distances between text and topic keywords.\n\nThis class processes CSV files containing question_text and source_text fields,\ncomputing distances to topic keywords from KeywordClusterer JSON files.\nSupports fetching embeddings from database by ID or using provided embeddings.",
        "methods": {
          "__init__": "Initialize the topic distance calculator.\n\nParameters\n----------\nkeyword_clusterer_json : Union[str, Path, Dict]\n    Path to KeywordClusterer JSON file or loaded dict with centroids and topic_descriptors\nembedder : Optional[Any]\n    KeywordEmbedder instance for encoding text.\n    Required if text needs to be embedded (not fetching from database).\n    If not a KeywordEmbedder, will be wrapped automatically.\nmetric : str\n    Distance metric: \"cosine\" or \"euclidean\". Default is \"cosine\".\nenable_cache : bool\n    Whether to cache embeddings for repeated texts. Default is True.\n    Disable for memory-constrained environments or unique texts.",
          "clear_cache": "Clear the embedding cache.\n\nReturns\n-------\nint\n    Number of cached entries that were cleared",
          "cache_size": "Get the current size of the embedding cache.\n\nReturns\n-------\nint\n    Number of cached embeddings",
          "cache_info": "Get detailed information about the embedding cache.\n\nReturns\n-------\nDict[str, Any]\n    Dictionary with cache statistics including size and enabled status",
          "_load_centroids_and_topics": "Load centroids and topic descriptors from KeywordClusterer data.",
          "_create_distance_json_mapping": "Create JSON mapping of topic descriptors to their distances.\n\nFor each cluster, assigns the cluster's distance to all its topic descriptors.\nThis creates a {topic_descriptor: distance} mapping similar to the format\nused by DomainAssessmentAgent's question_term_relevance_scores.\n\nNote: If a topic descriptor appears in multiple clusters, the minimum distance\nis retained (closest cluster wins).\n\nParameters\n----------\ndistances : np.ndarray\n    Array of distances to each cluster centroid, shape (n_clusters,)\n\nReturns\n-------\nstr\n    JSON string mapping topic_descriptor to distance value",
          "_compute_distances_to_centroids": "Compute distances from embedding to all centroids.\n\nParameters\n----------\nembedding : np.ndarray\n    Input embedding vector, shape (embedding_dim,)\n\nReturns\n-------\nnp.ndarray\n    Distances to each centroid, shape (n_clusters,)",
          "_get_embedding_from_text": "Get embedding for text using KeywordEmbedder.\n\nUses caching to avoid re-embedding the same text multiple times.\n\nParameters\n----------\ntext : str\n    Text to embed\n\nReturns\n-------\nnp.ndarray\n    Embedding vector",
          "_get_embeddings_batch": "Get embeddings for multiple texts in batch using KeywordEmbedder.\n\nUses caching and batch processing for efficiency.\n\nParameters\n----------\ntexts : list\n    List of texts to embed\n\nReturns\n-------\nDict[str, np.ndarray]\n    Dictionary mapping original text to embedding vector",
          "_get_embedding_from_database": "Fetch embedding from database by ID.\n\nParameters\n----------\nitem_id : str\n    ID of the item in the database\ndatabase : Any\n    Database object with get_embedding method or similar interface\ncollection_name : str\n    Collection name (\"questions\" or \"sources\")\n\nReturns\n-------\nOptional[np.ndarray]\n    Embedding vector or None if not found",
          "calculate_distances_for_csv": "Calculate embedding distances to topic keywords for CSV file.\n\nParameters\n----------\ncsv_path : Union[str, Path]\n    Path to input CSV file with question_text and source_text columns\nquestion_col : str\n    Column name for question text. Default is \"question_text\".\nsource_col : str\n    Column name for source text. Default is \"source_text\".\nquestion_id_col : Optional[str]\n    Column name for question IDs (for database lookup). If provided, fetches embeddings from database.\nsource_id_col : Optional[str]\n    Column name for source IDs (for database lookup). If provided, fetches embeddings from database.\ndatabase : Optional[Any]\n    Database object for fetching embeddings by ID. Required if using ID columns.\noutput_path : Optional[Union[str, Path]]\n    Path to save output CSV. If None, doesn't save.\nshow_progress : bool\n    Show progress bar. Default is True.\nsave_on_interrupt : bool\n    If True, saves partial results when interrupted with KeyboardInterrupt.\n    The whole original DataFrame is saved with distance scores filled only for\n    processed rows (unprocessed rows have None/NaN).\n    Results are saved to output_path (overwrites if exists).\n    The function returns the partial DataFrame instead of raising the interrupt.\n    Default is True.\n\nReturns\n-------\npd.DataFrame\n    DataFrame with added columns for distances to each topic cluster:\n    - question_term_distance_scores: JSON mapping {topic_descriptor: distance} for question\n    - source_term_distance_scores: JSON mapping {topic_descriptor: distance} for source"
        }
      }
    },
    "functions": {
      "calculate_topic_distances_from_csv": "Calculate topic distances from CSV file.\n\nThis function processes a CSV file with question_text and source_text fields,\ncalculating embedding distances to topic keywords from KeywordClusterer JSON.\nUnlike TOPIC_RELEVANCE_PROB agent method, this uses direct embedding distance\ncalculation without LLM.\n\nParameters\n----------\ncsv_path : Union[str, Path]\n    Path to input CSV file\nkeyword_clusterer_json : Union[str, Path, Dict]\n    Path to KeywordClusterer JSON file or loaded dict\nembedder : Optional[Any]\n    KeywordEmbedder instance for encoding text. Required if not using database IDs.\nquestion_col : str\n    Column name for question text. Default is \"question_text\".\nsource_col : str\n    Column name for source text. Default is \"source_text\".\nquestion_id_col : Optional[str]\n    Column name for question IDs (for database lookup)\nsource_id_col : Optional[str]\n    Column name for source IDs (for database lookup)\ndatabase : Optional[Any]\n    Database object for fetching embeddings by ID\nmetric : str\n    Distance metric: \"cosine\" or \"euclidean\". Default is \"cosine\".\nenable_cache : bool\n    Whether to cache embeddings for repeated texts. Default is True.\n    Disable for memory-constrained environments or unique texts.\noutput_path : Optional[Union[str, Path]]\n    Path to save output CSV\nshow_progress : bool\n    Show progress bar. Default is True.\nsave_on_interrupt : bool\n    If True, saves partial results when interrupted with KeyboardInterrupt.\n    The whole original DataFrame is saved with distance scores filled only for\n    processed rows (unprocessed rows have None/NaN).\n    Results are saved to output_path (overwrites if exists).\n    The function returns the partial DataFrame instead of raising the interrupt.\n    Default is True.\n\nReturns\n-------\npd.DataFrame\n    DataFrame with added distance columns:\n    - question_term_distance_scores: JSON mapping {topic_descriptor: distance} for question\n    - source_term_distance_scores: JSON mapping {topic_descriptor: distance} for source\n\nExamples\n--------\n>>> from RAG_supporters.embeddings import KeywordEmbedder\n>>> from RAG_supporters.clustering.topic_distance_calculator import (\n...     calculate_topic_distances_from_csv,\n... )\n>>>\n>>> # Using text embedding\n>>> embedder = KeywordEmbedder()\n>>> result_df = calculate_topic_distances_from_csv(\n...     csv_path=\"data.csv\",\n...     keyword_clusterer_json=\"clusters.json\",\n...     embedder=embedder,\n...     output_path=\"results.csv\"\n... )\n>>>\n>>> # Using database IDs\n>>> result_df = calculate_topic_distances_from_csv(\n...     csv_path=\"data.csv\",\n...     keyword_clusterer_json=\"clusters.json\",\n...     question_id_col=\"question_id\",\n...     source_id_col=\"source_id\",\n...     database=my_database,\n...     output_path=\"results.csv\"\n... )"
    }
  },
  {
    "path": "RAG_supporters/clustering_ops/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "clustering_ops",
    "package": "RAG_supporters",
    "module_docstring": "Clustering operations for keyword-based cluster assignment.\n\nThis module provides tools for parsing cluster metadata from KeywordClusterer\nand linking data items to clusters via keyword matching.\n\nKey Features:\n- Parse KeywordClusterer JSON format\n- Keyword-to-cluster matching (exact and fuzzy)\n- Source-cluster linking via majority voting\n- Fallback strategies for unmatched items\n- Cluster coverage statistics\n\nExamples\n--------\n>>> from RAG_supporters.clustering_ops import ClusterParser, SourceClusterLinker\n>>>\n>>> # Parse cluster metadata\n>>> parser = ClusterParser(\"clusters.json\")\n>>> cluster_id = parser.match_keyword(\"machine learning\")\n>>>\n>>> # Link pairs to clusters\n>>> linker = SourceClusterLinker(parser)\n>>> df_linked = linker.link_dataframe(df, keywords_col=\"keywords\")\n>>> print(df_linked[[\"pair_id\", \"cluster_id\"]].head())",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/clustering_ops/link_sources.py",
    "file": "link_sources.py",
    "module": "link_sources",
    "parent_module": "clustering_ops",
    "package": "RAG_supporters",
    "module_docstring": "Source-Cluster Linker for JASPER Steering Dataset Builder.\n\nThis module links sources to clusters via keywords from KeywordClusterer JSON.\nResolves primary cluster assignments for each question-source pair based on\nkeyword overlap and cluster memberships.\n\nKey Features:\n- Link pairs to clusters via keyword intersection\n- Resolve primary cluster using majority voting\n- Handle pairs with no keyword matches (fallback to closest cluster)\n- Validate cluster assignments for dataset integrity",
    "classes": {
      "SourceClusterLinker": {
        "docstring": "Link question-source pairs to clusters via keywords.\n\nThis class assigns each pair to a primary cluster based on keyword\noverlap with cluster assignments from KeywordClusterer. Uses majority\nvoting when multiple clusters are present, with fallback strategies\nfor pairs without keyword matches.\n\nParameters\n----------\ncluster_parser : ClusterParser\n    Parser for cluster metadata and keyword-cluster mappings\nfallback_strategy : str, optional\n    Strategy for pairs without keyword matches:\n    - \"random\": Assign to random cluster\n    - \"largest\": Assign to largest cluster\n    - \"uniform\": Distribute uniformly across clusters\n    Default: \"largest\"\n\nAttributes\n----------\ncluster_parser : ClusterParser\n    Cluster metadata parser\nn_clusters : int\n    Number of clusters from KeywordClusterer\nfallback_strategy : str\n    Strategy for handling unmatched pairs\n\nExamples\n--------\n>>> from RAG_supporters.dataset import ClusterParser, SourceClusterLinker\n>>> parser = ClusterParser(\"clusters.json\")\n>>> linker = SourceClusterLinker(parser)\n>>>\n>>> # Link single pair\n>>> keywords = [\"machine learning\", \"neural networks\"]\n>>> cluster_id = linker.link_pair(keywords)\n>>> print(f\"Primary cluster: {cluster_id}\")\n>>>\n>>> # Link DataFrame of pairs\n>>> df = pd.read_csv(\"merged.csv\")\n>>> cluster_assignments = linker.link_dataframe(df, keywords_col=\"keywords\")\n>>> print(f\"Assigned {len(cluster_assignments)} pairs to clusters\")",
        "methods": {
          "__init__": "Initialize source-cluster linker.",
          "_compute_cluster_sizes": "Compute number of keywords per cluster.\n\nReturns\n-------\nDict[int, int]\n    Mapping from cluster ID to number of keywords",
          "_get_fallback_cluster": "Get fallback cluster ID for pairs without keyword matches.\n\nParameters\n----------\npair_id : int, optional\n    Pair ID for reproducible uniform assignment\n\nReturns\n-------\nint\n    Fallback cluster ID",
          "link_pair": "Link a single pair to its primary cluster.\n\nUses majority voting: cluster with most keyword matches wins.\nIf no keywords match, uses fallback strategy.\n\nParameters\n----------\nkeywords : List[str]\n    List of keywords associated with the pair\npair_id : int, optional\n    Pair ID for reproducible fallback assignment\n\nReturns\n-------\nint\n    Primary cluster ID (0 to n_clusters-1)\n\nExamples\n--------\n>>> linker = SourceClusterLinker(parser)\n>>> cluster_id = linker.link_pair([\"python\", \"programming\", \"data science\"])\n>>> print(cluster_id)\n1",
          "link_batch": "Link multiple pairs to clusters in batch.\n\nParameters\n----------\nkeywords_list : List[List[str]]\n    List of keyword lists, one per pair\npair_ids : List[int], optional\n    List of pair IDs for reproducible fallback assignment\n\nReturns\n-------\nList[int]\n    List of primary cluster IDs\n\nExamples\n--------\n>>> linker = SourceClusterLinker(parser)\n>>> keywords_list = [\n...     [\"python\", \"programming\"],\n...     [\"database\", \"sql\"],\n...     [\"machine learning\"]\n... ]\n>>> cluster_ids = linker.link_batch(keywords_list)\n>>> print(cluster_ids)\n[1, 2, 0]",
          "link_dataframe": "Link all pairs in DataFrame to clusters.\n\nAdds a new column with primary cluster assignments.\n\nParameters\n----------\ndf : pd.DataFrame\n    DataFrame with pairs and keywords\nkeywords_col : str, optional\n    Column name containing keyword lists (default: \"keywords\")\npair_id_col : str, optional\n    Column name containing pair IDs (default: \"pair_id\")\noutput_col : str, optional\n    Output column name for cluster assignments (default: \"cluster_id\")\nshow_progress : bool, optional\n    Show progress bar (default: True)\n\nReturns\n-------\npd.DataFrame\n    DataFrame with cluster_id column added\n\nExamples\n--------\n>>> linker = SourceClusterLinker(parser)\n>>> df = pd.read_csv(\"merged.csv\")\n>>> df_linked = linker.link_dataframe(df)\n>>> print(df_linked[[\"pair_id\", \"keywords\", \"cluster_id\"]].head())",
          "validate_assignments": "Validate cluster assignments for dataset integrity.\n\nChecks:\n- All cluster IDs are in valid range [0, n_clusters-1]\n- All clusters have at least one assignment\n- Distribution is not too imbalanced\n\nParameters\n----------\ncluster_assignments : List[int] or np.ndarray or torch.Tensor\n    Array of cluster assignments\n\nReturns\n-------\nDict[str, any]\n    Validation results with keys:\n    - \"valid\": bool - Whether assignments are valid\n    - \"errors\": List[str] - List of validation errors\n    - \"warnings\": List[str] - List of validation warnings\n    - \"statistics\": Dict - Assignment statistics\n\nExamples\n--------\n>>> linker = SourceClusterLinker(parser)\n>>> cluster_ids = [0, 0, 1, 1, 2, 2]\n>>> validation = linker.validate_assignments(cluster_ids)\n>>> if validation[\"valid\"]:\n...     print(\"Assignments are valid!\")\n>>> else:\n...     print(\"Errors:\", validation[\"errors\"])"
        }
      }
    },
    "functions": {
      "link_sources": "Convenience function to link pairs to clusters.\n\nParameters\n----------\ndf : pd.DataFrame\n    DataFrame with question-source pairs and keywords\ncluster_parser : ClusterParser\n    Parser for cluster metadata\nkeywords_col : str, optional\n    Column name containing keyword lists (default: \"keywords\")\npair_id_col : str, optional\n    Column name containing pair IDs (default: \"pair_id\")\noutput_col : str, optional\n    Output column name for cluster assignments (default: \"cluster_id\")\nfallback_strategy : str, optional\n    Strategy for pairs without keyword matches (default: \"largest\")\nshow_progress : bool, optional\n    Show progress bar (default: True)\n\nReturns\n-------\npd.DataFrame\n    DataFrame with cluster_id column added\n\nExamples\n--------\n>>> from RAG_supporters.dataset import ClusterParser, link_sources\n>>> import pandas as pd\n>>>\n>>> parser = ClusterParser(\"clusters.json\")\n>>> df = pd.read_csv(\"merged.csv\")\n>>> df_linked = link_sources(df, parser)\n>>> print(df_linked[[\"pair_id\", \"cluster_id\"]].head())"
    }
  },
  {
    "path": "RAG_supporters/clustering_ops/parse_clusters.py",
    "file": "parse_clusters.py",
    "module": "parse_clusters",
    "parent_module": "clustering_ops",
    "package": "RAG_supporters",
    "module_docstring": "Cluster Parser for JASPER Steering Dataset Builder.\n\nThis module parses KeywordClusterer JSON format and provides keyword matching\nwith exact string matching and cosine similarity fallback.\n\nKey Features:\n- Parse cluster metadata: assignments, centroids, topic descriptors, embeddings\n- Exact keyword matching (case-insensitive)\n- Cosine similarity fallback for fuzzy matching\n- Normalize keyword text for robust matching\n- Integration with ClusteringData from clustering module",
    "classes": {
      "ClusterParser": {
        "docstring": "Parse KeywordClusterer JSON and match keywords to clusters.\n\nThis parser loads cluster metadata from KeywordClusterer JSON format\nand provides keyword-to-cluster matching with exact and fuzzy (cosine)\nfallback strategies.\n\nParameters\n----------\nclustering_json_path : str or Path\n    Path to KeywordClusterer JSON results file (from KeywordClusterer.save_results())\ncosine_threshold : float, optional\n    Minimum cosine similarity for fuzzy keyword matching (default: 0.7)\n    Only used when exact match fails and embeddings are available\n\nAttributes\n----------\nclustering_data : ClusteringData\n    Parsed cluster metadata container\nkeyword_to_cluster : Dict[str, int]\n    Mapping from normalized keywords to cluster IDs\nkeyword_embeddings : Dict[str, np.ndarray], optional\n    Keyword embeddings if available in JSON\ncluster_keywords : Dict[int, List[str]]\n    Mapping from cluster ID to list of keywords\n\nExamples\n--------\n>>> parser = ClusterParser(\"clusters.json\")\n>>> cluster_id = parser.match_keyword(\"machine learning\")\n>>> print(f\"Matched to cluster {cluster_id}\")\n>>>\n>>> # Fuzzy matching with cosine fallback\n>>> cluster_id, similarity = parser.match_keyword_fuzzy(\"ml algorithms\")\n>>> if cluster_id is not None:\n...     print(f\"Fuzzy matched to cluster {cluster_id} (similarity={similarity:.3f})\")\n>>>\n>>> # Get cluster info\n>>> descriptors = parser.get_cluster_descriptors(cluster_id)\n>>> centroid = parser.get_centroid(cluster_id)",
        "methods": {
          "__init__": "Initialize cluster parser.",
          "_normalize_keyword": "Normalize keyword for matching.\n\nParameters\n----------\nkeyword : str\n    Raw keyword text\n\nReturns\n-------\nstr\n    Normalized keyword (lowercase, stripped)",
          "n_clusters": "Return number of clusters from loaded clustering metadata.",
          "_build_keyword_mapping": "Build normalized keyword-to-cluster mapping.\n\nReturns\n-------\nDict[str, int]\n    Mapping from normalized keywords to cluster IDs",
          "_build_cluster_keywords": "Build cluster-to-keywords mapping.\n\nReturns\n-------\nDict[int, List[str]]\n    Mapping from cluster ID to list of keywords",
          "_load_keyword_embeddings": "Load keyword embeddings if available in JSON.\n\nReturns\n-------\nDict[str, np.ndarray] or None\n    Mapping from normalized keywords to embeddings, or None if not available",
          "match_keyword": "Match keyword to cluster using exact matching.\n\nPerforms case-insensitive exact matching.\n\nParameters\n----------\nkeyword : str\n    Keyword to match\n\nReturns\n-------\nint or None\n    Cluster ID if matched, None otherwise\n\nExamples\n--------\n>>> parser = ClusterParser(\"clusters.json\")\n>>> cluster_id = parser.match_keyword(\"Machine Learning\")\n>>> print(cluster_id)\n3",
          "match_keyword_fuzzy": "Match keyword to cluster with cosine similarity fallback.\n\nFirst attempts exact matching. If exact match fails and embeddings\nare available, falls back to cosine similarity matching.\n\nParameters\n----------\nkeyword : str\n    Keyword to match\nreturn_similarity : bool, optional\n    If True, return (cluster_id, similarity). If False, return cluster_id only.\n    Default: True\n\nReturns\n-------\nTuple[int or None, float] or int or None\n    If return_similarity=True: (cluster_id, similarity_score)\n    If return_similarity=False: cluster_id only\n    Returns (None, 0.0) or None if no match found\n\nExamples\n--------\n>>> parser = ClusterParser(\"clusters.json\")\n>>> cluster_id, similarity = parser.match_keyword_fuzzy(\"ML models\")\n>>> if cluster_id is not None:\n...     print(f\"Matched to cluster {cluster_id} (sim={similarity:.3f})\")",
          "match_keywords_batch": "Match multiple keywords to clusters (exact matching).\n\nParameters\n----------\nkeywords : List[str]\n    List of keywords to match\n\nReturns\n-------\nList[int or None]\n    List of cluster IDs (None for unmatched keywords)\n\nExamples\n--------\n>>> parser = ClusterParser(\"clusters.json\")\n>>> keywords = [\"machine learning\", \"deep learning\", \"unknown term\"]\n>>> cluster_ids = parser.match_keywords_batch(keywords)\n>>> print(cluster_ids)\n[3, 3, None]",
          "get_cluster_descriptors": "Get topic descriptors for a cluster.\n\nParameters\n----------\ncluster_id : int\n    Cluster ID\n\nReturns\n-------\nList[str] or None\n    Topic descriptor keywords, or None if not available\n\nExamples\n--------\n>>> parser = ClusterParser(\"clusters.json\")\n>>> descriptors = parser.get_cluster_descriptors(3)\n>>> print(descriptors)\n['machine learning', 'neural networks', 'AI']",
          "get_centroid": "Get centroid embedding for a cluster.\n\nParameters\n----------\ncluster_id : int\n    Cluster ID\n\nReturns\n-------\nnp.ndarray or None\n    Centroid embedding vector, or None if not available\n\nExamples\n--------\n>>> parser = ClusterParser(\"clusters.json\")\n>>> centroid = parser.get_centroid(3)\n>>> print(centroid.shape)\n(384,)",
          "get_all_centroids": "Get all cluster centroids.\n\nReturns\n-------\nnp.ndarray or None\n    Array of shape (n_clusters, embedding_dim) containing all centroids,\n    or None if centroids are not available\n\nExamples\n--------\n>>> parser = ClusterParser(\"clusters.json\")\n>>> centroids = parser.get_all_centroids()\n>>> print(centroids.shape)\n(20, 384)",
          "get_cluster_keywords": "Get all keywords assigned to a cluster.\n\nParameters\n----------\ncluster_id : int\n    Cluster ID\n\nReturns\n-------\nList[str] or None\n    List of keywords in cluster, or None if cluster not found\n\nExamples\n--------\n>>> parser = ClusterParser(\"clusters.json\")\n>>> keywords = parser.get_cluster_keywords(3)\n>>> print(len(keywords))\n45",
          "get_all_keywords": "Get all unique keywords across all clusters.\n\nReturns\n-------\nList[str]\n    List of all unique keywords (normalized)",
          "get_clusters_for_keywords": "Get cluster assignments for multiple keywords.\n\nParameters\n----------\nkeywords : List[str]\n    List of keywords to look up\nignore_missing : bool, optional\n    If True, include None values for unmatched keywords.\n    If False, only return matched keywords.\n    Default: True\n\nReturns\n-------\nDict[str, int or None]\n    Mapping from keywords to cluster IDs\n\nExamples\n--------\n>>> parser = ClusterParser(\"clusters.json\")\n>>> keywords = [\"machine learning\", \"deep learning\", \"unknown\"]\n>>> assignments = parser.get_clusters_for_keywords(keywords)\n>>> print(assignments)\n{'machine learning': 3, 'deep learning': 3, 'unknown': None}",
          "compute_cluster_coverage": "Compute cluster coverage statistics for a keyword list.\n\nParameters\n----------\nkeywords : List[str]\n    List of keywords to analyze\n\nReturns\n-------\nTuple[int, int, float, Set[int]]\n    (matched_count, total_count, coverage_ratio, covered_clusters)\n\nExamples\n--------\n>>> parser = ClusterParser(\"clusters.json\")\n>>> keywords = [\"ML\", \"deep learning\", \"NLP\", \"unknown\"]\n>>> matched, total, ratio, clusters = parser.compute_cluster_coverage(keywords)\n>>> print(f\"Coverage: {matched}/{total} = {ratio:.2%}\")\nCoverage: 3/4 = 75.00%\n>>> print(f\"Covered clusters: {sorted(clusters)}\")\nCovered clusters: [1, 3, 5]",
          "validate": "Validate cluster parser state.\n\nRaises\n------\nValueError\n    If validation fails",
          "get_metadata": "Get cluster metadata for config.json.\n\nReturns\n-------\nDict[str, any]\n    Metadata dictionary with:\n    - n_clusters: Number of clusters\n    - n_keywords: Number of unique keywords\n    - embedding_dim: Embedding dimension\n    - clustering_source: Path to JSON file\n\nExamples\n--------\n>>> parser = ClusterParser(\"clusters.json\")\n>>> metadata = parser.get_metadata()\n>>> print(metadata['n_clusters'])\n20"
        }
      }
    },
    "functions": {
      "parse_clusters": "Convenience function to parse KeywordClusterer JSON.\n\nParameters\n----------\nclustering_json_path : str or Path\n    Path to KeywordClusterer JSON results file\nvalidate : bool, optional\n    Whether to run validation checks (default: True)\ncosine_threshold : float, optional\n    Minimum cosine similarity for fuzzy matching (default: 0.7)\n\nReturns\n-------\nClusterParser\n    Initialized parser with loaded cluster metadata\n\nExamples\n--------\n>>> parser = parse_clusters(\"clusters.json\")\n>>> cluster_id = parser.match_keyword(\"machine learning\")"
    }
  },
  {
    "path": "RAG_supporters/contrastive/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "contrastive",
    "package": "RAG_supporters",
    "module_docstring": "Contrastive learning tools for hard negative mining and steering signals.\n\nThis module provides highly reusable tools for contrastive learning:\n- Hard negative mining with 4-tier sampling strategy\n- Steering signal generation for curriculum learning\n- Distance-based negative sampling\n- Centroid-based steering computation\n\nThese tools can be used for any contrastive learning project including metric learning,\nsiamese networks, and triplet loss training.\n\nKey Features:\n- 4-tier negative sampling (in-cluster, adjacent, hard, random)\n- Centroid steering (normalized direction to cluster center)\n- Keyword-weighted steering (weighted average of keywords)\n- Residual steering (off-center signals)\n- Curriculum learning support via distance metrics\n\nExamples\n--------\n>>> from RAG_supporters.contrastive import NegativeMiner, SteeringBuilder\n>>>\n>>> # Mine hard negatives\n>>> miner = NegativeMiner(\n...     source_embeddings=source_embs,\n...     question_embeddings=question_embs,\n...     centroid_embeddings=centroid_embs,\n...     pair_indices=pair_indices,\n...     pair_cluster_ids=pair_cluster_ids,\n...     source_cluster_ids=source_cluster_ids,\n...     n_neg=12,\n...     tier_proportions=[3, 4, 3, 2]\n... )\n>>> results = miner.mine_all_negatives()\n>>>\n>>> # Build steering signals\n>>> builder = SteeringBuilder(\n...     question_embeddings=q_embs,\n...     keyword_embeddings=k_embs,\n...     centroid_embeddings=c_embs,\n...     pair_indices=indices,\n...     pair_cluster_ids=cluster_ids,\n...     pair_keyword_ids=keyword_ids\n... )\n>>> steering_results = builder.build_all_steering()",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/contrastive/build_steering.py",
    "file": "build_steering.py",
    "module": "build_steering",
    "parent_module": "contrastive",
    "package": "RAG_supporters",
    "module_docstring": "Steering Signal Builder for JASPER Steering Dataset.\n\nThis module generates steering signals for curriculum learning:\n- Centroid steering: Direction from question to cluster centroid\n- Keyword-weighted steering: Weighted average of pair keywords\n- Residual steering: Residual from question to centroid\n- Centroid distances: Cosine distance for curriculum scheduling\n\nKey Features:\n- Three steering variants for diverse training strategies\n- Normalized steering vectors (unit length)\n- Centroid distance computation for curriculum learning\n- Validation of steering vector properties\n- Support for pairs with no keywords (fallback strategies)",
    "classes": {
      "SteeringBuilder": {
        "docstring": "Build steering signals for curriculum learning.\n\nThis class generates three types of steering signals:\n1. Centroid steering: Normalized direction from question to cluster centroid\n2. Keyword-weighted: Normalized weighted average of keyword embeddings\n3. Residual: Difference between question and centroid (off-center signal)\n\nAdditionally computes centroid distances for curriculum scheduling.\n\nParameters\n----------\nquestion_embeddings : torch.Tensor\n    Question embeddings [n_questions, dim]\nkeyword_embeddings : torch.Tensor\n    Keyword embeddings [n_keywords, dim]\ncentroid_embeddings : torch.Tensor\n    Cluster centroid embeddings [n_clusters, dim]\npair_indices : torch.Tensor\n    Pair indices [n_pairs, 2] mapping to (question_idx, source_idx)\npair_cluster_ids : torch.Tensor\n    Primary cluster ID per pair [n_pairs]\npair_keyword_ids : List[List[int]]\n    Keyword IDs associated with each pair\nnormalize_residual : bool, optional\n    Whether to normalize residual steering vectors (default: False)\nfallback_strategy : str, optional\n    How to handle pairs with no keywords:\n    - \"centroid\": Use centroid steering only\n    - \"zero\": Use zero vector\n    - \"random\": Use random unit vector\n    Default: \"centroid\"\nshow_progress : bool, optional\n    Show progress bars (default: True)\n\nAttributes\n----------\nquestion_embeddings : torch.Tensor\n    Question embeddings\nkeyword_embeddings : torch.Tensor\n    Keyword embeddings\ncentroid_embeddings : torch.Tensor\n    Centroid embeddings\npair_indices : torch.Tensor\n    Pair index mapping\npair_cluster_ids : torch.Tensor\n    Cluster assignments\npair_keyword_ids : List[List[int]]\n    Keyword associations\nn_pairs : int\n    Total number of pairs\nembedding_dim : int\n    Embedding dimension\nnormalize_residual : bool\n    Whether to normalize residuals\nfallback_strategy : str\n    Keyword fallback strategy\nshow_progress : bool\n    Progress bar visibility\n\nExamples\n--------\n>>> import torch\n>>> from RAG_supporters.dataset import SteeringBuilder\n>>>\n>>> # Prepare embeddings\n>>> question_embs = torch.randn(100, 384)\n>>> keyword_embs = torch.randn(50, 384)\n>>> centroid_embs = torch.randn(5, 384)\n>>> pair_indices = torch.randint(0, 100, (500, 2))\n>>> pair_cluster_ids = torch.randint(0, 5, (500,))\n>>> pair_keyword_ids = [[0, 1, 2], [3, 4], ...]  # Variable length\n>>>\n>>> # Build steering signals\n>>> builder = SteeringBuilder(\n...     question_embeddings=question_embs,\n...     keyword_embeddings=keyword_embs,\n...     centroid_embeddings=centroid_embs,\n...     pair_indices=pair_indices,\n...     pair_cluster_ids=pair_cluster_ids,\n...     pair_keyword_ids=pair_keyword_ids\n... )\n>>>\n>>> # Generate all steering signals\n>>> results = builder.build_all_steering()\n>>> print(results.keys())\ndict_keys(['centroid', 'keyword_weighted', 'residual', 'distances'])",
        "methods": {
          "__init__": "Initialize steering builder.",
          "_validate_inputs": "Validate input tensors and shapes.",
          "build_centroid_steering": "Build centroid-based steering vectors and distances.\n\nFor each pair, computes:\n- Steering vector: Normalized direction from question to cluster centroid\n- Distance: Cosine distance to centroid (1 - cosine_similarity)\n\nReturns\n-------\nTuple[torch.Tensor, torch.Tensor]\n    (steering_vectors, centroid_distances)\n    - steering_vectors: [n_pairs, dim], normalized to unit length\n    - centroid_distances: [n_pairs], cosine distance in [0, 2]\n\nExamples\n--------\n>>> steering, distances = builder.build_centroid_steering()\n>>> print(steering.shape, distances.shape)\ntorch.Size([500, 384]) torch.Size([500])\n>>> print(f\"Min distance: {distances.min():.3f}\")\n>>> print(f\"Max distance: {distances.max():.3f}\")",
          "build_keyword_weighted_steering": "Build keyword-weighted steering vectors.\n\nFor each pair, computes weighted average of keyword embeddings\nassociated with the pair, normalized to unit length.\n\nFor pairs with no keywords, uses fallback strategy:\n- \"centroid\": Use centroid steering\n- \"zero\": Use zero vector\n- \"random\": Use random unit vector\n\nReturns\n-------\ntorch.Tensor\n    Keyword-weighted steering vectors [n_pairs, dim]\n\nExamples\n--------\n>>> steering = builder.build_keyword_weighted_steering()\n>>> print(steering.shape)\ntorch.Size([500, 384])\n>>> # Check normalization\n>>> norms = torch.norm(steering, dim=1)\n>>> print(f\"All unit vectors: {torch.allclose(norms, torch.ones_like(norms), atol=1e-6)}\")",
          "build_residual_steering": "Build residual steering vectors.\n\nFor each pair, computes residual between question embedding and\ncluster centroid. This captures off-center signals that can be\nuseful for curriculum learning.\n\nReturns\n-------\ntorch.Tensor\n    Residual steering vectors [n_pairs, dim]\n    Normalized to unit length if normalize_residual=True\n\nExamples\n--------\n>>> steering = builder.build_residual_steering()\n>>> print(steering.shape)\ntorch.Size([500, 384])",
          "build_all_steering": "Build all steering variants and distances.\n\nConvenience method that generates all steering signals in one call.\n\nReturns\n-------\nDict[str, torch.Tensor]\n    Dictionary with keys:\n    - \"centroid\": Centroid steering [n_pairs, dim]\n    - \"keyword_weighted\": Keyword steering [n_pairs, dim]\n    - \"residual\": Residual steering [n_pairs, dim]\n    - \"distances\": Centroid distances [n_pairs]\n\nExamples\n--------\n>>> results = builder.build_all_steering()\n>>> for key, tensor in results.items():\n...     print(f\"{key}: {tensor.shape}\")\ncentroid: torch.Size([500, 384])\nkeyword_weighted: torch.Size([500, 384])\nresidual: torch.Size([500, 384])\ndistances: torch.Size([500])",
          "_get_fallback_steering": "Get fallback steering for pairs with no keywords.\n\nParameters\n----------\npair_idx : int\n    Index of the pair\n\nReturns\n-------\ntorch.Tensor\n    Fallback steering vector [dim]",
          "_validate_steering_vectors": "Validate steering vector properties.\n\nParameters\n----------\nsteering : torch.Tensor\n    Steering vectors to validate\nvariant : str\n    Variant name (for error messages)\nallow_zero : bool, optional\n    Whether to allow zero vectors (default: False)",
          "_validate_distances": "Validate centroid distances.\n\nParameters\n----------\ndistances : torch.Tensor\n    Distances to validate",
          "save": "Save steering tensors to files.\n\nParameters\n----------\noutput_dir : str or Path\n    Output directory for tensor files\nsteering_results : Dict[str, torch.Tensor], optional\n    Pre-computed steering results (if None, will compute)\n\nExamples\n--------\n>>> builder.save(\"output_dir/\")\n>>> # Or save pre-computed results\n>>> results = builder.build_all_steering()\n>>> builder.save(\"output_dir/\", steering_results=results)"
        }
      }
    },
    "functions": {
      "build_steering": "Build and save steering signals (convenience function).\n\nParameters\n----------\nquestion_embeddings : torch.Tensor\n    Question embeddings [n_questions, dim]\nkeyword_embeddings : torch.Tensor\n    Keyword embeddings [n_keywords, dim]\ncentroid_embeddings : torch.Tensor\n    Cluster centroid embeddings [n_clusters, dim]\npair_indices : torch.Tensor\n    Pair indices [n_pairs, 2]\npair_cluster_ids : torch.Tensor\n    Cluster IDs per pair [n_pairs]\npair_keyword_ids : List[List[int]]\n    Keyword IDs per pair\noutput_dir : str or Path\n    Output directory for saved tensors\nnormalize_residual : bool, optional\n    Normalize residual vectors (default: False)\nfallback_strategy : str, optional\n    Keyword fallback strategy (default: \"centroid\")\nshow_progress : bool, optional\n    Show progress bars (default: True)\n\nReturns\n-------\nDict[str, torch.Tensor]\n    Dictionary with steering variants and distances\n\nExamples\n--------\n>>> from RAG_supporters.dataset import build_steering\n>>> results = build_steering(\n...     question_embeddings=q_embs,\n...     keyword_embeddings=k_embs,\n...     centroid_embeddings=c_embs,\n...     pair_indices=indices,\n...     pair_cluster_ids=cluster_ids,\n...     pair_keyword_ids=keyword_ids,\n...     output_dir=\"dataset/\"\n... )"
    }
  },
  {
    "path": "RAG_supporters/contrastive/mine_negatives.py",
    "file": "mine_negatives.py",
    "module": "mine_negatives",
    "parent_module": "contrastive",
    "package": "RAG_supporters",
    "module_docstring": "Hard Negative Miner for JASPER Steering Dataset.\n\nThis module mines hard negatives for contrastive learning with 4-tier sampling:\n- Tier 1: In-cluster negatives (same cluster, excluding true source)\n- Tier 2: Adjacent cluster negatives (top-K nearest clusters)\n- Tier 3: High-similarity negatives (highest cosine similarity, wrong clusters)\n- Tier 4: Random distant negatives (uniform random from far clusters)\n\nKey Features:\n- Stratified negative sampling by difficulty tier\n- True source never in own negative set\n- Configurable tier proportions\n- Handles edge cases (small clusters, insufficient negatives)\n- Validation of sampling properties",
    "classes": {
      "NegativeMiner": {
        "docstring": "Mine hard negatives for contrastive learning.\n\nGenerates stratified hard negatives across 4 difficulty tiers:\n1. In-cluster: Same cluster as positive (tests within-cluster discrimination)\n2. Adjacent: Top-K nearest clusters (tests cluster boundaries)\n3. High-similarity: Highest cosine similarity, wrong clusters (hardest negatives)\n4. Random: Uniform random from distant clusters (easy negatives for stability)\n\nParameters\n----------\nsource_embeddings : torch.Tensor\n    Source embeddings [n_sources, dim]\nquestion_embeddings : torch.Tensor\n    Question embeddings [n_questions, dim]\ncentroid_embeddings : torch.Tensor\n    Cluster centroid embeddings [n_clusters, dim]\npair_indices : torch.Tensor\n    Pair indices [n_pairs, 2] mapping to (question_idx, source_idx)\npair_cluster_ids : torch.Tensor\n    Primary cluster ID per pair [n_pairs]\nsource_cluster_ids : torch.Tensor\n    Cluster assignment for each source [n_sources]\nn_neg : int\n    Total number of negatives per pair\ntier_proportions : List[int], optional\n    Number of negatives per tier [tier1, tier2, tier3, tier4].\n    Must sum to n_neg. Default: Equal distribution.\nadjacent_k : int, optional\n    Number of adjacent clusters to consider for Tier 2 (default: 3)\nrandom_seed : int, optional\n    Random seed for reproducibility (default: 42)\nshow_progress : bool, optional\n    Show progress bars (default: True)\n\nAttributes\n----------\nsource_embeddings : torch.Tensor\n    Source embeddings\nquestion_embeddings : torch.Tensor\n    Question embeddings\ncentroid_embeddings : torch.Tensor\n    Centroid embeddings\npair_indices : torch.Tensor\n    Pair index mapping\npair_cluster_ids : torch.Tensor\n    Cluster assignments for pairs\nsource_cluster_ids : torch.Tensor\n    Cluster assignments for sources\nn_neg : int\n    Total negatives per pair\ntier_proportions : List[int]\n    Negatives per tier\nadjacent_k : int\n    Number of adjacent clusters\nshow_progress : bool\n    Progress bar visibility\n\nExamples\n--------\n>>> import torch\n>>> from RAG_supporters.dataset import NegativeMiner\n>>>\n>>> # Prepare data\n>>> source_embs = torch.randn(1000, 384)\n>>> question_embs = torch.randn(100, 384)\n>>> centroid_embs = torch.randn(10, 384)\n>>> pair_indices = torch.randint(0, 100, (500, 2))\n>>> pair_cluster_ids = torch.randint(0, 10, (500,))\n>>> source_cluster_ids = torch.randint(0, 10, (1000,))\n>>>\n>>> # Mine negatives\n>>> miner = NegativeMiner(\n...     source_embeddings=source_embs,\n...     question_embeddings=question_embs,\n...     centroid_embeddings=centroid_embs,\n...     pair_indices=pair_indices,\n...     pair_cluster_ids=pair_cluster_ids,\n...     source_cluster_ids=source_cluster_ids,\n...     n_neg=12,\n...     tier_proportions=[3, 4, 3, 2]\n... )\n>>>\n>>> # Generate negatives\n>>> results = miner.mine_all_negatives()\n>>> print(results['hard_negatives'].shape)  # [500, 12]\n>>> print(results['negative_tiers'].shape)  # [500, 12]",
        "methods": {
          "__init__": "Initialize negative miner.",
          "_validate_inputs": "Validate input tensors and parameters.",
          "_build_cluster_structures": "Build cluster membership structures for efficient sampling.",
          "_compute_cluster_distances": "Precompute cosine distances between all cluster centroids.",
          "mine_all_negatives": "Mine all hard negatives for all pairs.\n\nReturns\n-------\nDict[str, torch.Tensor]\n    Dictionary with keys:\n    - 'hard_negatives': Source indices [n_pairs, n_neg]\n    - 'negative_tiers': Tier labels [n_pairs, n_neg]",
          "_mine_pair_negatives": "Mine negatives for a single pair.\n\nParameters\n----------\nquestion_idx : int\n    Question index\ntrue_source_idx : int\n    True source index (to exclude)\ncluster_id : int\n    Cluster ID for the pair\n\nReturns\n-------\nTuple[torch.Tensor, torch.Tensor]\n    (negative_indices, tier_labels) both [n_neg]",
          "_sample_in_cluster": "Sample negatives from the same cluster.",
          "_sample_adjacent_clusters": "Sample negatives from adjacent clusters.",
          "_sample_high_similarity": "Sample negatives with highest similarity to question (from other clusters).",
          "_sample_random": "Sample random negatives from distant clusters.",
          "_validate_negatives": "Validate mined negatives.",
          "save": "Save mined negatives to disk.\n\nParameters\n----------\noutput_dir : str or Path\n    Directory to save outputs\nresults : Dict[str, torch.Tensor]\n    Results from mine_all_negatives()"
        }
      }
    },
    "functions": {
      "mine_negatives": "Convenience function to mine negatives and save to disk.\n\nParameters\n----------\nsource_embeddings : torch.Tensor\n    Source embeddings [n_sources, dim]\nquestion_embeddings : torch.Tensor\n    Question embeddings [n_questions, dim]\ncentroid_embeddings : torch.Tensor\n    Cluster centroid embeddings [n_clusters, dim]\npair_indices : torch.Tensor\n    Pair indices [n_pairs, 2]\npair_cluster_ids : torch.Tensor\n    Primary cluster ID per pair [n_pairs]\nsource_cluster_ids : torch.Tensor\n    Cluster assignment for each source [n_sources]\nn_neg : int\n    Total number of negatives per pair\noutput_dir : str or Path\n    Directory to save outputs\ntier_proportions : List[int], optional\n    Number of negatives per tier\nadjacent_k : int, optional\n    Number of adjacent clusters (default: 3)\nrandom_seed : int, optional\n    Random seed (default: 42)\nshow_progress : bool, optional\n    Show progress bars (default: True)\n\nReturns\n-------\nDict[str, torch.Tensor]\n    Dictionary with 'hard_negatives' and 'negative_tiers'\n\nExamples\n--------\n>>> from RAG_supporters.dataset import mine_negatives\n>>>\n>>> results = mine_negatives(\n...     source_embeddings=source_embs,\n...     question_embeddings=question_embs,\n...     centroid_embeddings=centroid_embs,\n...     pair_indices=pair_indices,\n...     pair_cluster_ids=pair_cluster_ids,\n...     source_cluster_ids=source_cluster_ids,\n...     n_neg=12,\n...     output_dir=\"./dataset\",\n...     tier_proportions=[3, 4, 3, 2]\n... )"
    }
  },
  {
    "path": "RAG_supporters/data_prep/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "data_prep",
    "package": "RAG_supporters",
    "module_docstring": "Generic data preprocessing utilities for RAG_supporters.\n\nThis module provides reusable CSV merging, deduplication, and dataset splitting\nfunctionality that works with any tabular data format.\n\nKey Features:\n- CSV merging with column aliasing\n- Deduplication with configurable merge rules\n- Stratified splitting with no-leakage guarantees\n- Works with any CSV format via column mapping\n\nExamples\n--------\n>>> from RAG_supporters.data_prep import merge_csv_files, DatasetSplitter\n>>>\n>>> # Merge CSV files with deduplication\n>>> df = merge_csv_files(\n...     csv_paths=[\"data1.csv\", \"data2.csv\"],\n...     output_path=\"merged.csv\"\n... )\n>>>\n>>> # Stratified split with question-level grouping\n>>> splitter = DatasetSplitter(\n...     pair_indices=pair_indices,\n...     pair_cluster_ids=cluster_ids,\n...     train_ratio=0.7,\n...     val_ratio=0.15,\n...     test_ratio=0.15\n... )\n>>> results = splitter.split()",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/data_prep/dataset_splitter.py",
    "file": "dataset_splitter.py",
    "module": "dataset_splitter",
    "parent_module": "data_prep",
    "package": "RAG_supporters",
    "module_docstring": "Dataset splitting utilities with persistent sample tracking.\n\nThis module provides functionality to split datasets into training and validation sets\nwith support for saving and restoring split assignments. This ensures consistency\nacross runs and experiments.",
    "classes": {
      "DatasetSplitter": {
        "docstring": "Utility class for splitting datasets with persistent sample tracking.\n\nThis class provides methods to split dataset indices into training and validation\nsets, save the split configuration, and restore it for consistent experiments.\n\nExamples\n--------\n>>> # Create a split\n>>> splitter = DatasetSplitter(random_state=42)\n>>> train_indices, val_indices = splitter.split(dataset_size=1000, val_ratio=0.2)\n>>> splitter.save_split('split_config.json')\n>>>\n>>> # Later, restore the same split\n>>> splitter2 = DatasetSplitter.load_split('split_config.json')\n>>> train_indices2, val_indices2 = splitter2.get_split()",
        "methods": {
          "__init__": "Initialize DatasetSplitter.\n\nParameters\n----------\nrandom_state : Optional[int]\n    Random seed for reproducibility. If None, splits will be non-deterministic.",
          "split": "Split dataset indices into training and validation sets.\n\nParameters\n----------\ndataset_size : int\n    Total number of samples in the dataset (must be >= 2)\nval_ratio : float, optional\n    Ratio of validation samples (between 0 and 1). Default is 0.2.\nshuffle : bool, optional\n    Whether to shuffle indices before splitting. Default is True.\n\nReturns\n-------\nTuple[List[int], List[int]]\n    Tuple of (train_indices, val_indices)\n\nRaises\n------\nValueError\n    If val_ratio is not between 0 and 1, dataset_size is not positive,\n    or if the split would result in empty train or validation sets.\n\nNotes\n-----\nBoth train and validation sets must have at least one sample. For small\ndatasets, ensure val_ratio is chosen such that both sets are non-empty:\n- Minimum val_ratio: 1/dataset_size\n- Maximum val_ratio: (dataset_size-1)/dataset_size",
          "get_split": "Get the current split.\n\nReturns\n-------\nTuple[List[int], List[int]]\n    Tuple of (train_indices, val_indices)\n\nRaises\n------\nValueError\n    If no split has been created or loaded",
          "save_split": "Save split configuration to a JSON file.\n\nParameters\n----------\noutput_path : Union[str, Path]\n    Path where the split configuration will be saved\nmetadata : Optional[Dict]\n    Additional metadata to save with the split\n\nRaises\n------\nValueError\n    If no split has been created",
          "load_split": "Load split configuration from a JSON file.\n\nParameters\n----------\ninput_path : Union[str, Path]\n    Path to the split configuration file\n\nReturns\n-------\nDatasetSplitter\n    DatasetSplitter instance with loaded split\n\nRaises\n------\nFileNotFoundError\n    If the split configuration file does not exist\nValueError\n    If the file format is invalid",
          "validate_split": "Validate that the current split is compatible with a dataset.\n\nParameters\n----------\ndataset_size : int\n    Size of the dataset to validate against\n\nReturns\n-------\nbool\n    True if split is valid for the dataset\n\nRaises\n------\nValueError\n    If split is invalid or incompatible"
        }
      }
    },
    "functions": {
      "create_train_val_split": "Create a train/val split.\n\nParameters\n----------\ndataset_size : int\n    Total number of samples in the dataset\nval_ratio : float, optional\n    Ratio of validation samples. Default is 0.2.\nrandom_state : Optional[int], optional\n    Random seed for reproducibility\nshuffle : bool, optional\n    Whether to shuffle indices before splitting. Default is True.\nsave_path : Optional[Union[str, Path]], optional\n    If provided, save split configuration to this path\nmetadata : Optional[Dict], optional\n    Additional metadata to save with the split\n\nReturns\n-------\nTuple[List[int], List[int]]\n    Tuple of (train_indices, val_indices)\n\nExamples\n--------\n>>> train_idx, val_idx = create_train_val_split(\n...     dataset_size=1000,\n...     val_ratio=0.2,\n...     random_state=42,\n...     save_path='my_split.json'\n... )"
    }
  },
  {
    "path": "RAG_supporters/data_prep/merge_csv.py",
    "file": "merge_csv.py",
    "module": "merge_csv",
    "parent_module": "data_prep",
    "package": "RAG_supporters",
    "module_docstring": "CSV Merger for JASPER Steering Dataset Builder.\n\nThis module handles merging multiple CSV files with question-source pairs,\nperforming column normalization, deduplication, and ID assignment.\n\nMerge Rules:\n- Questions/sources are deduplicated by exact text match\n- For duplicates: max relevance score, union of keywords, longest answer\n- Each unique pair gets a sequential ID",
    "classes": {
      "CSVMerger": {
        "docstring": "Merge multiple CSV files into a unified dataset.\n\nHandles column aliasing, deduplication, and ID assignment for\nquestion-source pairs. Preserves many-to-many relationships:\n- One question can have multiple sources (separate pairs)\n- One source can answer multiple questions (separate pairs)\n- Only exact duplicate pairs (same question + same source) are merged\n\nParameters\n----------\ncolumn_aliases : Dict[str, List[str]], optional\n    Mapping from standard column names to alternative names.\n    Default aliases:\n    - question: [\"question\", \"question_text\", \"query\"]\n    - source: [\"source\", \"source_text\", \"context\", \"passage\"]\n    - answer: [\"answer\", \"answer_text\", \"response\"]\n    - keywords: [\"keywords\", \"keyword\", \"topics\", \"tags\"]\n    - relevance_score: [\"relevance_score\", \"score\", \"relevance\", \"label\"]\n\nExamples\n--------\n>>> merger = CSVMerger()\n>>> merged_df = merger.merge_csv_files(\n...     csv_paths=[\"data1.csv\", \"data2.csv\"],\n...     output_path=\"merged.csv\"\n... )\n>>> print(f\"Merged {len(merged_df)} pairs\")\n>>> print(f\"Questions: {merged_df['question_id'].nunique()}\")\n>>> print(f\"Sources: {merged_df['source_id'].nunique()}\")",
        "methods": {
          "__init__": "Initialize CSV merger.",
          "_find_column": "Find column in DataFrame using aliases.\n\nParameters\n----------\ndf : pd.DataFrame\n    DataFrame to search\nstandard_name : str\n    Standard column name (e.g., \"question\")\n\nReturns\n-------\nstr or None\n    Actual column name if found, None otherwise",
          "_normalize_dataframe": "Normalize DataFrame to standard column names.\n\nParameters\n----------\ndf : pd.DataFrame\n    Input DataFrame\nsource_file : str\n    Original file name (for logging)\n\nReturns\n-------\npd.DataFrame\n    Normalized DataFrame with standard column names\n\nRaises\n------\nValueError\n    If required columns (question, source) are missing",
          "_parse_keywords": "Parse keywords from various formats.\n\nHandles:\n- JSON list: '[\"keyword1\", \"keyword2\"]'\n- Comma-separated: \"keyword1, keyword2\"\n- Single string: \"keyword1\"\n- NaN/empty: []\n\nParameters\n----------\nvalue : any\n    Keyword value from DataFrame\n\nReturns\n-------\nList[str]\n    Parsed keywords",
          "_merge_duplicates": "Merge duplicate question-source pairs.\n\nNOTE: Many-to-many relationships are preserved:\n- Same question + different sources = multiple pairs (NOT merged)\n- Different questions + same source = multiple pairs (NOT merged)\n- Same question + same source = duplicate pair (merged)\n\nMerge rules for duplicate pairs:\n- Max relevance score\n- Union of keywords (deduplicated)\n- Longest answer\n\nParameters\n----------\ndf : pd.DataFrame\n    DataFrame with potential duplicates\n\nReturns\n-------\npd.DataFrame\n    DataFrame with duplicate pairs merged, many-to-many preserved",
          "_assign_ids": "Assign unique IDs to questions, sources, and pairs.\n\nParameters\n----------\ndf : pd.DataFrame\n    Merged DataFrame\n\nReturns\n-------\npd.DataFrame\n    DataFrame with ID columns added:\n    - question_id: Unique question ID\n    - source_id: Unique source ID\n    - pair_id: Unique pair ID (sequential)",
          "merge_csv_files": "Merge multiple CSV files into a unified dataset.\n\nThis method preserves many-to-many relationships:\n- A question can appear with multiple sources (1 question \u2192 N sources)\n- A source can appear with multiple questions (N questions \u2192 1 source)\n- Only exact duplicate pairs (same question + same source) are merged\n\nParameters\n----------\ncsv_paths : List[str or Path]\n    Paths to CSV files to merge\noutput_path : str or Path, optional\n    If provided, save merged DataFrame to this path\n\nReturns\n-------\npd.DataFrame\n    Merged DataFrame with columns:\n    - pair_id: Unique pair identifier\n    - question_id: Unique question identifier\n    - source_id: Unique source identifier\n    - question: Question text\n    - source: Source text\n    - answer: Answer text (optional, may be empty)\n    - keywords: List of keywords\n    - relevance_score: Relevance score in [0, 1]\n\nExamples\n--------\n>>> merger = CSVMerger()\n>>> df = merger.merge_csv_files(\n...     csv_paths=[\"data1.csv\", \"data2.csv\"],\n...     output_path=\"merged.csv\"\n... )\n>>> print(f\"Total pairs: {len(df)}\")\n>>> print(f\"Unique questions: {df['question_id'].nunique()}\")\n>>> print(f\"Unique sources: {df['source_id'].nunique()}\")\n>>> # Example: question with multiple sources\n>>> q1_pairs = df[df['question_id'] == 0]\n>>> print(f\"Question 0 has {len(q1_pairs)} sources\")",
          "create_inspection_metadata": "Create optional inspection.json metadata for debugging.\n\nParameters\n----------\ndf : pd.DataFrame\n    Merged DataFrame with IDs\nclustering_source : str\n    Path to KeywordClusterer JSON\n\nReturns\n-------\nDict\n    Inspection metadata dictionary"
        }
      }
    },
    "functions": {
      "merge_csv_files": "Convenience function to merge CSV files.\n\nPreserves many-to-many relationships between questions and sources.\nOnly exact duplicate pairs (same question + same source) are merged.\n\nParameters\n----------\ncsv_paths : List[str or Path]\n    Paths to CSV files to merge\noutput_path : str or Path, optional\n    If provided, save merged DataFrame to this path\ncolumn_aliases : Dict[str, List[str]], optional\n    Custom column name aliases\n\nReturns\n-------\npd.DataFrame\n    Merged DataFrame with many-to-many relationships preserved\n\nExamples\n--------\n>>> df = merge_csv_files(\n...     csv_paths=[\"data1.csv\", \"data2.csv\"],\n...     output_path=\"merged.csv\"\n... )\n>>> # Verify many-to-many relationship\n>>> print(f\"Avg sources per question: {len(df) / df['question_id'].nunique():.2f}\")"
    }
  },
  {
    "path": "RAG_supporters/data_validation/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "data_validation",
    "package": "RAG_supporters",
    "module_docstring": "Data validation and utilities for RAG_supporters.\n\nThis module provides tensor validation, tensor I/O, and label normalization\nutilities. These are highly reusable across any PyTorch project.\n\nKey Features:\n- Tensor shape and type validation\n- NaN/Inf detection\n- Index bounds validation\n- Dimension consistency checks\n- Tensor I/O with shape validation\n- Label normalization (softmax, L1)\n\nExamples\n--------\n>>> from RAG_supporters.data_validation import validate_tensor_2d, load_tensor_artifact\n>>>\n>>> # Validate tensor shape\n>>> validate_tensor_2d(embeddings, \"embeddings\", expected_cols=384)\n>>>\n>>> # Load tensor with validation\n>>> tensor = load_tensor_artifact(\"output/\", \"embeddings.pt\", expected_shape=(None, 384))",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/data_validation/label_calculator.py",
    "file": "label_calculator.py",
    "module": "label_calculator",
    "parent_module": "data_validation",
    "package": "RAG_supporters",
    "module_docstring": "Label calculation system for domain assessment dataset.\n\nCalculates 3 types of labels:\n1. Source/question labels: Based on CSV cluster_probabilities or centroid distances\n2. Steering embedding labels: Based on centroid distances\n3. Combined labels: Average of source and steering labels (for masking augmentation)",
    "classes": {
      "LabelNormalizationMethod": {
        "docstring": "Supported label normalization methods.",
        "methods": {}
      },
      "LabelNormalizer": {
        "docstring": "Base class for label normalization strategies.",
        "methods": {
          "normalize": "Normalize distances to probability distribution.\n\nArgs:\n    distances: Array of distances to cluster centroids\n\nReturns:\n    Normalized probability vector"
        }
      },
      "SoftmaxNormalizer": {
        "docstring": "Softmax normalization: exp(-d) / sum(exp(-d)).",
        "methods": {
          "__init__": "Initialize normalizer.\n\nArgs:\n    temperature: Temperature parameter for softmax (default 1.0)",
          "normalize": "Softmax normalization with temperature scaling."
        }
      },
      "L1Normalizer": {
        "docstring": "L1 normalization: inverse distances normalized to sum to 1.",
        "methods": {
          "normalize": "L1 normalization."
        }
      },
      "LabelCalculator": {
        "docstring": "Calculates cluster membership labels for dataset samples.\n\nSupports 3 label types:\n1. Source/question labels (from CSV or computed)\n2. Steering embedding labels (computed from centroids)\n3. Combined labels (for masking augmentation)",
        "methods": {
          "__init__": "Initialize label calculator.\n\nArgs:\n    clustering_data: ClusteringData with centroids\n    normalization_method: Method to normalize distances to probabilities\n    temperature: Temperature for softmax normalization",
          "calculate_source_labels_from_suggestions": "Calculate source labels from suggestion texts and their embeddings.\n\nComputes average distance of all suggestion embeddings to each centroid,\nthen normalizes to probability distribution.\n\nArgs:\n    suggestions: List of suggestion dicts with 'term' field\n    suggestion_embeddings: Dict mapping suggestion terms to embeddings\n\nReturns:\n    Probability vector of shape (n_clusters,)",
          "calculate_question_labels_from_csv": "Use pre-computed cluster probabilities from CSV.\n\nIf probabilities not available, returns uniform distribution.\n\nArgs:\n    cluster_probabilities: Probability vector from CSV\n\nReturns:\n    Probability vector of shape (n_clusters,)",
          "calculate_question_labels_from_embedding": "Calculate question labels from embedding (distance to centroids).\n\nArgs:\n    question_embedding: Question embedding vector\n\nReturns:\n    Probability vector of shape (n_clusters,)",
          "calculate_steering_labels": "Calculate steering embedding labels from centroid distances.\n\nArgs:\n    steering_embedding: Steering embedding vector\n\nReturns:\n    Probability vector of shape (n_clusters,)",
          "calculate_combined_labels": "Calculate combined labels for masking augmentation.\n\nWeighted average of source/question and steering labels.\n\nArgs:\n    source_or_question_labels: Source or question label vector\n    steering_labels: Steering label vector\n    steering_weight: Weight for steering labels (0-1)\n\nReturns:\n    Combined probability vector",
          "calculate_all_labels": "Calculate all 3 label types for a sample.\n\nArgs:\n    source_or_question_embedding: Base embedding\n    steering_embedding: Steering embedding\n    csv_cluster_probabilities: Pre-computed probabilities from CSV (for questions)\n    suggestions: List of suggestions (for sources)\n    suggestion_embeddings: Embeddings for suggestions (for sources)\n    use_csv_for_question: If True and csv_cluster_probabilities available,\n                          use them for question labels\n    steering_weight: Weight for combined labels\n\nReturns:\n    Tuple of (source_question_labels, steering_labels, combined_labels)"
        }
      }
    },
    "functions": {
      "get_normalizer": "Get label normalizer instance for the specified method.\n\nArgs:\n    method: Normalization method\n    **kwargs: Additional arguments (e.g., temperature for softmax)\n\nReturns:\n    LabelNormalizer instance"
    }
  },
  {
    "path": "RAG_supporters/data_validation/tensor_utils.py",
    "file": "tensor_utils.py",
    "module": "tensor_utils",
    "parent_module": "data_validation",
    "package": "RAG_supporters",
    "module_docstring": "Tensor loading and storage utilities for dataset builder pipeline.\n\nThis module provides common tensor I/O operations to avoid code duplication\nin dataset loading (JASPERSteeringDataset, DatasetFinalizer, etc.).\n\nKey Features:\n- Standardized tensor loading with error handling\n- Optional shape validation\n- Support for weights_only flag\n- Clear error messages for missing files",
    "classes": {},
    "functions": {
      "load_tensor_artifact": "Load tensor artifact with validation.\n\nParameters\n----------\ndataset_dir : str or Path\n    Directory containing tensor files\nfilename : str\n    Tensor filename (e.g., \"question_embs.pt\")\nweights_only : bool, optional\n    Whether to use weights_only flag in torch.load (default: True)\nexpected_shape : Tuple[Optional[int], ...], optional\n    Expected tensor shape. Use None for dimensions that can vary.\n    Example: (None, 384) means any number of rows, 384 columns\nrequired : bool, optional\n    Whether file is required (default: True). If False and file missing,\n    returns None instead of raising error.\n\nReturns\n-------\ntorch.Tensor or None\n    Loaded tensor, or None if not required and missing\n\nRaises\n------\nFileNotFoundError\n    If required file is missing\nValueError\n    If tensor shape doesn't match expected_shape\n\nExamples\n--------\n>>> # Load embeddings with shape validation\n>>> question_embs = load_tensor_artifact(\n...     \"output/dataset\",\n...     \"question_embs.pt\",\n...     expected_shape=(None, 384)  # Any rows, 384 cols\n... )\n>>>\n>>> # Load optional file\n>>> metadata = load_tensor_artifact(\n...     \"output/dataset\",\n...     \"metadata.pt\",\n...     required=False\n... )",
      "load_multiple_tensors": "Load multiple tensor artifacts efficiently.\n\nParameters\n----------\ndataset_dir : str or Path\n    Directory containing tensor files\nfile_specs : List[Tuple[str, str, bool, Optional[Tuple]]]\n    List of (key, filename, weights_only, expected_shape) tuples\n\nReturns\n-------\ndict\n    Dictionary mapping keys to loaded tensors\n\nExamples\n--------\n>>> specs = [\n...     (\"question_embs\", \"question_embs.pt\", True, (None, 384)),\n...     (\"source_embs\", \"source_embs.pt\", True, (None, 384)),\n...     (\"pair_index\", \"pair_index.pt\", True, (None, 2)),\n... ]\n>>> tensors = load_multiple_tensors(\"output/dataset\", specs)\n>>> print(tensors.keys())\ndict_keys(['question_embs', 'source_embs', 'pair_index'])",
      "save_tensor_artifact": "Save tensor artifact with optional validation.\n\nParameters\n----------\ntensor : torch.Tensor\n    Tensor to save\noutput_dir : str or Path\n    Output directory\nfilename : str\n    Output filename (e.g., \"embeddings.pt\")\nvalidate : bool, optional\n    Whether to check for NaN/Inf before saving (default: True)\n\nRaises\n------\nValueError\n    If tensor contains NaN or Inf values (when validate=True)\n\nExamples\n--------\n>>> embeddings = torch.randn(100, 384)\n>>> save_tensor_artifact(embeddings, \"output/dataset\", \"embeddings.pt\")"
    }
  },
  {
    "path": "RAG_supporters/data_validation/validation_utils.py",
    "file": "validation_utils.py",
    "module": "validation_utils",
    "parent_module": "data_validation",
    "package": "RAG_supporters",
    "module_docstring": "Shared validation utilities for dataset builder pipeline.\n\nThis module provides common validation functions to avoid code duplication\nacross builder classes (SteeringBuilder, NegativeMiner, DatasetSplitter, etc.).\n\nKey Features:\n- Tensor type and shape validation\n- Dimension consistency checks\n- Index bounds validation\n- Ratio validation for splits",
    "classes": {},
    "functions": {
      "validate_tensor_2d": "Validate that tensor is 2D with optional column count check.\n\nParameters\n----------\ntensor : Any\n    Object to validate\nname : str\n    Name of tensor for error messages\nexpected_cols : int, optional\n    Expected number of columns, if None no check is performed\nmin_rows : int, optional\n    Minimum number of rows required (default: 1)\n\nRaises\n------\nTypeError\n    If tensor is not torch.Tensor\nValueError\n    If tensor is not 2D, has wrong column count, or too few rows\n\nExamples\n--------\n>>> validate_tensor_2d(torch.randn(10, 384), \"embeddings\", expected_cols=384)\n>>> validate_tensor_2d(torch.randn(5, 2), \"pair_indices\", expected_cols=2)",
      "validate_tensor_1d": "Validate that tensor is 1D with optional length check.\n\nParameters\n----------\ntensor : Any\n    Object to validate\nname : str\n    Name of tensor for error messages\nexpected_length : int, optional\n    Expected length, if None no check is performed\nmin_length : int, optional\n    Minimum length required (default: 1)\n\nRaises\n------\nTypeError\n    If tensor is not torch.Tensor\nValueError\n    If tensor is not 1D, has wrong length, or too short\n\nExamples\n--------\n>>> validate_tensor_1d(torch.tensor([1, 2, 3]), \"cluster_ids\", expected_length=3)",
      "validate_embedding_dimensions": "Validate that all embedding tensors have consistent dimensions.\n\nParameters\n----------\n*tensors_and_names : Tuple[torch.Tensor, str]\n    Pairs of (tensor, name) to validate\n\nReturns\n-------\nint\n    The common embedding dimension\n\nRaises\n------\nValueError\n    If tensors have inconsistent dimensions\n\nExamples\n--------\n>>> q_emb = torch.randn(100, 384)\n>>> s_emb = torch.randn(200, 384)\n>>> k_emb = torch.randn(50, 384)\n>>> dim = validate_embedding_dimensions(\n...     (q_emb, \"question_embs\"),\n...     (s_emb, \"source_embs\"),\n...     (k_emb, \"keyword_embs\")\n... )\n>>> print(dim)  # 384",
      "validate_pair_indices_bounds": "Validate that pair indices are within bounds.\n\nParameters\n----------\npair_indices : torch.Tensor\n    Pair indices [n_pairs, 2] with (question_idx, source_idx)\nn_questions : int\n    Number of available questions\nn_sources : int\n    Number of available sources\nname : str, optional\n    Name for error messages (default: \"pair_indices\")\n\nRaises\n------\nValueError\n    If any indices are out of bounds\n\nExamples\n--------\n>>> pair_indices = torch.tensor([[0, 5], [1, 10], [2, 3]])\n>>> validate_pair_indices_bounds(pair_indices, n_questions=50, n_sources=20)",
      "validate_cluster_ids_bounds": "Validate that cluster IDs are within bounds.\n\nParameters\n----------\ncluster_ids : torch.Tensor\n    Cluster IDs tensor [n_items]\nn_clusters : int\n    Number of available clusters\nname : str, optional\n    Name for error messages (default: \"cluster_ids\")\n\nRaises\n------\nValueError\n    If any cluster IDs are out of bounds\n\nExamples\n--------\n>>> cluster_ids = torch.tensor([0, 1, 2, 1, 0])\n>>> validate_cluster_ids_bounds(cluster_ids, n_clusters=5)",
      "validate_length_consistency": "Validate that tensors/lists have consistent lengths.\n\nParameters\n----------\n*tensors_and_names : Tuple[Any, str, int]\n    Tuples of (tensor/list, name, expected_length)\n\nRaises\n------\nValueError\n    If lengths don't match expectations\n\nExamples\n--------\n>>> pairs = torch.randn(100, 2)\n>>> clusters = torch.randint(0, 5, (100,))\n>>> keywords = [[1, 2], [3]] * 50  # 100 items\n>>> validate_length_consistency(\n...     (pairs, \"pair_indices\", 100),\n...     (clusters, \"pair_cluster_ids\", 100),\n...     (keywords, \"pair_keyword_ids\", 100)\n... )",
      "validate_split_ratios": "Validate train/val/test split ratios.\n\nParameters\n----------\ntrain_ratio : float\n    Training set ratio\nval_ratio : float\n    Validation set ratio\ntest_ratio : float\n    Test set ratio\ntolerance : float, optional\n    Tolerance for sum check (default: 1e-6)\n\nRaises\n------\nValueError\n    If ratios are invalid or don't sum to 1.0\n\nExamples\n--------\n>>> validate_split_ratios(0.7, 0.15, 0.15)\n>>> validate_split_ratios(0.8, 0.1, 0.1)",
      "validate_keyword_ids_list": "Validate list-of-lists structure for pair keyword IDs.\n\nParameters\n----------\npair_keyword_ids : Any\n    Should be list of lists of keyword IDs\nn_pairs : int\n    Expected number of pairs\nn_keywords : int\n    Number of available keywords for bounds checking\nname : str, optional\n    Name for error messages (default: \"pair_keyword_ids\")\n\nRaises\n------\nTypeError\n    If structure is invalid\nValueError\n    If lengths don't match or IDs are out of bounds\n\nExamples\n--------\n>>> pair_keyword_ids = [[0, 1, 2], [1, 3], [], [4, 5]]\n>>> validate_keyword_ids_list(pair_keyword_ids, n_pairs=4, n_keywords=10)",
      "validate_no_nan_inf": "Validate tensor contains no NaN or Inf values.\n\nParameters\n----------\ntensor : torch.Tensor\n    Tensor to validate\nname : str\n    Name for error messages\n\nRaises\n------\nValueError\n    If tensor contains NaN or Inf values\n\nExamples\n--------\n>>> tensor = torch.randn(10, 5)\n>>> validate_no_nan_inf(tensor, \"embeddings\")  # Passes\n>>>\n>>> bad_tensor = torch.tensor([1.0, float('nan'), 3.0])\n>>> validate_no_nan_inf(bad_tensor, \"scores\")  # Raises ValueError",
      "validate_values_in_range": "Validate all tensor values are within specified range.\n\nParameters\n----------\ntensor : torch.Tensor\n    Tensor to validate\nname : str\n    Name for error messages\nmin_value : float\n    Minimum allowed value\nmax_value : float\n    Maximum allowed value\ninclusive : bool, optional\n    Whether range is inclusive (default: True)\n    If True: [min_value, max_value]\n    If False: (min_value, max_value)\n\nRaises\n------\nValueError\n    If any values are outside the specified range\n\nExamples\n--------\n>>> tensor = torch.tensor([0.0, 0.5, 1.0])\n>>> validate_values_in_range(tensor, \"scores\", 0.0, 1.0)  # Passes\n>>>\n>>> tensor = torch.tensor([0, 1, 2, 5, 10])\n>>> validate_values_in_range(tensor, \"indices\", 0, 4)  # Raises ValueError"
    }
  },
  {
    "path": "RAG_supporters/dataset/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "dataset",
    "package": "RAG_supporters",
    "module_docstring": "Dataset module for RAG Support DNN.\n\nLegacy module that now only contains domain assessment dataset builders.\nMost functionality has been moved to specialized modules:\n\n- PyTorch datasets: RAG_supporters.pytorch_datasets\n- Dataset building: RAG_supporters.jasper\n- Data preprocessing: RAG_supporters.data_prep\n- Contrastive learning: RAG_supporters.contrastive\n- Data validation: RAG_supporters.data_validation\n- Embedding operations: RAG_supporters.embeddings_ops (includes SteeringConfig, SteeringMode)\n- Clustering operations: RAG_supporters.clustering_ops\n\nThis module provides:\n- DomainAssessmentDatasetBuilder: Build domain assessment datasets\n- DomainAssessmentParser: Parse domain assessment CSVs\n\nNote: SteeringConfig and SteeringMode have been moved to RAG_supporters.embeddings_ops.\nImport them directly from there instead of from this module.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/dataset/domain_assessment_dataset_builder.py",
    "file": "domain_assessment_dataset_builder.py",
    "module": "domain_assessment_dataset_builder",
    "parent_module": "dataset",
    "package": "RAG_supporters",
    "module_docstring": "Dataset builder for domain assessment CSV + clustering JSON approach.",
    "classes": {
      "DomainAssessmentDatasetBuilder": {
        "docstring": "Builds cluster-labeled dataset from CSV domain assessments + clustering JSON.\n\nStorage: SQLite (metadata/labels) + numpy memmap (embeddings).\nLabels: 3 types (source, steering, combined) for augmentation support.",
        "methods": {
          "__init__": "Initialize dataset builder.\n\nArgs:\n    csv_paths: Path(s) to CSV files from domain_assessment.py\n    clustering_json_path: Path to clustering JSON from keyword_clustering.py\n    output_dir: Output directory for dataset\n    embedding_model: Model name (str), KeywordEmbedder instance, or raw model.\n                   If string: creates KeywordEmbedder with model_name.\n                   Supports both sentence-transformers and LangChain models.\n    steering_config: Steering configuration (defaults to ZERO mode)\n    label_normalizer: Normalization method ('softmax', 'l1')\n    label_temp: Temperature for softmax normalization\n    combined_label_weight: Weight for combined labels (0=source, 1=steering)\n    augment_noise_prob: Probability of noise augmentation\n    augment_zero_prob: Probability of zero steering augmentation\n    augment_noise_level: Std deviation for noise augmentation\n    chunk_size: Chunk size for CSV reading (None = read all)\n    source_suggestions_col: Column name for source suggestions (default: 'suggestions')\n    question_suggestions_col: Column name for question suggestions (default: 'selected_terms')",
          "build": "Build complete dataset from CSV + clustering data.",
          "_calculate_source_label": "Calculate source/question label from CSV probabilities or embedding.\n\nArgs:\n    sample: Sample dictionary from CSV\n    base_embedding: Embedding of the text\n    sample_type: Type of sample ('source' or 'question') - used to select appropriate column",
          "_get_primary_cluster": "Get primary cluster ID from sample.\n\nArgs:\n    sample: Sample dictionary from CSV\n    sample_type: Type of sample ('source' or 'question') - used to select appropriate column",
          "_load_suggestion_embeddings": "Load suggestion/keyword embeddings from clustering JSON.\n\nReturns:\n    Dictionary mapping suggestion terms to embeddings",
          "_save_embeddings": "Save embeddings as memory-mapped numpy array.",
          "close": "Close storage connections."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/dataset/domain_assessment_parser.py",
    "file": "domain_assessment_parser.py",
    "module": "domain_assessment_parser",
    "parent_module": "dataset",
    "package": "RAG_supporters",
    "module_docstring": "CSV parser for domain assessment data.\n\nParses CSV files generated by domain_assessment.py agent with:\n- source_text, question_text, chroma_ids\n- suggestions (JSON formatted list of DomainSuggestion objects)\n- cluster_probabilities (JSON formatted probability vector)",
    "classes": {
      "DomainAssessmentRecord": {
        "docstring": "TypedDict representing a single record from domain assessment CSV.",
        "methods": {}
      },
      "DomainAssessmentParser": {
        "docstring": "Parser for domain assessment CSV files.\n\nHandles:\n- Multiple CSV files (concatenates and deduplicates)\n- Chunked reading for large files\n- Validation of required fields",
        "methods": {
          "__init__": "Initialize parser.\n\nArgs:\n    chunksize: If set, reads CSV in chunks to reduce memory usage",
          "parse_csv": "Parse single CSV file.\n\nArgs:\n    csv_path: Path to CSV file\n\nReturns:\n    List of parsed records\n\nRaises:\n    FileNotFoundError: If CSV not found\n    ValueError: If CSV invalid or missing required fields",
          "parse_multiple_csvs": "Parse multiple CSV files and optionally deduplicate.\n\nArgs:\n    csv_paths: List of CSV file paths\n    deduplicate: If True, removes duplicate records based on chroma IDs\n\nReturns:\n    Combined list of parsed records",
          "_parse_dataframe": "Parse DataFrame into records.\n\nArgs:\n    df: Pandas DataFrame\n\nReturns:\n    List of parsed records",
          "_deduplicate_records": "Remove duplicate records based on chroma IDs.\n\nArgs:\n    records: List of records\n\nReturns:\n    Deduplicated list"
        }
      }
    },
    "functions": {
      "create_domain_assessment_record": "Create DomainAssessmentRecord from a pandas Series (CSV row).\n\nArgs:\n    row: Pandas Series from CSV\n\nReturns:\n    DomainAssessmentRecord dictionary\n\nRaises:\n    ValueError: If required fields missing or invalid format"
    }
  },
  {
    "path": "RAG_supporters/dataset/finalize.py",
    "file": "finalize.py",
    "module": "finalize",
    "parent_module": "dataset",
    "package": "RAG_supporters",
    "module_docstring": "Finalize and validate JASPER dataset builder outputs.\n\nThis module implements Task 8 of the dataset builder pipeline:\n- Cross-validate all generated output tensors/files\n- Verify referential integrity and dimensional consistency\n- Produce final ``config.json`` as single source of truth",
    "classes": {
      "DatasetFinalizer": {
        "docstring": "Validate built dataset artifacts and write final config.\n\nParameters\n----------\noutput_dir : str or Path\n    Dataset output directory containing generated artifacts.",
        "methods": {
          "__init__": "Initialize finalizer.",
          "_validate_required_files_pt": "Validate all required PT artifacts exist.",
          "_load_pt_artifacts": "Load all PT artifacts.\n\nReturns\n-------\nDict[str, object]\n    Loaded tensors and pair keyword IDs list.",
          "_validate_embedding_tensor": "Validate embedding tensor shape and return embedding dim.",
          "_validate_pair_keyword_ids": "Validate pair keyword IDs list structure and bounds.",
          "_validate_splits": "Validate split tensors cover all pairs without overlap.",
          "_validate_pt_artifacts": "Validate loaded PT artifacts and return computed statistics.",
          "_resolve_config": "Resolve config source: argument, existing file, or minimal constructor.",
          "finalize": "Validate outputs and write final config.json.\n\nParameters\n----------\nconfig : BuildConfig, optional\n    Existing build configuration to enrich with computed statistics.\nclustering_source : str or Path, optional\n    Optional override for clustering JSON path in final config.\nsave : bool, optional\n    Whether to persist final config.json (default: True).\n\nReturns\n-------\nBuildConfig\n    Final validated configuration."
        }
      }
    },
    "functions": {
      "finalize_dataset": "Convenience function for dataset finalization.\n\nParameters\n----------\noutput_dir : str or Path\n    Dataset output directory.\nconfig : BuildConfig, optional\n    Optional BuildConfig to enrich and validate.\nclustering_source : str or Path, optional\n    Optional clustering JSON path override.\nsave : bool, optional\n    Whether to write ``config.json`` to disk (default: True).\n\nReturns\n-------\nBuildConfig\n    Final validated configuration."
    }
  },
  {
    "path": "RAG_supporters/dataset/templates/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "templates",
    "package": "RAG_supporters",
    "module_docstring": "Dataset templates package for RAG dataset generation.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/dataset/templates/rag_mini_bioasq.py",
    "file": "rag_mini_bioasq.py",
    "module": "rag_mini_bioasq",
    "parent_module": "templates",
    "package": "RAG_supporters",
    "module_docstring": "RAG Mini BioASQ dataset template implementation.",
    "classes": {
      "RagMiniBioASQBase": {
        "docstring": "RAG dataset generator for mini BioASQ dataset.\n\nThis class handles the loading, initialization, and generation of samples from the\nBioASQ mini dataset for retrieval-augmented generation (RAG) triplet training.\n\nParameters\n----------\ndataset_dir : str\n    Directory where the dataset and Chroma databases will be stored.\nembed_function : callable, optional\n    Function to create embeddings. If None, OpenAIEmbeddings will be used.\n**kwargs : dict\n    Additional parameters:\n    - openai_api_key : str, optional\n        OpenAI API key, used if embed_function is None.\n    - model : str, optional\n        Model to use for embeddings, default is \"text-embedding-3-small\".",
        "methods": {
          "__init__": "Initialize the RAG Mini BioASQ dataset generator.",
          "load_dataset": "Load or initialize the dataset and Chroma databases.\n\nCreates and initializes two Chroma databases:\n- question_db: containing questions from the BioASQ dataset\n- text_corpus_db: containing passages from the BioASQ text corpus\n\nIf databases exist, it loads them; otherwise, it initializes them\nwith data from the BioASQ dataset.",
          "validate_dataset": "Validate if dataset is correctly loaded.\n\nChecks:\n- Databases are not empty\n- All questions have 'relevant_chroma_ids' metadata\n\nRaises\n------\nValueError\n    If databases are empty or questions lack required metadata.",
          "generate_samples": "Generate dataset samples based on sample type.",
          "_init_text_corpus_db": "Initialize the text corpus database with passages from the BioASQ dataset.\n\nLoads passages from the BioASQ dataset and adds them to the text corpus\nChroma database in batches. Also creates a mapping from passage IDs to\nChroma IDs.\n\nParameters\n----------\nbatch_size : int, default=10\n    Number of passages to process in a single batch.",
          "_init_questions_db": "Initialize the question database with questions from the BioASQ dataset.\n\nLoads questions, their IDs, and relevant passage IDs from the BioASQ\ndataset and adds them to the question Chroma database in batches.\n\nParameters\n----------\nbatch_size : int, default=10\n    Number of questions to process in a single batch.",
          "_passage_id_text_to_chroma_id": "Convert a passage ID to its corresponding Chroma ID.\n\nParameters\n----------\npassage_id : str\n    The passage ID to convert.\n\nReturns\n-------\nstr\n    The corresponding Chroma ID.\n\nRaises\n------\nFileNotFoundError\n    If the mapping file is not found.\nValueError\n    If the passage ID is not found in the mapping.",
          "_generate_positive_triplet_samples": "Generate triplets consisting of a question and two relevant passages.\n\nCreates triplet samples where both passage answers are relevant to the question.\nThese are considered positive examples where both passages are equally good.\n\nParameters\n----------\nquestion_db_id : str\n    The Chroma ID of the question embedding.\nrelevant_passage_db_ids : list of str\n    IDs of passages that are relevant to the question.\n**kwargs : dict\n    Additional parameters (not used).\n\nReturns\n-------\nList[SampleTripletRAGChroma]\n    List of triplet samples with question and two relevant passages.\n    Label is set to -1 indicating no preference between the two passages.",
          "_generate_contrastive_triplet_samples": "Generate triplets consisting of a question, one relevant passage, and one non-relevant passage.\n\nCreates triplet samples where one passage is relevant to the question and the other is not.\nThese contrastive samples help the model learn to differentiate between relevant and\nnon-relevant passages.\n\nParameters\n----------\nquestion_db_id : str\n    The Chroma ID of the question embedding.\nrelevant_passage_db_ids : list of str\n    IDs of passages that are relevant to the question.\nnum_negative_samples : int, default=5\n    Number of negative samples to generate per question.\nkeep_same_negatives : bool, default=False\n    If True, use the same set of negative passages for all triplets.\nassume_relevant_best : bool, default=True\n    If True, label is 1 (relevant passage is better), otherwise -1.\n**kwargs : dict\n    Additional parameters.\n\nReturns\n-------\nList[SampleTripletRAGChroma]\n    List of triplet samples with contrastive examples.",
          "_generate_similar_triplet_samples": "Generate triplets with a question, relevant passage, and a \"similar\" non-relevant passage.\n\nCreates challenging triplet samples where the non-relevant passage is semantically\nclose to the question in the embedding space, making it harder to distinguish from\ntruly relevant passages.\n\nParameters\n----------\nquestion_db_id : str\n    The Chroma ID of the question embedding.\nrelevant_passage_db_ids : list of str\n    IDs of passages that are relevant to the question.\nscore_threshold : float, default=0.3\n    Distance threshold for considering a passage as \"similar\" to the question.\nassume_relevant_best : bool, default=TrueQ\n    If True, label is 1 (relevant passage is better), otherwise -1.\n**kwargs : dict\n    Additional parameters:\n    - top_k : int, optional\n        Number of similar passages to retrieve, default is 3.\n\nReturns\n-------\nList[SampleTripletRAGChroma]\n    List of triplet samples with challenging examples.",
          "_generate_all_existing_pairs": "Generate all question-source pairs for ALL_EXISTING criterion.\n\nThis generator processes the text corpus in batches to avoid RAM exhaustion.\n\nParameters\n----------\nquestion_db_ids : List[str]\n    List of question IDs to generate pairs for\n**kwargs : dict\n    Additional parameters including optional 'batch_size'\n\nYields\n------\ndict\n    Dictionary containing question_id, question_text, source_id, source_text, and answer",
          "_generate_pair_samples_df": null,
          "_save_passage_json": "Save the passage database to a JSON file.\n\nThe JSON file will contain the mapping of passage IDs to their corresponding\nChroma IDs. This is useful for later retrieval and analysis."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/dataset/utils/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "utils",
    "package": "RAG_supporters",
    "module_docstring": "Dataset utilities package.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/dataset/utils/dataset_loader.py",
    "file": "dataset_loader.py",
    "module": "dataset_loader",
    "parent_module": "utils",
    "package": "RAG_supporters",
    "module_docstring": "Dataset loading utilities.",
    "classes": {},
    "functions": {
      "count_csv_rows_chunked": "Count rows by processing in chunks.",
      "parse_suggestions_safe": "Safely parse suggestions JSON without using eval().",
      "filter_suggestions": "Filter suggestions based on confidence and type, return terms.",
      "compute_cache_version": "Compute a version hash for cache validation."
    }
  },
  {
    "path": "RAG_supporters/embeddings/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "embeddings",
    "package": "RAG_supporters",
    "module_docstring": "Embedding utilities for keyword extraction, embedding creation, and processing.\n\nThis package provides modular components for working with embeddings:\n\n- keyword_embedder: KeywordEmbedder class for creating and managing embeddings\n- io: CSV loading utilities for suggestions\n\nThese tools support the RAG and clustering workflows by enabling efficient\nkeyword extraction, embedding generation, and similarity-based operations.\n\nRecommended usage:\n    from RAG_supporters.embeddings import KeywordEmbedder\n    from RAG_supporters.embeddings.io import load_suggestions_from_csv",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/embeddings/io.py",
    "file": "io.py",
    "module": "io",
    "parent_module": "embeddings",
    "package": "RAG_supporters",
    "module_docstring": "I/O utilities for loading suggestions from CSV files.\n\nThis module handles file operations for loading LLM suggestions:\n- Loading suggestions from CSV files with chunked processing\n\nFor embedding save/load operations, use KeywordEmbedder.save_embeddings() and\nKeywordEmbedder.load_embeddings() static methods.\n\nSupports chunked processing for large files and includes progress tracking.\nPart of the RAG_supporters.embeddings package.",
    "classes": {},
    "functions": {
      "load_suggestions_from_csv": "Load and parse suggestions from CSV file (supports large files).\n\nParameters\n----------\ncsv_path : str\n    Path to the CSV file\nsuggestion_column : str\n    Name of the column containing suggestions (as JSON)\nchunksize : int\n    Number of rows to process at a time (for large files)\nshow_progress : bool\n    Whether to show progress bar for large files\n\nReturns\n-------\nList[Dict[str, Any]]\n    List of parsed suggestions\n\nExamples\n--------\n>>> suggestions = load_suggestions_from_csv('results.csv', 'suggestions')\n>>> len(suggestions)\n150"
    }
  },
  {
    "path": "RAG_supporters/embeddings/keyword_embedder.py",
    "file": "keyword_embedder.py",
    "module": "keyword_embedder",
    "parent_module": "embeddings",
    "package": "RAG_supporters",
    "module_docstring": "KeywordEmbedder class for keyword embedding operations.\n\nThis module provides a high-level interface for the complete embedding pipeline:\n- Creating embeddings for strings using sentence transformers or LangChain models\n- Saving and loading embeddings to/from JSON\n- Processing CSV files with LLM suggestions into embeddings\n\nThe KeywordEmbedder class encapsulates embedding model management and provides\nboth instance methods and static utility methods for embedding operations.",
    "classes": {
      "KeywordEmbedder": {
        "docstring": "Class for keyword embedding operations.\n\nSupports both sentence-transformers and LangChain embedding models.\nProvides methods for creating, saving, and loading keyword embeddings.",
        "methods": {
          "__init__": "Initialize the keyword embedder.\n\nParameters\n----------\nembedding_model : Optional[Any]\n    Pre-loaded embedding model (sentence-transformers or LangChain)\nmodel_name : Optional[str]\n    Name of the embedding model to use. If None and embedding_model is provided,\n    will attempt to extract from the model instance. If None and no model provided,\n    defaults to \"sentence-transformers/all-MiniLM-L6-v2\"\nmodel_type : Optional[Literal[\"sentence-transformers\", \"langchain\"]]\n    Type of embedding model. If None, will be auto-detected.",
          "_detect_model_type": "Detect whether the model is from sentence-transformers or LangChain.\n\nParameters\n----------\nmodel : Any\n    Embedding model instance\n\nReturns\n-------\nstr\n    Model type: \"sentence-transformers\" or \"langchain\"\n\nRaises\n------\nValueError\n    If model type cannot be detected",
          "_extract_model_name": "Extract model name from a model instance.\n\nParameters\n----------\nmodel : Any\n    Embedding model instance\n\nReturns\n-------\nstr\n    Extracted model name or a default value",
          "_load_default_model": "Load the default embedding model based on model_type.\n\nReturns\n-------\nAny\n    Loaded embedding model\n\nRaises\n------\nImportError\n    If required library is not installed\nValueError\n    If model_type is not supported",
          "_generate_embeddings": "Generate embeddings using the appropriate model type.\n\nParameters\n----------\ntexts : List[str]\n    List of texts to embed\nbatch_size : int\n    Batch size for embedding generation\nshow_progress : bool\n    Whether to show progress bar\nnormalize_embeddings : bool\n    Whether to L2-normalize the embeddings\n\nReturns\n-------\nnp.ndarray\n    Array of embeddings with shape (len(texts), embedding_dim)\n\nRaises\n------\nValueError\n    If model_type is not supported",
          "create_embeddings": "Create embeddings for a list of strings.\n\nParameters\n----------\nstr_list : List[str]\n    List of strings to embed\nbatch_size : int\n    Batch size for embedding generation\nshow_progress : bool\n    Whether to show progress bar during embedding generation\nnormalize_embeddings : bool\n    Whether to L2-normalize the embeddings\n\nReturns\n-------\nDict[str, np.ndarray]\n    Dictionary mapping each string to its embedding vector\n\nRaises\n------\nValueError\n    If string list is empty\n\nExamples\n--------\n>>> embedder = KeywordEmbedder()\n>>> str_list = ['machine learning', 'data science']\n>>> embeddings = embedder.create_embeddings(str_list)\n>>> len(embeddings)\n2\n\n>>> # With LangChain\n>>> from langchain_openai import OpenAIEmbeddings\n>>> model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n>>> embedder = KeywordEmbedder(embedding_model=model)\n>>> embeddings = embedder.create_embeddings(str_list)",
          "save_embeddings": "Save keyword embeddings to JSON file.\n\nParameters\n----------\nkeyword_embeddings : Dict[str, np.ndarray]\n    Dictionary mapping keywords to embedding vectors\noutput_path : str\n    Path to save JSON file\nmodel_name : str\n    Name of the embedding model used\nmetadata : Optional[Dict[str, Any]]\n    Additional metadata to include\n\nExamples\n--------\n>>> embeddings = {'keyword1': np.array([0.1, 0.2])}\n>>> KeywordEmbedder.save_embeddings(embeddings, 'embeddings.json', 'my-model')",
          "load_embeddings": "Load keyword embeddings from JSON file.\n\nParameters\n----------\ninput_path : str\n    Path to JSON file\n\nReturns\n-------\nTuple[Dict[str, np.ndarray], Dict[str, Any]]\n    Tuple of (keyword_embeddings, metadata)\n\nExamples\n--------\n>>> embeddings, metadata = KeywordEmbedder.load_embeddings('embeddings.json')\n>>> print(metadata['model_name'])\n'sentence-transformers/all-MiniLM-L6-v2'",
          "process_csv_to_embeddings": "Complete pipeline: load CSV, filter, aggregate, embed, and save.\n\nParameters\n----------\ncsv_path : str\n    Path to input CSV file\noutput_path : str\n    Path to save embeddings JSON\nmin_confidence : float\n    Minimum confidence threshold\nsuggestion_column : str\n    Name of the suggestion column in CSV\nnormalize_keywords : bool\n    Whether to normalize keywords (lowercase, strip)\nbatch_size : int\n    Batch size for embedding generation\nshow_progress : bool\n    Whether to show progress bar\n\nReturns\n-------\nDict[str, np.ndarray]\n    Dictionary mapping keywords to embeddings\n\nExamples\n--------\n>>> embedder = KeywordEmbedder()\n>>> embeddings = embedder.process_csv_to_embeddings(\n...     'suggestions.csv',\n...     'embeddings.json',\n...     min_confidence=0.7\n... )"
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/embeddings_ops/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "embeddings_ops",
    "package": "RAG_supporters",
    "module_docstring": "Embedding operations for batch generation and validation.\n\nThis module provides tools for generating embeddings with validation,\nsanity checks, and batch processing support.\n\nKey Features:\n- Batch embedding generation with progress tracking\n- Model type detection (sentence-transformers or LangChain)\n- NaN/Inf detection and validation\n- Centroid similarity validation\n- Memory-efficient batch processing\n- Steering embedding generation with multiple modes\n\nExamples\n--------\n>>> from RAG_supporters.embeddings_ops import EmbeddingGenerator, generate_embeddings\n>>> from sentence_transformers import SentenceTransformer\n>>>\n>>> model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n>>> generator = EmbeddingGenerator(model, cluster_parser)\n>>>\n>>> # Generate embeddings for all dataset components\n>>> embeddings = generator.generate_all_embeddings(df)\n>>> print(embeddings.keys())\ndict_keys(['question_embs', 'source_embs', 'keyword_embs', 'centroid_embs', ...])",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/embeddings_ops/embed.py",
    "file": "embed.py",
    "module": "embed",
    "parent_module": "embeddings_ops",
    "package": "RAG_supporters",
    "module_docstring": "Embedding Generator for JASPER Steering Dataset Builder.\n\nThis module handles batch embedding generation for questions, sources, keywords,\nand cluster centroids. Includes sanity checks for NaN/Inf values and centroid\nsimilarity validation.\n\nKey Features:\n- Batch embedding generation for all text types\n- Automatic model type detection (sentence-transformers or LangChain)\n- Sanity checks: NaN/Inf detection, centroid similarity validation\n- Progress tracking with tqdm\n- Memory-efficient batch processing\n- Outputs: PyTorch tensors saved as *.pt files",
    "classes": {
      "EmbeddingGenerator": {
        "docstring": "Generate and validate embeddings for dataset components.\n\nThis class handles embedding generation for all dataset components:\nquestions, sources, keywords, and cluster centroids. Performs sanity\nchecks to ensure embedding quality before saving.\n\nParameters\n----------\nembedding_model : Any\n    Embedding model (sentence-transformers or LangChain)\ncluster_parser : ClusterParser, optional\n    Parser for cluster metadata (required for centroid validation)\nbatch_size : int, optional\n    Batch size for embedding generation (default: 32)\nshow_progress : bool, optional\n    Show progress bars (default: True)\nnormalize_embeddings : bool, optional\n    L2-normalize embeddings (default: False)\n\nAttributes\n----------\nembedder : KeywordEmbedder\n    Unified embedding interface\ncluster_parser : ClusterParser, optional\n    Cluster metadata parser\nbatch_size : int\n    Batch size for processing\nshow_progress : bool\n    Whether to show progress bars\nnormalize_embeddings : bool\n    Whether to L2-normalize embeddings\n\nExamples\n--------\n>>> from sentence_transformers import SentenceTransformer\n>>> from RAG_supporters.dataset import EmbeddingGenerator, ClusterParser\n>>>\n>>> model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n>>> parser = ClusterParser(\"clusters.json\")\n>>> generator = EmbeddingGenerator(model, parser)\n>>>\n>>> # Generate question embeddings\n>>> questions = [\"What is Python?\", \"What is Java?\"]\n>>> question_embs = generator.generate_text_embeddings(questions)\n>>> print(question_embs.shape)\ntorch.Size([2, 384])",
        "methods": {
          "__init__": "Initialize embedding generator.",
          "_check_for_invalid_values": "Check embeddings for NaN or Inf values.\n\nParameters\n----------\nembeddings : np.ndarray\n    Embeddings array to check\ntext_type : str\n    Type of embeddings (for error messages)\n\nReturns\n-------\nTuple[bool, List[str]]\n    (is_valid, list_of_errors)",
          "_validate_centroid_similarity": "Validate that centroids are similar to their cluster keywords.\n\nFor each cluster, computes cosine similarity between centroid and\nkeywords in that cluster. Checks that average similarity is above\nthreshold.\n\nParameters\n----------\nkeyword_embeddings : Dict[str, np.ndarray]\n    Mapping from keyword strings to embeddings\ncentroid_embeddings : np.ndarray\n    Cluster centroid embeddings [n_clusters, dim]\nmin_similarity : float, optional\n    Minimum average similarity required (default: 0.3)\n\nReturns\n-------\nTuple[bool, List[str]]\n    (is_valid, list_of_warnings)",
          "generate_text_embeddings": "Generate embeddings for a list of texts.\n\nParameters\n----------\ntexts : List[str]\n    List of texts to embed\ntext_type : str, optional\n    Type of text (for logging and validation)\nvalidate : bool, optional\n    Perform sanity checks on embeddings (default: True)\n\nReturns\n-------\ntorch.Tensor\n    Embeddings tensor [n_texts, embedding_dim]\n\nRaises\n------\nValueError\n    If embeddings contain NaN or Inf values\n\nExamples\n--------\n>>> generator = EmbeddingGenerator(model)\n>>> texts = [\"Sample text 1\", \"Sample text 2\"]\n>>> embeddings = generator.generate_text_embeddings(texts, text_type=\"source\")\n>>> print(embeddings.shape)\ntorch.Size([2, 384])",
          "generate_keyword_embeddings": "Generate embeddings for keywords.\n\nParameters\n----------\nkeywords : List[str]\n    List of unique keywords\nvalidate : bool, optional\n    Perform sanity checks (default: True)\n\nReturns\n-------\nTuple[torch.Tensor, Dict[str, int]]\n    (embeddings_tensor, keyword_to_id_mapping)\n    - embeddings_tensor: [n_keywords, embedding_dim]\n    - keyword_to_id_mapping: Maps keyword string to index\n\nExamples\n--------\n>>> generator = EmbeddingGenerator(model)\n>>> keywords = [\"python\", \"java\", \"programming\"]\n>>> embs, kw_map = generator.generate_keyword_embeddings(keywords)\n>>> print(embs.shape, len(kw_map))\ntorch.Size([3, 384]) 3",
          "generate_centroid_embeddings": "Generate or load cluster centroid embeddings.\n\nIf cluster_parser is provided, loads precomputed centroids from\nKeywordClusterer JSON and validates them against keyword embeddings.\n\nParameters\n----------\nkeyword_embeddings_dict : Dict[str, np.ndarray], optional\n    Keyword embeddings for validation\nvalidate : bool, optional\n    Perform sanity checks (default: True)\nmin_similarity : float, optional\n    Minimum average similarity for centroid validation (default: 0.3)\n\nReturns\n-------\ntorch.Tensor\n    Centroid embeddings [n_clusters, embedding_dim]\n\nRaises\n------\nValueError\n    If cluster_parser is None or centroids are invalid\n\nExamples\n--------\n>>> generator = EmbeddingGenerator(model, cluster_parser)\n>>> centroids = generator.generate_centroid_embeddings()\n>>> print(centroids.shape)\ntorch.Size([20, 384])",
          "generate_all_embeddings": "Generate embeddings for all dataset components.\n\nGenerates embeddings for:\n- Unique questions\n- Unique sources\n- Unique keywords\n- Cluster centroids (if cluster_parser provided)\n\nParameters\n----------\ndf : pd.DataFrame\n    DataFrame with question-source pairs\nquestion_col : str, optional\n    Column name for questions (default: \"question\")\nsource_col : str, optional\n    Column name for sources (default: \"source\")\nkeywords_col : str, optional\n    Column name for keywords (default: \"keywords\")\nvalidate : bool, optional\n    Perform sanity checks (default: True)\n\nReturns\n-------\nDict[str, torch.Tensor]\n    Dictionary with keys:\n    - \"question_embs\": [n_questions, dim]\n    - \"source_embs\": [n_sources, dim]\n    - \"keyword_embs\": [n_keywords, dim]\n    - \"centroid_embs\": [n_clusters, dim] (if cluster_parser provided)\n    - \"question_to_id\": Dict[str, int]\n    - \"source_to_id\": Dict[str, int]\n    - \"keyword_to_id\": Dict[str, int]\n\nExamples\n--------\n>>> generator = EmbeddingGenerator(model, cluster_parser)\n>>> df = pd.read_csv(\"merged.csv\")\n>>> embeddings = generator.generate_all_embeddings(df)\n>>> print(embeddings.keys())\ndict_keys(['question_embs', 'source_embs', 'keyword_embs', 'centroid_embs', ...])",
          "save_embeddings": "Save embeddings to PyTorch tensor files.\n\nParameters\n----------\nembeddings_dict : Dict[str, torch.Tensor]\n    Dictionary of embeddings to save\noutput_dir : str or Path\n    Output directory for tensor files\nprefix : str, optional\n    Prefix for output files (default: \"\")\n\nExamples\n--------\n>>> generator = EmbeddingGenerator(model)\n>>> embeddings = generator.generate_all_embeddings(df)\n>>> generator.save_embeddings(embeddings, \"output/\")"
        }
      }
    },
    "functions": {
      "generate_embeddings": "Convenience function to generate and optionally save embeddings.\n\nParameters\n----------\ndf : pd.DataFrame\n    DataFrame with question-source pairs\nembedding_model : Any\n    Embedding model (sentence-transformers or LangChain)\ncluster_parser : ClusterParser, optional\n    Parser for cluster metadata\noutput_dir : str or Path, optional\n    If provided, save embeddings to this directory\nquestion_col : str, optional\n    Column name for questions (default: \"question\")\nsource_col : str, optional\n    Column name for sources (default: \"source\")\nkeywords_col : str, optional\n    Column name for keywords (default: \"keywords\")\nbatch_size : int, optional\n    Batch size for embedding generation (default: 32)\nnormalize_embeddings : bool, optional\n    L2-normalize embeddings (default: False)\nvalidate : bool, optional\n    Perform sanity checks (default: True)\n\nReturns\n-------\nDict[str, torch.Tensor]\n    Dictionary of embeddings and mappings\n\nExamples\n--------\n>>> from sentence_transformers import SentenceTransformer\n>>> from RAG_supporters.dataset import generate_embeddings, ClusterParser\n>>>\n>>> model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n>>> parser = ClusterParser(\"clusters.json\")\n>>> df = pd.read_csv(\"merged.csv\")\n>>>\n>>> embeddings = generate_embeddings(\n...     df=df,\n...     embedding_model=model,\n...     cluster_parser=parser,\n...     output_dir=\"output/\"\n... )"
    }
  },
  {
    "path": "RAG_supporters/embeddings_ops/steering_config.py",
    "file": "steering_config.py",
    "module": "steering_config",
    "parent_module": "embeddings_ops",
    "package": "RAG_supporters",
    "module_docstring": "Steering configuration with embedded steering modes.",
    "classes": {
      "SteeringMode": {
        "docstring": "Steering embedding modes for cluster/subspace steering.",
        "methods": {}
      },
      "SteeringConfig": {
        "docstring": "Configuration for steering embeddings.\n\nAttributes:\n    mode: List of (mode, probability) tuples for steering mode selection\n    multi_label_mode: \"hard\" for one-hot, \"soft\" for probabilities\n    random_seed: Seed for reproducible mode selection",
        "methods": {
          "__post_init__": "Validate configuration after initialization.",
          "from_single_mode": "Create config from a single steering mode.\n\nArgs:\n    mode: Steering mode\n    multi_label_mode: Target label mode\n    random_seed: Random seed\n\nReturns:\n    SteeringConfig instance\n\nRaises:\n    ValueError: If mode is not a SteeringMode instance",
          "from_mode_list": "Create config from list of modes with probabilities.\n\nArgs:\n    modes: List of (mode, probability) tuples\n    multi_label_mode: Target label mode\n    random_seed: Random seed\n\nReturns:\n    SteeringConfig instance",
          "get_mode_probabilities": "Get mode probabilities as a dictionary.\n\nReturns:\n    Dictionary mapping modes to probabilities",
          "has_mode": "Check if a specific mode is configured.\n\nArgs:\n    mode: Steering mode to check\n\nReturns:\n    True if mode is in configuration"
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/embeddings_ops/steering_embedding_generator.py",
    "file": "steering_embedding_generator.py",
    "module": "steering_embedding_generator",
    "parent_module": "embeddings_ops",
    "package": "RAG_supporters",
    "module_docstring": "Steering embedding generator with mode support and augmentations.",
    "classes": {
      "SteeringEmbeddingGenerator": {
        "docstring": "Generates steering embeddings based on configured modes.\n\nSupports:\n- SUGGESTION: Uses suggestion embeddings\n- CLUSTER_DESCRIPTOR: Uses cluster descriptor embeddings\n- LLM_GENERATED: Uses LLM-generated steering text embeddings\n- ZERO: Returns zero vector (no steering)\n\nIntegrates augmentations from embedding.py for noise injection.",
        "methods": {
          "__init__": "Initialize steering embedding generator.\n\nArgs:\n    config: Steering configuration with modes\n    clustering_data: Cluster data with centroids and descriptors\n    embedding_model: Model name (str), KeywordEmbedder instance, or raw model.\n                   If string: creates KeywordEmbedder with model_name.\n                   Supports both sentence-transformers and LangChain models.\n    suggestion_embeddings: Pre-computed suggestion embeddings {suggestion: embedding}\n    llm_steering_texts: LLM-generated steering texts {key: text}\n    augment_noise_prob: Probability of applying noise augmentation\n    augment_zero_prob: Probability of zeroing steering embedding\n    augment_noise_level: Std deviation for noise augmentation",
          "generate": "Generate steering embedding for a sample.\n\nArgs:\n    sample_id: Sample identifier\n    suggestions: List of suggestion strings or dicts with 'term' field\n    cluster_id: Primary cluster ID for descriptor mode\n\nReturns:\n    Tuple of (steering_embedding, mode_used)",
          "_select_mode": "Select steering mode based on configured probabilities.",
          "_generate_zero_embedding": "Generate zero embedding (no steering).",
          "_generate_suggestion_embedding": "Generate embedding from suggestions.\n\nArgs:\n    suggestions: List of suggestion strings or dicts with 'term' field\n\nReturns:\n    Suggestion embedding (averaged if multiple)",
          "_generate_descriptor_embedding": "Generate embedding from cluster descriptors.\n\nArgs:\n    cluster_id: Cluster ID\n\nReturns:\n    Descriptor embedding (averaged)",
          "_generate_llm_embedding": "Generate embedding from LLM-generated steering text.\n\nArgs:\n    sample_id: Sample identifier\n\nReturns:\n    LLM steering text embedding",
          "_apply_augmentations": "Apply augmentations from embedding.py.\n\nArgs:\n    embedding: Original embedding\n\nReturns:\n    Augmented embedding"
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/jasper/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "jasper",
    "package": "RAG_supporters",
    "module_docstring": "JASPER-specific dataset builder orchestration.\n\nThis module implements the JASPER dataset builder pipeline (Tasks 1-9):\n- Task 1: CSV merging and deduplication\n- Task 2: Cluster parsing and keyword linking\n- Task 3: Embedding generation\n- Task 4: Steering signal computation\n- Task 5: Hard negative mining\n- Task 6: Dataset splitting\n- Task 7: Artifact persistence\n- Task 8: Validation and finalization\n- Task 9: Orchestration and timing\n\nThese tools are specific to JASPER but demonstrate patterns for building\nother contrastive learning datasets.\n\nKey Features:\n- End-to-end pipeline orchestration\n- Task timing and profiling\n- Configuration management (BuildConfig)\n- SQLite storage backend\n- Cross-validation and integrity checks\n- Reproducible builds with random seeds\n\nExamples\n--------\n>>> from RAG_supporters.jasper import build_dataset, BuildConfig\n>>> from sentence_transformers import SentenceTransformer\n>>>\n>>> # Build complete JASPER dataset\n>>> config = build_dataset(\n...     csv_paths=[\"data1.csv\", \"data2.csv\"],\n...     cluster_json_path=\"clusters.json\",\n...     embedding_model=SentenceTransformer(\"all-MiniLM-L6-v2\"),\n...     output_dir=\"output/dataset\",\n...     n_neg=12,\n...     split_ratios=[0.7, 0.15, 0.15],\n...     random_seed=42\n... )\n>>> print(f\"Built dataset with {config.n_pairs} pairs\")\n>>>\n>>> # Load configuration later\n>>> loaded_config = BuildConfig.load(\"output/dataset/config.json\")",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/jasper/build.py",
    "file": "build.py",
    "module": "build",
    "parent_module": "jasper",
    "package": "RAG_supporters",
    "module_docstring": "Build orchestrator for JASPER Steering Dataset pipeline.\n\nThis module implements Task 9 of the dataset builder pipeline:\n- Run Tasks 1-8 in sequence\n- Persist all required PT artifacts\n- Log per-task timing and overall runtime",
    "classes": {},
    "functions": {
      "_build_config_from_input": "Normalize user-provided config into BuildConfig.",
      "_to_keyword_id_lists": "Convert pair keywords to keyword ID lists.",
      "_compute_source_cluster_ids": "Compute a primary cluster assignment for each source.\n\nMajority voting over pair-level cluster assignments is used.\nTies are broken by selecting the smallest cluster ID.",
      "_save_pair_level_artifacts": "Build and save pair-level tensors required by downstream tasks.",
      "_timed_task": "Execute a task and record elapsed time in seconds.",
      "build_dataset": "Build JASPER steering dataset by orchestrating Tasks 1-8.\n\nParameters\n----------\ncsv_paths : List[str or Path]\n    Input CSV files with question-source pairs.\ncluster_json_path : str or Path\n    Path to KeywordClusterer JSON output.\nembedding_model : Any\n    Embedding model accepted by ``KeywordEmbedder``.\noutput_dir : str or Path\n    Target directory for built dataset artifacts.\nconfig : BuildConfig or Dict[str, Any], optional\n    Optional preconfigured BuildConfig or dict payload.\nstorage_format : str, optional\n    Storage format (currently only ``pt`` is supported end-to-end).\ninclude_inspection_file : bool, optional\n    Whether to write optional ``inspection.json``.\n\nReturns\n-------\nBuildConfig\n    Final validated configuration persisted to ``config.json``."
    }
  },
  {
    "path": "RAG_supporters/jasper/builder_config.py",
    "file": "builder_config.py",
    "module": "builder_config",
    "parent_module": "jasper",
    "package": "RAG_supporters",
    "module_docstring": "Configuration dataclass for JASPER Steering Dataset Builder.\n\nThis module provides build configuration with JSON serialization support\nfor reproducible dataset construction.",
    "classes": {
      "BuildConfig": {
        "docstring": "Configuration for building JASPER Steering Dataset.\n\nThis configuration is saved as config.json in the output directory\nand serves as the single source of truth for dataset metadata.\n\nParameters\n----------\nembedding_dim : int\n    Dimension of embeddings (e.g., 384 for all-MiniLM-L6-v2)\nn_neg : int\n    Number of hard negatives per positive pair\nclustering_source : str\n    Path to KeywordClusterer JSON results file\nsplit_ratios : List[float]\n    Train/val/test split ratios, must sum to 1.0\nsteering_probabilities : Dict[str, float]\n    Probabilities for steering variants:\n    - \"zero\": Zero vector (no steering)\n    - \"centroid\": Cluster centroid direction\n    - \"keyword\": Keyword-weighted direction\n    - \"residual\": Residual from centroid\n    Must sum to 1.0\ncurriculum : Dict[str, Union[float, str]]\n    Curriculum learning configuration:\n    - \"mode\": \"fixed\" | \"linear\" | \"cosine\"\n    - \"start_distance\": Initial centroid distance threshold\n    - \"end_distance\": Final distance threshold\n    - \"warmup_epochs\": Epochs to transition from start to end\nn_pairs : int, optional\n    Total number of question-source pairs (computed during build)\nn_questions : int, optional\n    Number of unique questions (computed during build)\nn_sources : int, optional\n    Number of unique sources (computed during build)\nn_keywords : int, optional\n    Number of unique keywords (computed during build)\nn_clusters : int, optional\n    Number of clusters from KeywordClusterer (computed during build)\nstorage_format : str\n    Storage format: \"pt\" (PyTorch) or \"hdf5\"\ninclude_inspection_file : bool\n    Whether to generate optional inspection.json for debugging\nrandom_seed : int\n    Random seed for deterministic splitting and sampling\n\nExamples\n--------\n>>> config = BuildConfig(\n...     embedding_dim=384,\n...     n_neg=12,\n...     clustering_source=\"clusters.json\",\n...     split_ratios=[0.8, 0.1, 0.1],\n...     steering_probabilities={\"zero\": 0.25, \"centroid\": 0.25,\n...                             \"keyword\": 0.25, \"residual\": 0.25},\n...     curriculum={\"mode\": \"linear\", \"start_distance\": 0.3,\n...                 \"end_distance\": 0.7, \"warmup_epochs\": 10}\n... )\n>>> config.save(\"output/config.json\")\n>>> loaded = BuildConfig.load(\"output/config.json\")",
        "methods": {
          "__post_init__": "Validate configuration values.",
          "save": "Save configuration to JSON file.\n\nParameters\n----------\npath : str or Path\n    Output path for config.json\n\nExamples\n--------\n>>> config.save(\"output_dir/config.json\")",
          "load": "Load configuration from JSON file.\n\nParameters\n----------\npath : str or Path\n    Path to config.json\n\nReturns\n-------\nBuildConfig\n    Loaded configuration object\n\nExamples\n--------\n>>> config = BuildConfig.load(\"output_dir/config.json\")",
          "to_dict": "Convert configuration to dictionary.\n\nReturns\n-------\nDict\n    Configuration as dictionary",
          "update_post_build": "Update configuration with computed statistics after build.\n\nCalled by build orchestrator after dataset construction is complete.\n\nParameters\n----------\nn_pairs : int\n    Total number of question-source pairs\nn_questions : int\n    Number of unique questions\nn_sources : int\n    Number of unique sources\nn_keywords : int\n    Number of unique keywords\nn_clusters : int\n    Number of clusters"
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/jasper/finalize.py",
    "file": "finalize.py",
    "module": "finalize",
    "parent_module": "jasper",
    "package": "RAG_supporters",
    "module_docstring": "Finalize and validate JASPER dataset builder outputs.\n\nThis module implements Task 8 of the dataset builder pipeline:\n- Cross-validate all generated output tensors/files\n- Verify referential integrity and dimensional consistency\n- Produce final ``config.json`` as single source of truth",
    "classes": {
      "DatasetFinalizer": {
        "docstring": "Validate built dataset artifacts and write final config.\n\nParameters\n----------\noutput_dir : str or Path\n    Dataset output directory containing generated artifacts.",
        "methods": {
          "__init__": "Initialize finalizer.",
          "_validate_required_files_pt": "Validate all required PT artifacts exist.",
          "_load_pt_artifacts": "Load all PT artifacts.\n\nReturns\n-------\nDict[str, object]\n    Loaded tensors and pair keyword IDs list.",
          "_validate_embedding_tensor": "Validate embedding tensor shape and return embedding dim.",
          "_validate_pair_keyword_ids": "Validate pair keyword IDs list structure and bounds.",
          "_validate_splits": "Validate split tensors cover all pairs without overlap.",
          "_validate_pt_artifacts": "Validate loaded PT artifacts and return computed statistics.",
          "_resolve_config": "Resolve config source: argument, existing file, or minimal constructor.",
          "finalize": "Validate outputs and write final config.json.\n\nParameters\n----------\nconfig : BuildConfig, optional\n    Existing build configuration to enrich with computed statistics.\nclustering_source : str or Path, optional\n    Optional override for clustering JSON path in final config.\nsave : bool, optional\n    Whether to persist final config.json (default: True).\n\nReturns\n-------\nBuildConfig\n    Final validated configuration."
        }
      }
    },
    "functions": {
      "finalize_dataset": "Convenience function for dataset finalization.\n\nParameters\n----------\noutput_dir : str or Path\n    Dataset output directory.\nconfig : BuildConfig, optional\n    Optional BuildConfig to enrich and validate.\nclustering_source : str or Path, optional\n    Optional clustering JSON path override.\nsave : bool, optional\n    Whether to write ``config.json`` to disk (default: True).\n\nReturns\n-------\nBuildConfig\n    Final validated configuration."
    }
  },
  {
    "path": "RAG_supporters/jasper/sqlite_storage.py",
    "file": "sqlite_storage.py",
    "module": "sqlite_storage",
    "parent_module": "jasper",
    "package": "RAG_supporters",
    "module_docstring": "SQLite storage manager for domain assessment dataset.",
    "classes": {
      "SQLiteStorageManager": {
        "docstring": "Manages SQLite database for dataset metadata, labels, and text fields.\n\nSchema:\n- samples: Main table with metadata, labels, text\n- embeddings_meta: Tracks numpy memmap files for embeddings\n- dataset_info: Dataset-level metadata",
        "methods": {
          "__init__": "Initialize SQLite storage manager.\n\nArgs:\n    db_path: Path to SQLite database file",
          "_init_database": "Initialize database schema if not exists.",
          "insert_sample": "Insert a sample into the database.\n\nArgs:\n    sample_type: 'source' or 'question'\n    text: Source or question text\n    source_label: Label for source/question (numpy array)\n    steering_label: Label for steering embedding (numpy array)\n    combined_label: Combined label (numpy array)\n    embedding_idx: Index into memmap file\n    chroma_id: ChromaDB ID\n    suggestions: List of suggestion strings\n    steering_mode: Steering mode used\n\nReturns:\n    sample_id of inserted row",
          "get_sample": "Retrieve a sample by ID.\n\nArgs:\n    sample_id: Sample ID\n\nReturns:\n    Dictionary with sample data or None",
          "get_all_samples": "Retrieve all samples.\n\nReturns:\n    List of sample dictionaries",
          "get_dataset_size": "Get total number of samples in dataset.\n\nReturns:\n    Number of samples",
          "get_sample_by_index": "Retrieve sample by its index (0-based position in ordered dataset).\n\nArgs:\n    idx: Sample index (0-based)\n\nReturns:\n    Sample dictionary or None if index out of range",
          "update_labels": "Update labels for a sample.\n\nArgs:\n    sample_id: Sample ID\n    source_label: New source label (if provided)\n    steering_label: New steering label (if provided)\n    combined_label: New combined label (if provided)",
          "register_embedding_file": "Register a numpy memmap file for embeddings.\n\nArgs:\n    embedding_type: 'base' or 'steering'\n    file_path: Path to .npy file\n    shape: Shape tuple (n_samples, embedding_dim)\n    dtype: Numpy dtype string",
          "get_embedding_file_info": "Get info about a registered embedding file.\n\nArgs:\n    embedding_type: 'base' or 'steering'\n\nReturns:\n    Dictionary with file info or None",
          "set_dataset_info": "Store dataset-level metadata.\n\nArgs:\n    key: Metadata key\n    value: Value (will be JSON-encoded)",
          "get_dataset_info": "Retrieve dataset-level metadata.\n\nArgs:\n    key: Metadata key\n\nReturns:\n    Value (JSON-decoded) or None",
          "get_sample_count": "Get total number of samples.",
          "_row_to_dict": "Convert SQLite row to dictionary with parsed JSON fields.",
          "_compute_file_checksum": "Compute SHA256 checksum of a file.",
          "close": "Close database connection.",
          "__enter__": "Enter context manager.",
          "__exit__": "Exit context manager and close connection."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/nn/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "nn",
    "package": "RAG_supporters",
    "module_docstring": "Neural network models package.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/nn/inference/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "inference",
    "package": "RAG_supporters",
    "module_docstring": "JASPER inference and XAI utilities.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/nn/inference/xai_interface.py",
    "file": "xai_interface.py",
    "module": "xai_interface",
    "parent_module": "inference",
    "package": "RAG_supporters",
    "module_docstring": "XAIInterface: explainability wrapper for JASPERPredictor and DecomposedJASPERPredictor.",
    "classes": {
      "XAIInterface": {
        "docstring": "Explainability interface for JASPER models.\n\nWorks with both :class:`~RAG_supporters.nn.models.decomposed_predictor.DecomposedJASPERPredictor`\n(full XAI) and :class:`~RAG_supporters.nn.models.jasper_predictor.JASPERPredictor`\n(limited XAI via proxy routing from centroid similarity).\n\nArgs:\n    model: A ``DecomposedJASPERPredictor`` or ``JASPERPredictor`` instance.\n    centroid_embs: Cluster centroid embeddings ``[K, D]``.\n    cluster_names: K human-readable subspace names.\n    training_pairs: Optional list of ``(question_emb [D], steering_emb [D],\n        source_emb [D])`` tuples used for nearest-neighbour retrieval in\n        :meth:`explain_prediction`.\n    device: Torch device.  ``None`` \u2192 auto-detect.",
        "methods": {
          "__init__": null,
          "explain_prediction": "Produce a full explanation for one question/steering pair.\n\nArgs:\n    question_emb: ``[D]`` or ``[1, D]``.\n    steering_emb: ``[D]`` or ``[1, D]``.\n\nReturns:\n    Dict with keys:\n\n    - ``\"prediction\"`` \u2014 predicted source embedding as a Python list.\n    - ``\"primary_subspace\"`` \u2014 name of the dominant routing cluster.\n    - ``\"primary_confidence\"`` \u2014 float confidence of the dominant cluster.\n    - ``\"routing_distribution\"`` \u2014 ``{cluster_name: float}`` dict.\n    - ``\"routing_entropy\"`` \u2014 routing entropy in nats.\n    - ``\"atypicality\"`` \u2014 \u2016fine\u2016 (or proxy for JASPERPredictor).\n    - ``\"coarse_vector\"`` \u2014 ``[D]`` list (``None`` for JASPERPredictor).\n    - ``\"fine_vector\"`` \u2014 ``[D]`` list (``None`` for JASPERPredictor).\n    - ``\"steering_influence\"`` \u2014 KL divergence measuring how much the\n      steering vector shifts the routing.\n    - ``\"similar_pairs\"`` \u2014 up to 5 nearest training pairs (list of dicts).\n    - ``\"actionable_signal\"`` \u2014 human-readable summary string.",
          "compare_steering_influence": "Compare how different steering vectors affect routing and prediction.\n\nArgs:\n    question_emb: ``[D]`` single question embedding.\n    steering_embs: List of N steering embeddings, each ``[D]``.\n    labels: Optional list of N human-readable labels.\n\nReturns:\n    Dict with:\n        - ``\"labels\"``: list of N labels.\n        - ``\"routing_matrices\"``: list of N routing distributions (list of K floats).\n        - ``\"predictions\"``: list of N predicted embeddings (as Python lists).\n        - ``\"atypicalities\"``: list of N floats.\n        - ``\"routing_kl_from_first\"``: KL(steer_i \u2016 steer_0) for i in 1..N-1.",
          "visualize_explanation": "Plot a three-panel explanation figure.\n\nPanels:\n1. Routing distribution (bar chart, primary subspace highlighted).\n2. Coarse vs. Fine magnitude (bar or single-value indicator).\n3. Steering influence (KL divergence gauge).\n\nArgs:\n    xai_dict: Output of :meth:`explain_prediction`.\n    title: Optional figure title.\n    save_path: If given, saves the figure to this path.\n\nReturns:\n    ``matplotlib.figure.Figure`` or ``None`` if matplotlib is unavailable.",
          "save_xai_outputs": "Serialise a list of XAI explanation dicts to JSON.\n\nArgs:\n    xai_results: List of :meth:`explain_prediction` outputs.\n    output_path: Path for the JSON file.",
          "_explain_decomposed": "Full XAI for DecomposedJASPERPredictor.",
          "_compute_steering_influence_decomposed": "KL divergence measuring how much the steering changes the routing.",
          "_explain_jasper": "Limited XAI for base JASPERPredictor via proxy routing.",
          "_compute_steering_influence_jasper": "Proxy steering influence for JASPERPredictor.",
          "_find_similar_pairs": "Find training pairs similar to this query by cosine similarity.",
          "_build_actionable_signal": "Build a concise human-readable summary from an XAI dict.",
          "__repr__": null
        }
      }
    },
    "functions": {
      "_json_default": "JSON serialiser for numpy/torch scalars."
    }
  },
  {
    "path": "RAG_supporters/nn/losses/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "losses",
    "package": "RAG_supporters",
    "module_docstring": "JASPER loss functions.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/nn/losses/jasper_losses.py",
    "file": "jasper_losses.py",
    "module": "jasper_losses",
    "parent_module": "losses",
    "package": "RAG_supporters",
    "module_docstring": "Multi-objective loss functions for JASPER model training.",
    "classes": {
      "JASPERLoss": {
        "docstring": "SmoothL1 (Huber) loss between predicted and EMA-target embeddings.\n\nMeasures how accurately the predictor reconstructs the target encoder's\nrepresentation.  SmoothL1 is preferred over MSE because it is less\nsensitive to outliers in the embedding space.\n\nArgs:\n    beta: Transition point between L1 and L2 behaviour (SmoothL1 ``beta``).\n    reduction: ``\"mean\"`` or ``\"sum\"``.",
        "methods": {
          "__init__": null,
          "forward": "Compute JASPER prediction loss.\n\nArgs:\n    predicted: Predicted source embeddings ``[B, D]``.\n    ema_target: EMA target encoder output ``[B, D]``.\n\nReturns:\n    Dict with key ``\"jasper\"`` containing the scalar loss."
        }
      },
      "ContrastiveLoss": {
        "docstring": "InfoNCE contrastive loss with hard negatives.\n\nPulls the predicted embedding toward the positive target while pushing it\naway from hard negative examples in the batch.\n\nArgs:\n    temperature: Softmax temperature \u03c4.  Lower = sharper distribution.\n    reduction: ``\"mean\"`` or ``\"sum\"``.",
        "methods": {
          "__init__": null,
          "forward": "Compute InfoNCE contrastive loss.\n\nArgs:\n    predicted: Predicted embeddings ``[B, D]``.\n    target: Positive target embeddings ``[B, D]``.\n    negatives: Hard negative embeddings ``[B, K, D]`` where K is the\n        number of negatives per sample.\n\nReturns:\n    Dict with key ``\"contrastive\"`` containing the scalar loss."
        }
      },
      "CentroidLoss": {
        "docstring": "Cross-entropy loss that asks: does the prediction land in the right cluster?\n\nUses cosine similarity between the predicted embedding and each cluster\ncentroid as logits, then applies cross-entropy against the ground-truth\ncluster ID.\n\nArgs:\n    temperature: Softmax temperature applied to centroid similarities.",
        "methods": {
          "__init__": null,
          "forward": "Compute centroid classification loss.\n\nArgs:\n    predicted_emb: Predicted source embeddings ``[B, D]``.\n    centroid_embs: Cluster centroid embeddings ``[C, D]`` where C is\n        the number of clusters.\n    cluster_ids: Ground-truth cluster indices ``[B]`` (long tensor).\n\nReturns:\n    Dict with keys ``\"centroid\"`` (total loss) and\n    ``\"centroid_acc\"`` (top-1 accuracy scalar, no grad)."
        }
      },
      "VICRegLoss": {
        "docstring": "VICReg regularisation: Variance + Invariance + Covariance.\n\nPrevents embedding collapse without requiring negative pairs.\n\n- **Variance**: keeps per-dimension std above a threshold \u03b3.\n- **Invariance**: SmoothL1 between two views (here predicted vs. target).\n- **Covariance**: penalises off-diagonal covariance entries to decorrelate dims.\n\nReference: Bardes et al. 2022 \"VICReg: Variance-Invariance-Covariance\nRegularization for Self-Supervised Learning\".\n\nArgs:\n    lambda_v: Weight for variance term.\n    lambda_i: Weight for invariance term.\n    lambda_c: Weight for covariance term.\n    gamma: Target std threshold for variance term.\n    eps: Small constant for numerical stability.",
        "methods": {
          "__init__": null,
          "forward": "Compute VICReg loss.\n\nArgs:\n    predicted: Predicted embeddings ``[B, D]``.\n    target: Target (e.g. EMA) embeddings ``[B, D]``.\n\nReturns:\n    Dict with keys ``\"vicreg\"``, ``\"vicreg_v\"``, ``\"vicreg_i\"``,\n    ``\"vicreg_c\"`` for total and individual components.",
          "_variance_loss": "Hinge loss on per-dimension standard deviation.",
          "_covariance_loss": "Off-diagonal covariance penalty."
        }
      },
      "JASPERMultiObjectiveLoss": {
        "docstring": "Combined multi-objective loss for JASPER training.\n\nCombines :class:`JASPERLoss`, :class:`ContrastiveLoss`,\n:class:`CentroidLoss`, and :class:`VICRegLoss` with configurable\nscalar weights.\n\nArgs:\n    lambda_jasper: Weight for the JASPER prediction loss.\n    lambda_contrastive: Weight for the contrastive (InfoNCE) loss.\n    lambda_centroid: Weight for the centroid classification loss.\n    lambda_vicreg: Weight for VICReg regularisation.\n    jasper_beta: SmoothL1 beta for :class:`JASPERLoss`.\n    contrastive_temperature: Temperature for :class:`ContrastiveLoss`.\n    centroid_temperature: Temperature for :class:`CentroidLoss`.\n    vicreg_lambda_v: VICReg variance weight.\n    vicreg_lambda_i: VICReg invariance weight.\n    vicreg_lambda_c: VICReg covariance weight.",
        "methods": {
          "__init__": null,
          "forward": "Compute weighted multi-objective loss.\n\nArgs:\n    predicted: Predicted source embeddings ``[B, D]``.\n    ema_target: EMA target encoder output ``[B, D]``.\n    negatives: Hard negative embeddings ``[B, K, D]``.\n    centroid_embs: Cluster centroid embeddings ``[C, D]``.\n    cluster_ids: Ground-truth cluster indices ``[B]``.\n\nReturns:\n    Tuple of:\n    - ``total_loss``: Weighted scalar loss (gradient-bearing).\n    - ``loss_dict``: Dict of all individual loss components\n      (for logging; detached except total).",
          "__repr__": null
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/nn/losses/routing_losses.py",
    "file": "routing_losses.py",
    "module": "routing_losses",
    "parent_module": "losses",
    "package": "RAG_supporters",
    "module_docstring": "Routing-specific loss functions for the subspace-routed JASPER model.",
    "classes": {
      "RoutingLoss": {
        "docstring": "Cross-entropy loss supervising the subspace router.\n\nEncourages the router to assign each sample to its ground-truth cluster\n(as provided by the dataset's ``cluster_id`` field).\n\nArgs:\n    weight: Scalar multiplier applied to the loss before returning.\n    label_smoothing: Label smoothing factor in ``[0, 1)``. 0 = disabled.",
        "methods": {
          "__init__": null,
          "forward": "Compute routing cross-entropy loss.\n\nArgs:\n    routing_logits: Raw (pre-softmax) router logits ``[B, K]``.\n    true_cluster_ids: Ground-truth cluster indices ``[B]`` (long tensor).\n\nReturns:\n    Dict with:\n        - ``\"routing\"``: scalar loss (gradient-bearing).\n        - ``\"routing_acc\"``: top-1 accuracy (detached, no grad)."
        }
      },
      "EntropyRegularization": {
        "docstring": "Entropy-annealing regulariser for routing weights.\n\nSteers routing entropy along a linear schedule:\n- **Early training** (epoch 0): pushes entropy toward ``entropy_high``\n  (uniform routing encourages exploration).\n- **Late training** (epoch >= ``anneal_epochs``): pushes entropy toward\n  ``entropy_low`` (confident, peaked routing).\n\nLoss:\n\n    ``L = weight \u00d7 (H(w) \u2212 target_entropy(epoch))\u00b2``\n\nwhere ``H(w) = \u2212\u2211_k w_k log(w_k)`` (per sample, averaged over the batch).\n\nArgs:\n    entropy_high: Target entropy at epoch 0 (nats).  ``None`` \u2192 defaults to\n        ``log(K)`` (the maximum entropy for K categories), inferred from the\n        first ``forward()`` call.\n    entropy_low: Target entropy after ``anneal_epochs`` epochs (nats).\n    anneal_epochs: Number of epochs to anneal from high to low.\n    weight: Scalar multiplier on the loss.\n    eps: Small constant for numerical stability inside the log.",
        "methods": {
          "__init__": null,
          "get_target_entropy": "Return the scheduled target entropy for this epoch.\n\nArgs:\n    current_epoch: Current epoch index (0-based).\n\nReturns:\n    Target entropy in nats.",
          "forward": "Compute entropy regularisation loss.\n\nArgs:\n    routing_weights: Soft routing probabilities ``[B, K]`` (post-Gumbel /\n        post-Softmax).  Values should be >= 0 and sum to 1 along dim=-1.\n    current_epoch: Current epoch index (0-based).\n\nReturns:\n    Dict with:\n        - ``\"entropy_reg\"``: scalar loss (gradient-bearing).\n        - ``\"routing_entropy\"``: mean batch entropy in nats (detached)."
        }
      },
      "ResidualPenalty": {
        "docstring": "Hinge loss penalising large fine (residual) vectors.\n\nEncourages the model to rely on subspace centroids rather than producing\na large residual correction that bypasses the routing mechanism.\n\nLoss:\n\n    ``L = weight \u00d7 mean(relu(\u2016fine\u2016 \u2212 margin))``\n\nArgs:\n    margin: Allowed residual L2 norm.  Samples below this threshold incur\n        zero penalty.  Scale with the typical embedding norm in your dataset:\n        use ~0.5\u20131.5 \u00d7 median(\u2016target_emb\u2016).  Default ``1.0`` is appropriate\n        for unit-normalised embeddings.\n    weight: Scalar multiplier on the loss.",
        "methods": {
          "__init__": null,
          "forward": "Compute residual penalty.\n\nArgs:\n    fine_vector: Fine (residual) embeddings ``[B, D]``.\n\nReturns:\n    Dict with:\n        - ``\"residual_penalty\"``: scalar hinge loss (gradient-bearing).\n        - ``\"residual_norm_mean\"``: mean residual norm (detached, for monitoring)."
        }
      },
      "DisentanglementLoss": {
        "docstring": "Covariance penalty across routing weight axes.\n\nEncourages different routing dimensions to be statistically independent.\nUses the same off-diagonal covariance formula as VICReg (Bardes et al. 2022),\napplied to the routing weight matrix ``[B, K]`` instead of embeddings.\n\nLoss:\n\n    ``L = weight \u00d7 off_diag_cov(routing_weights).pow(2).sum() / K``\n\nArgs:\n    weight: Scalar multiplier on the loss.\n    eps: Added to batch variance for numerical stability.",
        "methods": {
          "__init__": null,
          "forward": "Compute disentanglement (routing covariance) loss.\n\nArgs:\n    routing_weights: Soft routing probabilities ``[B, K]``.\n\nReturns:\n    Dict with ``\"disentanglement\"``: scalar loss (gradient-bearing).",
          "_covariance_loss": "Off-diagonal covariance penalty (same formula as VICRegLoss._covariance_loss)."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/nn/models/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "models",
    "package": "RAG_supporters",
    "module_docstring": "Neural network model implementations and builders.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/nn/models/decomposed_predictor.py",
    "file": "decomposed_predictor.py",
    "module": "decomposed_predictor",
    "parent_module": "models",
    "package": "RAG_supporters",
    "module_docstring": "DecomposedJASPERPredictor: coarse subspace routing + fine residual correction.",
    "classes": {
      "DecomposedJASPERConfig": {
        "docstring": "Configuration for :class:`DecomposedJASPERPredictor`.\n\nArgs:\n    embedding_dim: Dimensionality of input/output embeddings (D).\n    hidden_dim: Width of hidden layers for question/steering encoders (H).\n    num_subspaces: Number of routing subspaces (K).\n        Must match the number of centroid embeddings passed to ``forward()``.\n    num_layers: MLP depth for question/steering encoders and fine MLP.\n    dropout: Dropout probability. 0 disables it.\n    activation: Name of ``torch.nn`` activation class (e.g. ``\"GELU\"``).\n    use_layer_norm: Whether to apply LayerNorm after hidden activations.\n    normalize_output: Whether to L2-normalise the final prediction.\n    router_hidden_dim: Hidden width for the :class:`SubspaceRouter` MLP.\n        Can differ from ``hidden_dim``.\n    router_temperature: Gumbel-Softmax / Softmax temperature for the router.\n    router_gumbel_hard: Whether to use straight-through hard Gumbel-Softmax.\n    router_normalize_input: Whether to L2-normalise the router's input.\n    fine_input_mode: How to combine latents and coarse vector in the fine MLP.\n        ``\"concat\"`` (default): ``fine_mlp`` receives ``[q_latent; s_latent; coarse]``\n        of size ``[B, 2H+D]``.\n        ``\"add\"``: ``fine_mlp`` receives the elementwise sum of projected latents\n        of size ``[B, H]``.",
        "methods": {
          "__post_init__": null,
          "from_dict": "Create config from a plain dictionary (e.g. loaded from YAML)."
        }
      },
      "DecomposedJASPERPredictor": {
        "docstring": "Two-stage JASPER predictor with explicit subspace routing.\n\nArchitecture\n------------\n::\n\n    question_emb [B,D] \u2500\u2500\u25ba question_encoder \u2500\u2500\u25ba q_latent [B,H]  \u2500\u2500\u2510\n                                                                    \u251c\u2500\u25ba fine_mlp \u2500\u2500\u25ba fine [B,D]\n    steering_emb [B,D] \u2500\u2500\u25ba steering_encoder \u2500\u2500\u25ba s_latent [B,H]  \u2500\u2500\u2524        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                                    \u2502        \u2502\n    centroid_embs [K,D] \u25c4\u2500\u2500 routing_weights [B,K] \u25c4\u2500\u2500 router \u25c4\u2500\u2500\u2500\u2500\u2518  coarse[B,D]\n            \u2502\n            \u2514\u2500\u25ba coarse = routing_weights @ centroid_embs [B,D]\n                                                     \u2502\n                                prediction = coarse + fine [B,D]\n\nThe **coarse** vector is a soft weighted sum of concept centroids, giving\nthe prediction a subspace-anchored starting point.  The **fine** (residual)\nvector refines it using the full question and steering context.\n\n``atypicality = \u2016fine\u2016`` measures how much the sample deviates from its\nassigned subspace.\n\nArgs:\n    config: Model configuration.  Can also be passed as a plain ``dict``.",
        "methods": {
          "__init__": null,
          "_init_weights": "Xavier-uniform init for Linear layers, zeros for biases.",
          "forward": "Predict source embedding via subspace routing + residual refinement.\n\nArgs:\n    question_emb: ``[B, D]`` question embedding.\n    steering_emb: ``[B, D]`` steering embedding.\n    centroid_embs: ``[K, D]`` cluster centroid embeddings.\n        K must equal ``config.num_subspaces``.\n    training: If ``None`` (default), uses ``self.training``.\n        Pass ``True`` to force Gumbel-Softmax or ``False`` for Softmax.\n\nReturns:\n    Tuple of:\n        - ``prediction`` ``[B, D]``: predicted source embedding.\n        - ``explanation_dict``: diagnostic tensors (all **detached**):\n            - ``\"routing_weights\"`` ``[B, K]``: soft routing distribution.\n            - ``\"concept_logits\"``  ``[B, K]``: raw router logits.\n            - ``\"coarse\"``          ``[B, D]``: centroid-anchored estimate.\n            - ``\"fine\"``            ``[B, D]``: residual correction.\n            - ``\"atypicality\"``     ``[B]``:   ``\u2016fine\u2016`` per sample.",
          "get_routing_weights": "Return soft routing weights ``[B, K]`` under the current training mode.\n\nArgs:\n    question_emb: ``[B, D]``\n    steering_emb: ``[B, D]``\n\nReturns:\n    Routing probabilities ``[B, K]`` summing to 1 along dim=-1.",
          "get_latent_representations": "Return cached intermediate activations from the last forward pass.\n\nAll tensors are detached and moved to CPU.\n\nReturns:\n    Dict with keys: ``\"question_latent\"``, ``\"steering_latent\"``,\n    ``\"routing_weights\"``, ``\"coarse\"``, ``\"fine\"``, ``\"atypicality\"``.\n    Empty if no forward pass has run yet.",
          "embedding_dim": "Input/output embedding dimension D.",
          "hidden_dim": "Hidden layer width H.",
          "num_subspaces": "Number of routing subspaces K.",
          "get_model_summary": "Return a human-readable parameter summary.",
          "__repr__": null
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/nn/models/ema_encoder.py",
    "file": "ema_encoder.py",
    "module": "ema_encoder",
    "parent_module": "models",
    "package": "RAG_supporters",
    "module_docstring": "EMA (Exponential Moving Average) target encoder wrapper for JASPER training.",
    "classes": {
      "EMAEncoder": {
        "docstring": "Exponential Moving Average wrapper that maintains a slowly-updating target encoder.\n\nThe *target encoder* is a copy of ``base_encoder`` whose parameters are\nupdated via momentum rather than backpropagation.  This prevents\nrepresentation collapse \u2014 the predictor must match a stable, slowly-drifting\ntarget rather than a moving one it can trivially align with.\n\nUsed in JEPA, BYOL, MoCo-style self-supervised architectures.\n\nArchitecture\n------------\n::\n\n    online_encoder  \u2190\u2500\u2500 gradient descent (normal training)\n    target_encoder  \u2190\u2500\u2500 EMA update:  \u03b8_t \u2190 \u03c4\u00b7\u03b8_t + (1-\u03c4)\u00b7\u03b8_online\n\nThe momentum coefficient \u03c4 (tau) is annealed from ``tau_min`` to\n``tau_max`` over ``max_steps`` using a cosine schedule, so the target\nencoder starts updating faster and gradually slows down.\n\nArgs:\n    base_encoder: The encoder module to wrap.  A deep copy is made for the\n        target; the original becomes the ``online_encoder``.\n    tau_min: Starting (lower) momentum value. Typical: 0.996.\n    tau_max: Ending (upper) momentum value. Typical: 0.999.\n    schedule: Annealing schedule for tau.  Currently only ``\"cosine\"``\n        is supported.",
        "methods": {
          "__init__": null,
          "_freeze_target": "Disable gradients on the target encoder (called once at init).",
          "update_target": "Perform one EMA update of the target encoder.\n\nShould be called **once per training step** after the optimizer step.\n\nArgs:\n    step: Current global training step (0-indexed).\n    max_steps: Total number of training steps.",
          "get_tau": "Compute the current momentum coefficient using the configured schedule.\n\nArgs:\n    step: Current global training step (0-indexed).\n    max_steps: Total number of training steps.\n\nReturns:\n    Current tau value in ``[tau_min, tau_max]``.",
          "forward": "Forward pass through the *online* encoder (trainable, with grad).\n\nArgs:\n    x: Input tensor.\n\nReturns:\n    Online encoder output.",
          "encode_target": "Forward pass through the *target* encoder (no gradient).\n\nArgs:\n    x: Input tensor.\n\nReturns:\n    Target encoder output (detached).",
          "state_dict": "Return state dict containing both encoders and tau configuration.",
          "load_state_dict": "Load state dict, restoring tau configuration if present.",
          "get_tau_info": "Return a dict with tau diagnostics for logging.",
          "__repr__": null
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/nn/models/jasper_predictor.py",
    "file": "jasper_predictor.py",
    "module": "jasper_predictor",
    "parent_module": "models",
    "package": "RAG_supporters",
    "module_docstring": "JASPER Predictor model: question + steering \u2192 predicted source embedding.",
    "classes": {
      "JASPERPredictorConfig": {
        "docstring": "Configuration for JASPERPredictor.\n\nArgs:\n    embedding_dim: Dimensionality of input question/steering/output embeddings (D).\n    hidden_dim: Width of hidden layers (H).\n    num_layers: Number of Linear blocks in each encoder and in the predictor head.\n        Must be >= 1.\n    dropout: Dropout probability applied after each hidden activation. 0 disables it.\n    activation: Name of ``torch.nn`` activation class to use (e.g. ``\"GELU\"``).\n    use_layer_norm: Whether to apply LayerNorm after each hidden activation.\n    normalize_output: Whether to L2-normalize the output embedding.",
        "methods": {
          "__post_init__": null,
          "from_dict": "Create config from a plain dictionary (e.g. loaded from YAML)."
        }
      },
      "JASPERPredictor": {
        "docstring": "JASPER predictor: ``question_emb + steering_emb \u2192 predicted_source_emb``.\n\nArchitecture\n------------\n::\n\n    question_emb  [B, D] \u2500\u2500\u25ba question_encoder  [B, H]  \u2500\u2500\u2510\n                                                           \u251c\u2500 concat [B, 2H] \u2500\u2500\u25ba predictor_head \u2500\u2500\u25ba [B, D]\n    steering_emb  [B, D] \u2500\u2500\u25ba steering_encoder  [B, H]  \u2500\u2500\u2518\n\nEach sub-network is a configurable MLP (see :class:`JASPERPredictorConfig`).\n\nArgs:\n    config: Model configuration.  Can also be passed as a plain ``dict``\n        which will be coerced via :meth:`JASPERPredictorConfig.from_dict`.",
        "methods": {
          "__init__": null,
          "_init_weights": "Xavier-uniform init for Linear layers, zeros for biases.",
          "forward": "Predict source embedding from question and steering.\n\nArgs:\n    question_emb: Float tensor of shape ``[B, D]``.\n    steering_emb: Float tensor of shape ``[B, D]``.\n\nReturns:\n    Predicted source embedding of shape ``[B, D]``.",
          "get_latent_representations": "Return cached intermediate activations from the last forward pass.\n\nKeys: ``\"question_latent\"``, ``\"steering_latent\"``.\n\nReturns:\n    Dict of detached CPU tensors (empty if no forward pass has run).",
          "embedding_dim": "Dimensionality of input and output embeddings.",
          "hidden_dim": "Width of hidden layers.",
          "get_model_summary": "Return a human-readable parameter summary.",
          "__repr__": null
        }
      }
    },
    "functions": {
      "_make_mlp": "Build a configurable MLP block.\n\nFor ``num_layers == 1`` the result is a single ``Linear(in_dim, out_dim)``\nwith no activation/dropout.  For ``num_layers > 1`` the intermediate layers\nmap ``in_dim \u2192 hidden_dim \u2192 \u2026 \u2192 hidden_dim`` each followed by activation,\noptional LayerNorm, and optional Dropout; the final layer maps\n``hidden_dim \u2192 out_dim`` with no activation."
    }
  },
  {
    "path": "RAG_supporters/nn/models/model_builder.py",
    "file": "model_builder.py",
    "module": "model_builder",
    "parent_module": "models",
    "package": "RAG_supporters",
    "module_docstring": "Neural network model builder utilities.",
    "classes": {
      "ConfigurableModel": {
        "docstring": "A PyTorch model that builds itself from a YAML configuration file.\n\nAttributes:\n    input_features: Number of input features\n    output_features: Number of output features\n    config_path: Path to the configuration file\n    device: Device to run the model on",
        "methods": {
          "__init__": "Initialize the configurable model.\n\nArgs:\n    config_path: Path to YAML configuration file\n    device: Device to run model on (default: auto-detect)\n    warmup_validate: Whether to validate model with dummy input",
          "forward": "Forward pass through the model.",
          "_load_config": "Load and validate configuration from YAML file.\n\nReturns:\n    Configuration dictionary\n\nRaises:\n    yaml.YAMLError: If YAML parsing fails\n    KeyError: If required configuration keys are missing",
          "_build_model": "Build the neural network model from configuration.\n\nReturns:\n    Sequential model built from configuration",
          "_build_sequential_layer": "Build a Sequential layer containing multiple sub-layers.",
          "_build_single_layer": "Build a single layer from configuration.",
          "_validate_model": "Validate the model with a dummy forward pass.\n\nRaises:\n    RuntimeError: If model validation fails",
          "get_model_summary": "Get a summary of the model architecture.\n\nReturns:\n    String representation of model summary",
          "save_config": "Save current configuration to file.",
          "__repr__": "Return string representation of the model."
        }
      }
    },
    "functions": {
      "_cast_name_to_class": "Convert string name to PyTorch layer class.\n\nArgs:\n    name: String name of the layer class\n\nReturns:\n    PyTorch layer class\n\nRaises:\n    ValueError: If layer name is not found"
    }
  },
  {
    "path": "RAG_supporters/nn/models/subspace_router.py",
    "file": "subspace_router.py",
    "module": "subspace_router",
    "parent_module": "models",
    "package": "RAG_supporters",
    "module_docstring": "SubspaceRouter: routes question+steering embeddings to concept subspaces.",
    "classes": {
      "SubspaceRouterConfig": {
        "docstring": "Configuration for :class:`SubspaceRouter`.\n\nArgs:\n    embedding_dim: Dimensionality of question/steering input embeddings (D).\n    hidden_dim: Width of hidden layers in the routing MLP (H).\n        Can be smaller than the main predictor hidden_dim.\n    num_subspaces: Number of concept subspaces to route to (K). Must be >= 2.\n    num_layers: Number of Linear blocks in the routing MLP. Must be >= 1.\n    dropout: Dropout probability. 0 disables it.\n    activation: Name of ``torch.nn`` activation class (e.g. ``\"GELU\"``).\n    use_layer_norm: Whether to apply LayerNorm after hidden activations.\n    temperature: Temperature for Gumbel-Softmax (training) and Softmax (inference).\n        Lower values produce sharper distributions. Must be > 0.\n    gumbel_hard: If ``True``, use straight-through hard Gumbel-Softmax\n        (one-hot during forward, but gradients flow through soft version).\n        Default ``False`` \u2014 soft Gumbel weights.\n    normalize_input: If ``True``, L2-normalise the concatenated\n        ``[question_emb; steering_emb]`` vector before feeding the MLP.",
        "methods": {
          "__post_init__": null,
          "from_dict": "Create config from a plain dictionary (e.g. loaded from YAML)."
        }
      },
      "SubspaceRouter": {
        "docstring": "Differentiable subspace router: routes (question, steering) to K concept subspaces.\n\nDuring training, Gumbel-Softmax produces stochastic-but-differentiable\nrouting weights.  During inference, plain Softmax gives deterministic\nsoft assignments (or hard argmax when ``config.gumbel_hard=True``).\n\nArgs:\n    config: Router configuration.  Can be passed as a plain ``dict``\n        which will be coerced via :meth:`SubspaceRouterConfig.from_dict`.\n\nExample::\n\n    router = SubspaceRouter(SubspaceRouterConfig(embedding_dim=768, num_subspaces=8))\n    weights, logits = router(question_emb, steering_emb)   # [B,K], [B,K]",
        "methods": {
          "__init__": null,
          "_init_weights": "Xavier-uniform init for Linear layers, zeros for biases.",
          "forward": "Compute routing weights and logits from question and steering embeddings.\n\nArgs:\n    question_emb: Float tensor ``[B, D]``.\n    steering_emb: Float tensor ``[B, D]``.\n    training: If ``True``, apply Gumbel-Softmax noise.\n        If ``False``, use deterministic Softmax.\n        Defaults to ``True``; callers can pass ``self.training`` explicitly.\n\nReturns:\n    Tuple of:\n        - ``routing_weights`` ``[B, K]``: soft routing probabilities summing to 1.\n        - ``concept_logits`` ``[B, K]``: raw unnormalised logits before (Gumbel-)Softmax.",
          "get_primary_subspace": "Return the dominant subspace index and confidence for each sample.\n\nArgs:\n    question_emb: ``[B, D]``\n    steering_emb: ``[B, D]``\n\nReturns:\n    Tuple of:\n        - ``cluster_ids`` ``[B]`` (long): argmax of routing weights.\n        - ``confidences`` ``[B]`` (float): corresponding max weight value.",
          "explain": "Return a human-readable routing explanation for a batch.\n\nArgs:\n    question_emb: ``[B, D]`` or ``[D]`` (single sample).\n    steering_emb: ``[B, D]`` or ``[D]``.\n    cluster_names: List of K human-readable subspace names.\n\nReturns:\n    Dict with keys:\n        - ``\"routing_weights\"``: list of K floats (mean over batch).\n        - ``\"primary_subspace\"``: name of the dominant subspace.\n        - ``\"primary_confidence\"``: float confidence of the dominant subspace.\n        - ``\"entropy\"``: routing entropy in nats (mean over batch).\n        - ``\"cluster_names\"``: the provided cluster names list.",
          "num_subspaces": "Number of concept subspaces K.",
          "embedding_dim": "Input embedding dimension D.",
          "get_model_summary": "Return a human-readable parameter summary.",
          "__repr__": null
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/nn/training/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "training",
    "package": "RAG_supporters",
    "module_docstring": "JASPER training utilities.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/nn/training/jasper_trainer.py",
    "file": "jasper_trainer.py",
    "module": "jasper_trainer",
    "parent_module": "training",
    "package": "RAG_supporters",
    "module_docstring": "JASPER training orchestrator with EMA updates, curriculum, and checkpointing.",
    "classes": {
      "JASPERTrainerConfig": {
        "docstring": "Configuration for :class:`JASPERTrainer`.\n\nArgs:\n    max_grad_norm: Maximum gradient norm for clipping.  Set to 0 to disable.\n    log_every_n_steps: Log metrics every N optimiser steps.\n    checkpoint_dir: Directory to save checkpoints.  Created if it does not exist.\n    save_every_n_epochs: Save a checkpoint every N epochs.  0 disables periodic saves.\n    keep_last_n_checkpoints: How many epoch checkpoints to keep (oldest deleted).\n        0 means keep all.\n    device: Torch device string or ``None`` to auto-detect.\n    mixed_precision: Whether to use ``torch.cuda.amp`` mixed precision training.",
        "methods": {
          "from_dict": null
        }
      },
      "JASPERTrainer": {
        "docstring": "Training orchestrator for the JASPER predictor model.\n\nResponsibilities\n----------------\n- EMA target encoder updates (once per optimiser step)\n- Multi-objective loss computation\n- Curriculum learning via :func:`set_epoch` on the dataset\n- Mixed-precision training (optional)\n- Gradient clipping\n- Checkpoint save / load (model + EMA + optimiser + scheduler + epoch)\n- Validation loop (no EMA updates, no gradient accumulation)\n- Structured logging of all loss components\n\nArgs:\n    config: Trainer configuration.\n    model: The :class:`~RAG_supporters.nn.models.jasper_predictor.JASPERPredictor`\n        (or any ``nn.Module`` with ``forward(question_emb, steering_emb)``).\n    ema_encoder: :class:`~RAG_supporters.nn.models.ema_encoder.EMAEncoder`\n        wrapping a source-side encoder.\n    loss_fn: :class:`~RAG_supporters.nn.losses.jasper_losses.JASPERMultiObjectiveLoss`.\n    optimizer: PyTorch optimiser bound to ``model.parameters()``.\n    train_loader: Training :class:`~torch.utils.data.DataLoader`.\n    val_loader: Validation :class:`~torch.utils.data.DataLoader`.\n    scheduler: Optional LR scheduler.  ``step()`` is called once per epoch.\n    monitor: Optional :class:`~RAG_supporters.nn.training.monitoring.TrainingMonitor`.",
        "methods": {
          "__init__": null,
          "fit": "Train for ``num_epochs`` epochs and return training history.\n\nArgs:\n    num_epochs: Number of epochs to train.\n\nReturns:\n    List of per-epoch metric dicts (train + val metrics).",
          "train_epoch": "Run one training epoch.\n\nArgs:\n    epoch: Current epoch index (0-based).\n\nReturns:\n    Dict of averaged loss metrics over the epoch.",
          "validate": "Run one validation epoch (no EMA updates, no grad).\n\nReturns:\n    Dict of averaged loss metrics over the validation set.",
          "save_checkpoint": "Save full training state to a checkpoint file.\n\nSaved state includes: model weights, EMA encoder state (both online and\ntarget + tau config), optimiser state, scheduler state, global step,\nepoch, and optional metrics.\n\nArgs:\n    path: File path for the checkpoint (``.pt`` / ``.pth``).\n    epoch: Current epoch index.\n    metrics: Optional metrics dict to store alongside the checkpoint.",
          "load_checkpoint": "Load training state from a checkpoint file.\n\nArgs:\n    path: Path to a checkpoint saved by :meth:`save_checkpoint`.\n\nReturns:\n    Tuple of ``(epoch, metrics)`` restored from the checkpoint.",
          "_train_step": "Run a single forward + backward step and return loss metrics.",
          "_eval_step": "Run a single validation step and return loss metrics.",
          "_to_device": null,
          "_get_centroid_embs": "Return cached centroid embeddings (or fall back to source mean).",
          "_load_centroids": "Try to extract centroid embeddings from the training dataset.",
          "_rotate_checkpoints": "Delete oldest checkpoint files if over the keep limit.",
          "_log_epoch": null,
          "global_step": "Total number of optimiser steps taken so far.",
          "best_val_loss": "Best validation loss seen during training."
        }
      }
    },
    "functions": {
      "_set_epoch": null
    }
  },
  {
    "path": "RAG_supporters/nn/training/monitoring.py",
    "file": "monitoring.py",
    "module": "monitoring",
    "parent_module": "training",
    "package": "RAG_supporters",
    "module_docstring": "Training monitor: metric collection, plotting, CSV export, and optional W&B logging.",
    "classes": {
      "TrainingMonitor": {
        "docstring": "Collect, visualise, and optionally sync training metrics.\n\nFeatures\n--------\n- Per-epoch metric logging (internal history)\n- Loss curve plots (all loss components in separate sub-plots)\n- Steering variant distribution plot over epochs\n- Full history export to CSV and JSON\n- Optional Weights & Biases (W&B) integration \u2014 gracefully disabled when\n  ``wandb`` is not installed or ``use_wandb=False``\n\nArgs:\n    output_dir: Directory where plots and CSV are saved.\n    use_wandb: Whether to log to Weights & Biases.\n    wandb_project: W&B project name.\n    wandb_name: W&B run name (auto-generated if ``None``).\n    wandb_config: Config dict to log to W&B.",
        "methods": {
          "__init__": null,
          "log_metrics": "Record metrics for one epoch.\n\nAlso syncs to W&B if enabled.\n\nArgs:\n    epoch: Epoch index (used as W&B step).\n    metrics_dict: Dict of metric name \u2192 scalar value.",
          "plot_losses": "Plot train/val loss curves for all tracked components.\n\nArgs:\n    save_path: File path for the saved figure.  Defaults to\n        ``<output_dir>/loss_curves.png``.\n\nReturns:\n    Absolute path to the saved figure, or ``None`` if matplotlib\n    is unavailable or there is no data.",
          "plot_steering_distribution": "Plot the steering variant distribution over epochs.\n\nExpects metric keys like ``train/steering_variant_<N>_frac`` to be\nlogged.  If no such keys are present the plot is skipped.\n\nArgs:\n    save_path: File path for the saved figure.\n\nReturns:\n    Absolute path to the saved figure, or ``None``.",
          "export_history": "Export the full metric history to CSV (and JSON sidecar).\n\nArgs:\n    save_path: CSV file path.  Defaults to\n        ``<output_dir>/training_history.csv``.\n\nReturns:\n    Path to the saved CSV file.",
          "get_summary_table": "Return training history as a pandas DataFrame.\n\nReturns:\n    ``pandas.DataFrame`` if pandas is installed, otherwise a plain\n    list of dicts.",
          "finish": "Finalise the monitoring session.\n\nCloses the W&B run if one is active.  Should be called at the end of\ntraining.",
          "num_epochs_logged": "Number of epochs recorded so far.",
          "__repr__": null
        }
      }
    },
    "functions": {
      "_json_default": "JSON serialiser for numpy/torch scalars."
    }
  },
  {
    "path": "RAG_supporters/prompts_templates/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "prompts_templates",
    "package": "RAG_supporters",
    "module_docstring": "Prompt templates package for RAG and agent operations.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/prompts_templates/domain_extraction.py",
    "file": "domain_extraction.py",
    "module": "domain_extraction",
    "parent_module": "prompts_templates",
    "package": "RAG_supporters",
    "module_docstring": "Prompt templates for domain extraction and analysis tasks. Used for training Knowledge Subspace Classifiers.",
    "classes": {},
    "functions": {
      "QUESTION_TOPIC_RELEVANCE_PROB_PROMPT": "Generate prompt for assessing topic relevance probabilities.\n\nParameters\n----------\ninclude_reason : bool, optional\n    If True, includes 'reason' field in each topic assessment. Default is False.\n\nReturns\n-------\nstr\n    The formatted prompt template string"
    }
  },
  {
    "path": "RAG_supporters/prompts_templates/rag_generators.py",
    "file": "rag_generators.py",
    "module": "rag_generators",
    "parent_module": "prompts_templates",
    "package": "RAG_supporters",
    "module_docstring": "RAG generation prompt templates.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/prompts_templates/rag_verifiers.py",
    "file": "rag_verifiers.py",
    "module": "rag_verifiers",
    "parent_module": "prompts_templates",
    "package": "RAG_supporters",
    "module_docstring": "RAG verification prompt templates.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/prompts_templates/text_augmentation.py",
    "file": "text_augmentation.py",
    "module": "text_augmentation",
    "parent_module": "prompts_templates",
    "package": "RAG_supporters",
    "module_docstring": "Prompts for text augmentation and rephrasing tasks.\n\nThese prompts are used by the TextAugmentationAgent to generate alternative\nversions of questions and sources while preserving their original meaning.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/pytorch_datasets/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "pytorch_datasets",
    "package": "RAG_supporters",
    "module_docstring": "PyTorch Dataset implementations for training.\n\nThis module provides PyTorch Dataset classes for different training scenarios:\n- JASPER steering dataset with curriculum learning\n- General RAG datasets\n- Cluster-labeled datasets\n- DataLoader factories with validation\n\nKey Features:\n- Pre-loaded embeddings (zero I/O during training)\n- Curriculum learning support\n- Hard negative sampling\n- Distributed training support\n- Device placement and context managers\n- Batch validation utilities\n\nExamples\n--------\n>>> from RAG_supporters.pytorch_datasets import JASPERSteeringDataset, create_loader\n>>>\n>>> # Create dataset\n>>> dataset = JASPERSteeringDataset(\n...     dataset_dir=\"output/dataset\",\n...     split=\"train\",\n...     epoch=0\n... )\n>>>\n>>> # Create DataLoader with curriculum learning\n>>> loader = create_loader(\n...     dataset_dir=\"output/dataset\",\n...     split=\"train\",\n...     batch_size=32,\n...     num_workers=4\n... )\n>>>\n>>> # Training loop\n>>> for epoch in range(100):\n...     loader.dataset_obj.set_epoch(epoch)\n...     for batch in loader:\n...         # Train model\n...         pass",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/pytorch_datasets/cluster_labeled_dataset.py",
    "file": "cluster_labeled_dataset.py",
    "module": "cluster_labeled_dataset",
    "parent_module": "pytorch_datasets",
    "package": "RAG_supporters",
    "module_docstring": "PyTorch Dataset for cluster-labeled domain assessment data.",
    "classes": {
      "ClusterLabeledDataset": {
        "docstring": "PyTorch Dataset for domain assessment with 3-type labels.\n\nReturns triplets: (base_embedding, steering_embedding, label)\n\nLabel types:\n- source_label: Label for base embedding (source/question)\n- steering_label: Label for steering embedding\n- combined_label: Weighted average for augmentation masking\n\nStorage: SQLite (metadata) + numpy memmap (embeddings).",
        "methods": {
          "__init__": "Initialize cluster-labeled dataset.\n\nArgs:\n    dataset_dir: Directory containing dataset.db and embedding files\n    label_type: Which label to return ('source', 'steering', 'combined')\n    return_metadata: Whether to return metadata dict\n    mmap_mode: Mode for numpy memmap ('r', 'r+', 'w+', 'c')\n    cache_size: Maximum number of samples to keep in LRU cache",
          "__len__": "Return dataset size.",
          "__getitem__": "Get sample at index.\n\nArgs:\n    idx: Sample index\n\nReturns:\n    If return_metadata=False:\n        (base_embedding, steering_embedding, label)\n    If return_metadata=True:\n        (base_embedding, steering_embedding, label, metadata)",
          "get_sample_by_id": "Get sample by database ID.\n\nArgs:\n    sample_id: Sample ID from database\n\nReturns:\n    Sample dictionary or None",
          "update_labels": "Update labels for a sample (for manual correction).\n\nArgs:\n    sample_id: Sample ID from database\n    source_label: New source label\n    steering_label: New steering label\n    combined_label: New combined label",
          "get_cache_stats": "Get cache performance statistics.\n\nReturns:\n    Dictionary with cache hits, misses, and hit rate",
          "_load_embeddings": "Load embeddings as memory-mapped array.\n\nArgs:\n    embedding_type: 'base' or 'steering'\n\nReturns:\n    Memory-mapped numpy array\n\nRaises:\n    ValueError: If embedding dimension doesn't match expected dimension\n    FileNotFoundError: If embedding file doesn't exist",
          "close": "Close storage connections and cleanup resources.",
          "__enter__": "Enter context manager.",
          "__exit__": "Exit context manager and close storage.",
          "__del__": "Destructor - safety fallback for cleanup.",
          "create_from_csvs": "Build and load dataset from CSV files.\n\nArgs:\n    csv_paths: Path(s) to CSV files\n    clustering_json_path: Path to clustering JSON\n    output_dir: Output directory\n    embedding_model: Embedding model\n    **builder_kwargs: Additional arguments for DomainAssessmentDatasetBuilder\n\nReturns:\n    Loaded ClusterLabeledDataset",
          "create_subset": "Create a subset of this dataset using the provided indices.\n\nThis is useful for creating train/val splits while maintaining access\nto the full dataset's methods and properties.\n\nArgs:\n    indices: List of indices to include in the subset\n\nReturns:\n    torch.utils.data.Subset wrapping this dataset with the specified indices\n\nExamples:\n    >>> dataset = ClusterLabeledDataset('path/to/dataset')\n    >>> from RAG_supporters.dataset import DatasetSplitter\n    >>> splitter = DatasetSplitter(random_state=42)\n    >>> train_idx, val_idx = splitter.split(len(dataset), val_ratio=0.2)\n    >>> train_dataset = dataset.create_subset(train_idx)\n    >>> val_dataset = dataset.create_subset(val_idx)",
          "split_dataset": "Split this dataset into training and validation subsets.\n\nArgs:\n    val_ratio: Ratio of validation samples (between 0 and 1). Default is 0.2.\n    random_state: Random seed for reproducibility\n    shuffle: Whether to shuffle indices before splitting. Default is True.\n    save_path: If provided, save split configuration to this path\n\nReturns:\n    Tuple of (train_subset, val_subset)\n\nExamples:\n    >>> dataset = ClusterLabeledDataset('path/to/dataset')\n    >>> train_dataset, val_dataset = dataset.split_dataset(\n    ...     val_ratio=0.2,\n    ...     random_state=42,\n    ...     save_path='split_config.json'\n    ... )\n    >>> print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")",
          "load_split": "Load a dataset and apply a saved split configuration.\n\nArgs:\n    dataset_dir: Directory containing the dataset\n    split_path: Path to the saved split configuration\n    label_type: Which label to return ('source', 'steering', 'combined')\n    **dataset_kwargs: Additional arguments for ClusterLabeledDataset\n\nReturns:\n    Tuple of (train_subset, val_subset)\n\nExamples:\n    >>> train_dataset, val_dataset = ClusterLabeledDataset.load_split(\n    ...     dataset_dir='path/to/dataset',\n    ...     split_path='split_config.json'\n    ... )"
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/pytorch_datasets/jasper_steering_dataset.py",
    "file": "jasper_steering_dataset.py",
    "module": "jasper_steering_dataset",
    "parent_module": "pytorch_datasets",
    "package": "RAG_supporters",
    "module_docstring": "PyTorch Dataset for JASPER (Joint Architecture for Subspace Prediction with Explainable Routing).\n\nThis module provides the JASPERSteeringDataset class that serves pre-computed embedding\ntriplets (question, steering, target_source) with hard negatives and subspace labels.\nAll embeddings are preloaded for zero I/O during training.\n\nStorage Formats:\n- PT (PyTorch): Default format, fast loading\n- HDF5: Compressed storage, lazy loading support\n- Memory-mapped: For large datasets (>10GB), loads on-demand from disk",
    "classes": {
      "JASPERSteeringDataset": {
        "docstring": "PyTorch Dataset for JASPER steering with pre-computed embeddings.\n\nServes (question, steering, target_source) triplets with hard negatives.\nAll data is preloaded from disk during initialization for zero I/O in __getitem__.\n\nParameters\n----------\ndataset_dir : str or Path\n    Directory containing all dataset files (config.json, *.pt tensors, etc.)\nsplit : Literal[\"train\", \"val\", \"test\"]\n    Which split to use\nepoch : int, optional\n    Initial epoch number for curriculum learning, by default 0\ndevice : torch.device, optional\n    Device to transfer tensors to during initialization (default: CPU)\nstorage_format : Literal[\"auto\", \"pt\", \"hdf5\"], optional\n    Storage format to use. \"auto\" detects available format (default: \"auto\")\nuse_mmap : bool or None, optional\n    Enable memory-mapped loading for large datasets. If None, auto-enables for >10GB datasets\n\nAttributes\n----------\nconfig : Dict[str, Any]\n    Dataset configuration loaded from config.json\nembedding_dim : int\n    Dimensionality of all embeddings\nn_neg : int\n    Number of hard negatives per sample\ndevice : torch.device\n    Device where tensors are stored\nstorage_format : str\n    Actual storage format being used (\"pt\" or \"hdf5\")\nuse_mmap : bool\n    Whether memory-mapped loading is enabled",
        "methods": {
          "__init__": "Initialize dataset and preload all tensors.",
          "_detect_storage_format": "Detect available storage format.\n\nParameters\n----------\nrequested_format : str\n    Requested format (\"auto\", \"pt\", or \"hdf5\")\n\nReturns\n-------\nstr\n    Actual format to use (\"pt\" or \"hdf5\")\n\nRaises\n------\nValueError\n    If requested format is not available",
          "_should_use_mmap": "Determine if memory-mapping should be enabled.\n\nParameters\n----------\nuse_mmap : bool or None\n    Explicit setting, or None for auto-detection\n\nReturns\n-------\nbool\n    Whether to use memory-mapping",
          "_load_embeddings": "Load all embedding tensors.",
          "_load_embeddings_pt": "Load embeddings from PT format.",
          "_load_embeddings_hdf5": "Load embeddings from HDF5 format.",
          "_load_mmap_tensor": "Load tensor using memory-mapping.\n\nParameters\n----------\nfilename : str\n    Name of PT file to load\n\nReturns\n-------\ntorch.Tensor\n    Memory-mapped tensor (backed by disk storage)\n\nNotes\n-----\nMemory-mapped tensors load pages on-demand, reducing RAM usage for large datasets.",
          "_load_tensor": "Load tensor based on storage format.\n\nParameters\n----------\nfilename : str\n    Name of tensor file (without format extension for HDF5)\nexpected_shape : tuple, optional\n    Expected shape for validation\n\nReturns\n-------\ntorch.Tensor\n    Loaded tensor",
          "_load_pair_data": "Load pair-level data.",
          "_load_negatives": "Load hard negative data.",
          "_load_steering": "Load steering tensors.",
          "_validate_referential_integrity": "Validate that all indices reference valid data.",
          "_move_to_device": "Move all tensors to target device.",
          "_compute_memory_usage": "Compute total memory usage in MB.",
          "_compute_steering_probs": "Compute steering variant probabilities based on curriculum.\n\nParameters\n----------\nepoch : int\n    Current training epoch\n\nReturns\n-------\nDict[str, float]\n    Probability for each steering variant (zero, centroid, keyword, residual)",
          "__len__": "Return number of samples in current split.",
          "__getitem__": "Get a single sample.\n\nParameters\n----------\nidx : int\n    Index within current split\n\nReturns\n-------\nDict[str, torch.Tensor]\n    Dictionary containing:\n    - question_emb: [D] tensor\n    - target_source_emb: [D] tensor\n    - steering: [D] tensor (selected variant or zeros)\n    - negative_embs: [N_neg, D] tensor\n    - cluster_id: scalar tensor\n    - relevance: scalar tensor\n    - centroid_distance: scalar tensor\n    - steering_variant: scalar tensor (0=zero, 1=centroid, 2=keyword, 3=residual)\n    - negative_tiers: [N_neg] tensor\n\nRaises\n------\nIndexError\n    If idx is out of bounds",
          "set_epoch": "Update epoch for curriculum learning and reseed RNG.\n\nParameters\n----------\nepoch : int\n    New epoch number",
          "reload_negatives": "Hot-reload hard negatives from disk.\n\nUseful for periodic negative refreshing during training without restarting.",
          "force_steering": "Force a specific steering variant (disables stochastic selection).\n\nParameters\n----------\nvariant : Optional[Literal[\"zero\", \"centroid\", \"keyword\", \"residual\"]]\n    Steering variant to force, or None to restore stochastic selection",
          "close": "Release resources and log statistics.",
          "__enter__": "Enter context manager.",
          "__exit__": "Exit context manager and release resources.",
          "__del__": "Destructor - safety fallback for cleanup.",
          "create_combined_splits": "Load all splits (train/val/test) from one directory.\n\nParameters\n----------\ndataset_dir : str or Path\n    Directory containing dataset files\nepoch : int, optional\n    Initial epoch for curriculum learning (default: 0)\ndevice : torch.device, optional\n    Device to load tensors to (default: CPU)\nstorage_format : Literal[\"auto\", \"pt\", \"hdf5\"], optional\n    Storage format to use (default: \"auto\")\nuse_mmap : bool or None, optional\n    Enable memory-mapped loading (default: None for auto-detect)\n\nReturns\n-------\nDict[str, JASPERSteeringDataset]\n    Dictionary with keys \"train\", \"val\", \"test\"\n\nExamples\n--------\n>>> splits = JASPERSteeringDataset.create_combined_splits(\n...     \"output/jasper_dataset\",\n...     device=torch.device(\"cuda\"),\n...     epoch=0\n... )\n>>> train_dataset = splits[\"train\"]\n>>> val_dataset = splits[\"val\"]",
          "convert_pt_to_hdf5": "Convert PT format dataset to HDF5 format.\n\nParameters\n----------\ndataset_dir : str or Path\n    Directory containing PT format dataset files\ncompression : str, optional\n    HDF5 compression algorithm (default: \"gzip\")\n\nRaises\n------\nValueError\n    If h5py is not installed or PT files don't exist\nImportError\n    If h5py is not available\n\nExamples\n--------\n>>> JASPERSteeringDataset.convert_pt_to_hdf5(\"output/jasper_dataset\")"
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/pytorch_datasets/loader.py",
    "file": "loader.py",
    "module": "loader",
    "parent_module": "pytorch_datasets",
    "package": "RAG_supporters",
    "module_docstring": "DataLoader factory and utilities for JASPER Steering Dataset.\n\nProvides functions to create DataLoaders with proper configuration for\ndistributed training, validation, and testing.",
    "classes": {},
    "functions": {
      "create_loader": "Create a DataLoader for JASPER Steering Dataset.\n\nParameters\n----------\ndataset_dir : str or Path\n    Directory containing dataset files\nsplit : Literal[\"train\", \"val\", \"test\"]\n    Which split to load\nbatch_size : int\n    Batch size for the DataLoader\nnum_workers : int, optional\n    Number of worker processes for data loading, by default 0\ndistributed : bool, optional\n    Whether to use DistributedSampler for multi-GPU training, by default False\nepoch : int, optional\n    Initial epoch for curriculum learning, by default 0\ndrop_last : Optional[bool], optional\n    Whether to drop last incomplete batch.\n    If None, defaults to True for train, False for val/test\npin_memory : bool, optional\n    Whether to pin memory for faster GPU transfer, by default True\n\nReturns\n-------\nDataLoader\n    Configured PyTorch DataLoader\n\nExamples\n--------\n>>> loader = create_loader(\n...     dataset_dir=\"/path/to/dataset\",\n...     split=\"train\",\n...     batch_size=32,\n...     num_workers=4,\n... )\n>>> for batch in loader:\n...     # Train model\n...     pass",
      "set_epoch": "Set epoch for both dataset and sampler (for curriculum and distributed training).\n\nParameters\n----------\nloader : DataLoader\n    DataLoader created by create_loader\nepoch : int\n    Epoch number to set\n\nExamples\n--------\n>>> loader = create_loader(...)\n>>> for epoch in range(100):\n...     set_epoch(loader, epoch)\n...     for batch in loader:\n...         # Train\n...         pass",
      "validate_first_batch": "Validate that the first batch has correct shapes and no NaN/Inf values.\n\nParameters\n----------\nloader : DataLoader\n    DataLoader to validate\n\nReturns\n-------\nbool\n    True if validation passes\n\nRaises\n------\nAssertionError\n    If any validation check fails\n\nExamples\n--------\n>>> loader = create_loader(...)\n>>> validate_first_batch(loader)\nTrue"
    }
  },
  {
    "path": "RAG_supporters/pytorch_datasets/rag_dataset.py",
    "file": "rag_dataset.py",
    "module": "rag_dataset",
    "parent_module": "pytorch_datasets",
    "package": "RAG_supporters",
    "module_docstring": "RAG dataset utilities for managing and evaluating RAG systems.",
    "classes": {
      "SamplePairingType": {
        "docstring": "Enum representing different types of sample pairings for RAG datasets.",
        "methods": {}
      },
      "SampleTripletRAGChroma": {
        "docstring": "A dataclass representing a triplet of question and two answers for RAG evaluation.\n\nAttributes\n----------\nquestion_id : str\n    Unique identifier for the question in ChromaDB\nsource_id_1 : str\n    Unique identifier for the first answer in ChromaDB\nsource_id_2 : str\n    Unique identifier for the second answer in ChromaDB\nlabel : int, optional\n    Indicates which answer is better:\n    -1 means not labeled, 0 means both are irrelevant, 1 means answer_1 is better, 2 means answer_2 is better",
        "methods": {}
      },
      "BaseRAGDatasetGenerator": {
        "docstring": "Abstract base class for generating RAG dataset samples.\n\nThis class provides the framework for creating different types of\nquestion-answer triplet samples for evaluating RAG systems.\n\nAttributes\n----------\n_question_db : Chroma\n    ChromaDB collection containing questions\n_text_corpus_db : Chroma\n    ChromaDB collection containing text corpus/passages\n_dataset_dir : str\n    Directory path for storing dataset files\n_embed_function : callable, optional\n    Function used for embedding text\n_dataset_metadata : dict, optional\n    Dictionary containing dataset metadata (source, embedding method, etc.)",
        "methods": {
          "_init_dataset_metadata": "Initialize dataset metadata with information about source and embedding method.\n\nParameters\n----------\ndataset_names : List[str]\n    List of dataset names\ndataset_sources : List[str]\n    List of dataset sources (e.g., HuggingFace repo names)\nembed_function : callable\n    The embedding function used\n**kwargs : dict\n    Additional parameters for metadata",
          "load_dataset": "Load the RAG dataset from the specified source.\n\nThis method should initialize the ChromaDB collections with\nthe questions and text corpus data.\n\nReturns\n-------\nNone",
          "validate_dataset": "Validate the loaded dataset for correctness and completeness.\n\nThis method should check that the dataset meets the required format\nand contains all necessary information.\n\nReturns\n-------\nbool\n    True if dataset is valid, False otherwise",
          "generate_samples": "Generate dataset samples based on the specified type.\n\nParameters\n----------\nsample_type : str\n    Type of samples to generate. Valid values are:\n    'positive', 'contrastive', 'similar'\n\nReturns\n-------\nList\n    List of generated samples",
          "_generate_positive_triplet_samples": "Generate triplets consisting of a question and two relevant passages.\n\nThese samples are intended for comparing two passages that are both\nassigned to the same question in the database.\n\nParameters\n----------\nquestion_chroma_id : str\n    Unique identifier of the question in ChromaDB\nrelevant_passage_ids : List[str]\n    List of passage IDs that are relevant to the question\n**kwargs : dict\n    Additional arguments for customizing sample generation\n\nReturns\n-------\nList[SampleTripletRAGChroma]\n    List of generated triplet samples",
          "_generate_contrastive_triplet_samples": "Generate triplets with a question, one relevant passage and one irrelevant passage.\n\nThese samples are intended for comparing a relevant passage with a randomly\nchosen passage not assigned to the question.\n\nParameters\n----------\nquestion_db_id : str\n    Unique identifier of the question in ChromaDB\nrelevant_passage_db_ids : List[str]\n    List of passage IDs that are relevant to the question\nnum_negative_samples : int, optional\n    Number of negative samples to generate per question-passage pair.\n    Default is 2.\nkeep_same_negatives : bool, optional\n    If True, reuse the same negative samples for different relevant passages.\n    Default is False.\n**kwargs : dict\n    Additional arguments for customizing sample generation\n\nReturns\n-------\nList[SampleTripletRAGChroma]\n    List of generated triplet samples",
          "_generate_similar_triplet_samples": "Generate triplets with a question, a relevant passage, and a similar but less relevant passage.\n\nThese samples are intended for comparing a relevant passage with another\npassage that is semantically similar to the question but may be less relevant.\n\nParameters\n----------\nquestion_db_id : str\n    Unique identifier of the question in ChromaDB\nrelevant_passage_db_ids : List[str]\n    List of passage IDs that are relevant to the question\n**kwargs : dict\n    Additional arguments for customizing sample generation\n\nReturns\n-------\nList[SampleTripletRAGChroma]\n    List of generated triplet samples",
          "_generate_pair_samples_df": "Generate pair variants (questions, source) in dataframe format.\n\nParameters\n----------\nquestion_db_ids : Optional[List[str]], optional\n    List of question IDs to generate pairs for. If None, all questions are used.\n    Default is None.\ncriterion : str, optional\n    Criterion to use for scoring the pairs. Default is \"relevance\".\n    Possible values: \"relevance\", \"embedding_similarity\", \"all_existing\"",
          "chroma_id_to_embedding": "Retrieve embeddings for the given ChromaDB IDs.\n\nParameters\n----------\nchroma_ids : Union[List[str], str]\n    ChromaDB ID or list of IDs to retrieve embeddings for\nsearch_db : str\n    Database to search in, either 'question' or 'text'\n\nReturns\n-------\nList[List[float]]\n    List of embeddings corresponding to the provided IDs\n\nRaises\n------\nValueError\n    If search_db is not 'question' or 'text'",
          "get_question_db_data": "Retrieve data from the question database.\n\nParameters\n----------\ninclude : Iterable[str], optional\n    Data fields to include in the response.\n    Default is (\"documents\", \"metadatas\", \"embeddings\").\n\nReturns\n-------\nDict\n    Dictionary containing the requested data fields",
          "get_text_corpus_db_data": "Retrieve data from the text corpus database.\n\nParameters\n----------\ninclude : Iterable[str], optional\n    Data fields to include in the response.\n    Default is (\"documents\", \"metadatas\", \"embeddings\").\nReturns\n-------\nDict\n    Dictionary containing the requested data fields",
          "save_dataset_metadata": "Save dataset metadata to a YAML file.\n\nThis method saves information about the dataset including source,\nembedding method, and other relevant metadata for later features\nlike concatenation.\n\nParameters\n----------\nmetadata_file : Optional[str], optional\n    Path where the metadata file will be saved.\n    If None, saves to {dataset_dir}/dataset_info.yaml\n\nReturns\n-------\nNone",
          "load_dataset_metadata": "Load dataset metadata from a YAML file.\n\nParameters\n----------\nmetadata_file : Optional[str], optional\n    Path to the metadata file.\n    If None, loads from {dataset_dir}/dataset_info.yaml\n\nReturns\n-------\nDict[str, Any]\n    Dictionary containing dataset metadata\n\nRaises\n------\nFileNotFoundError\n    If the metadata file does not exist",
          "evaluate_pair_samples": "Evaluate pair samples using LLM-based source evaluation.\n\nThis method uses the SourceEvaluationAgent to evaluate question-source pairs\nand assign comprehensive scores across multiple dimensions.\n\nParameters\n----------\nllm : BaseChatModel\n    Language model to use for evaluation\npairs_df : pd.DataFrame\n    DataFrame containing pair samples. If None, generates new pairs.\nskip_evaluated : bool, optional\n    If True, skip pairs that already have evaluation scores. Default is True.\ninclude_reasoning : bool, optional\n    If True, include reasoning for each score dimension. Default is False.\nsave_path : Optional[str], optional\n    Path to save the evaluated pairs as CSV. Default is None.\nmax_retries : int, optional\n    Maximum retries for LLM evaluation. Default is 3.\nevaluation_prompt : str, optional\n    Prompt template for evaluation. Default is SINGLE_SRC_SCORE_PROMPT.\ncheckpoint_batch_size : Optional[int], optional\n    If provided, saves intermediate results every N processed pairs.\n\nReturns\n-------\npd.DataFrame\n    DataFrame with evaluated pairs including scores across all dimensions\n\nRaises\n------\nValueError\n    If neither pairs_df nor question_db_ids are provided and no questions exist",
          "label_triplet_samples_with_llm": "Validate triplet samples using LLM-based verification.\n\nThis method uses a language model to evaluate and label triplet samples\nby determining which of the two passages better answers the question.\n\nParameters\n----------\nllm : object\n    Language model to use for validation\nsamples : List[SampleTripletRAGChroma]\n    List of triplet samples to validate\nanalysis_prompt : str, optional\n    Prompt template for analysis. Default is SRC_COMPARE_PROMPT_WITH_SCORES.\nskip_labeled : bool, optional\n    If True, skip samples that already have a label. Default is True.\noverwrite_mismatched_labels : bool\n    If True, overwrite labels other than -1 that have mismatch between ground truth and prediction\n\nReturns\n-------\nList[SampleTripletRAGChroma]\n    List of validated triplet samples with labels",
          "_raw_similarity_search": "Perform similarity search by vector and return relevance scores.\n\nThis is a low-level method that directly queries the ChromaDB collection.\n\nParameters\n----------\nsearch_db : Literal[\"question\", \"text\"]\n    Database to search in, either 'question' or 'text'\nembedding_or_text : List[float]\n    Embedding vector to search with\nk : int, optional\n    Number of results to return. Default is 4.\nwhere : Optional[Dict[str, str]], optional\n    Dictionary to filter results by metadata fields.\n    E.g. {\"color\" : \"red\", \"price\": 4.20}\nwhere_document : Optional[Dict[str, str]], optional\n    Dictionary to filter by document content.\n    E.g. {$contains: {\"text\": \"hello\"}}\n**kwargs : Any\n    Additional keyword arguments to pass to Chroma collection query\n\nReturns\n-------\nDict[str, Optional[List[Any]]]\n    Dictionary containing query results from ChromaDB",
          "save_triplets_to_csv": "Save triplet samples to CSV with both IDs and corresponding text content.\n\nParameters\n----------\ntriplets : List[SampleTripletRAGChroma]\n    The list of triplet samples to save\noutput_file : str\n    Path where the CSV file will be saved\ninclude_embeddings : bool, optional\n    Whether to include embeddings in the output. Default is False.\n\nReturns\n-------\nNone"
        }
      }
    },
    "functions": {}
  },
  {
    "path": "RAG_supporters/utils/__init__.py",
    "file": "__init__.py",
    "module": "__init__",
    "parent_module": "utils",
    "package": "RAG_supporters",
    "module_docstring": "Utility functions for RAG supporters.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "RAG_supporters/utils/suggestion_processing.py",
    "file": "suggestion_processing.py",
    "module": "suggestion_processing",
    "parent_module": "utils",
    "package": "RAG_supporters",
    "module_docstring": "Utilities for processing LLM suggestions from CSV files.\n\nThis module provides functions for processing suggestion data returned from LLM\ncalls, typically stored in CSV files as JSON-formatted fields:\n- Filtering suggestions by field values (e.g., confidence scores)\n- Aggregating unique terms from suggestions\n\nThese utilities are independent of embedding operations and can be used for\ngeneral data processing tasks.",
    "classes": {},
    "functions": {
      "filter_by_field_value": "Filter suggestions by a numeric field threshold.\n\nParameters\n----------\nsuggestions : List[Dict[str, Any]]\n    List of suggestions, each containing numeric field to filter by\nmin_value : float\n    Minimum threshold value (0.0 to 1.0 for confidence-like fields)\nfield_name : str\n    Name of the numeric field to filter by (default: 'confidence')\n\nReturns\n-------\nList[Dict[str, Any]]\n    Filtered suggestions that meet the threshold\n\nExamples\n--------\n>>> suggestions = [\n...     {'term': 'machine learning', 'confidence': 0.9},\n...     {'term': 'data science', 'confidence': 0.6},\n...     {'term': 'AI', 'confidence': 0.85}\n... ]\n>>> filtered = filter_by_field_value(suggestions, 0.7)\n>>> len(filtered)\n2\n\n>>> # Filter by custom field\n>>> suggestions = [\n...     {'term': 'keyword1', 'score': 0.8},\n...     {'term': 'keyword2', 'score': 0.5}\n... ]\n>>> filtered = filter_by_field_value(suggestions, 0.6, field_name='score')\n>>> len(filtered)\n1",
      "aggregate_unique_terms": "Aggregate terms into unique keywords.\n\nParameters\n----------\nsuggestions : List[Dict[str, Any]]\n    List of suggestions, each containing term information\nterm_key : str\n    Key name for the term in suggestion dictionaries\nnormalize : bool\n    Whether to normalize terms (lowercase, strip whitespace)\nreturn_counts : bool\n    If True, also return occurrence counts for each unique term\n\nReturns\n-------\nTuple[List[str], Optional[Dict[str, int]]]\n    Tuple of (unique_keywords, counts_dict)\n    - unique_keywords: List of unique keywords (order preserved)\n    - counts_dict: Dict mapping keywords to counts if return_counts=True, else None\n\nExamples\n--------\n>>> suggestions = [\n...     {'term': 'Machine Learning', 'confidence': 0.9},\n...     {'term': 'machine learning', 'confidence': 0.85},\n...     {'term': 'Deep Learning', 'confidence': 0.8},\n...     {'term': 'Machine Learning', 'confidence': 0.75}\n... ]\n>>> keywords, counts = aggregate_unique_terms(suggestions, normalize=True)\n>>> keywords\n['machine learning', 'deep learning']\n>>> counts is None\nTrue\n\n>>> keywords, counts = aggregate_unique_terms(suggestions, normalize=True, return_counts=True)\n>>> keywords\n['machine learning', 'deep learning']\n>>> counts\n{'machine learning': 3, 'deep learning': 1}"
    }
  },
  {
    "path": "RAG_supporters/utils/text_splitters.py",
    "file": "text_splitters.py",
    "module": "text_splitters",
    "parent_module": "utils",
    "package": "RAG_supporters",
    "module_docstring": "Text splitting utilities for RAG dataset processing.",
    "classes": {},
    "functions": {
      "base_text_splitter": "Split text into chunks of max_words words.",
      "split_into_sentences": "Split text into sentences using basic heuristics.\n\nParameters\n----------\ntext : str\n    Text to split into sentences.\n\nReturns\n-------\nList[str]\n    List of sentences extracted from the text."
    }
  },
  {
    "path": "RAG_supporters/utils/text_utils.py",
    "file": "text_utils.py",
    "module": "text_utils",
    "parent_module": "utils",
    "package": "RAG_supporters",
    "module_docstring": "Text processing utilities.",
    "classes": {},
    "functions": {
      "is_empty_text": "Check if the text is empty or only whitespace.",
      "normalize_string": "Normalize a string by converting to lowercase and removing multiple spaces.\n\nParameters\n----------\ntext : str\n    String to normalize\n\nReturns\n-------\nstr\n    Normalized string\n\nExamples\n--------\n>>> normalize_string(\"  Machine Learning  \")\n'machine learning'\n>>> normalize_string(\"DATA   SCIENCE\")\n'data science'",
      "parse_json_or_literal": "Robustly parse JSON or Python literal format strings.\n\nAttempts JSON parsing first (fast and standard), then falls back to\nsafe Python literal evaluation if needed.\n\nParameters\n----------\ndata_str : str\n    String containing data in JSON or Python literal format\nexpected_type : Optional[type]\n    Expected type for validation (e.g., list, dict)\ndebug : bool, default=False\n    Enable debug logging for parsing details\n\nReturns\n-------\nUnion[List, Dict, Any]\n    Parsed data structure\n\nRaises\n------\nValueError\n    If parsing fails or type validation fails"
    }
  },
  {
    "path": "tests/conftest.py",
    "file": "conftest.py",
    "module": "conftest",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Pytest configuration for RAG_supporters tests.",
    "classes": {},
    "functions": {}
  },
  {
    "path": "tests/test_build_steering.py",
    "file": "test_build_steering.py",
    "module": "test_build_steering",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for Steering Signal Builder.",
    "classes": {
      "TestSteeringBuilderInit": {
        "docstring": "Test SteeringBuilder initialization.",
        "methods": {
          "test_init_valid": "Test initialization with valid inputs.",
          "test_init_custom_fallback": "Test initialization with custom fallback strategy.",
          "test_init_invalid_fallback": "Test initialization fails with invalid fallback strategy.",
          "test_init_wrong_tensor_type": "Test initialization fails with non-tensor inputs.",
          "test_init_dimension_mismatch": "Test initialization fails when embedding dimensions don't match.",
          "test_init_pair_count_mismatch": "Test initialization fails when pair counts don't match.",
          "test_init_invalid_index_bounds": "Test initialization fails with out-of-bounds indices."
        }
      },
      "TestCentroidSteering": {
        "docstring": "Test centroid steering generation.",
        "methods": {
          "test_centroid_steering_shape": "Test centroid steering has correct shape.",
          "test_centroid_steering_normalized": "Test centroid steering vectors are normalized.",
          "test_centroid_distances_range": "Test centroid distances are in valid range [0, 2].",
          "test_centroid_steering_no_nan_inf": "Test centroid steering contains no NaN or Inf values."
        }
      },
      "TestKeywordWeightedSteering": {
        "docstring": "Test keyword-weighted steering generation.",
        "methods": {
          "test_keyword_steering_shape": "Test keyword steering has correct shape.",
          "test_keyword_steering_normalized": "Test keyword steering vectors are normalized.",
          "test_keyword_steering_fallback_centroid": "Test keyword steering uses centroid fallback for pairs with no keywords.",
          "test_keyword_steering_fallback_zero": "Test keyword steering uses zero fallback.",
          "test_keyword_steering_fallback_random": "Test keyword steering uses random fallback."
        }
      },
      "TestResidualSteering": {
        "docstring": "Test residual steering generation.",
        "methods": {
          "test_residual_steering_shape": "Test residual steering has correct shape.",
          "test_residual_steering_unnormalized": "Test residual steering is not normalized by default.",
          "test_residual_steering_normalized": "Test residual steering can be normalized."
        }
      },
      "TestBuildAllSteering": {
        "docstring": "Test build_all_steering method.",
        "methods": {
          "test_build_all_steering_keys": "Test build_all_steering returns all expected keys.",
          "test_build_all_steering_shapes": "Test build_all_steering returns correct shapes."
        }
      },
      "TestSteeringSave": {
        "docstring": "Test saving steering tensors.",
        "methods": {
          "test_save_creates_files": "Test save creates all expected files.",
          "test_save_with_precomputed_results": "Test save with pre-computed steering results."
        }
      },
      "TestBuildSteeringFunction": {
        "docstring": "Test build_steering convenience function.",
        "methods": {
          "test_build_steering_function": "Test build_steering convenience function works."
        }
      },
      "TestEdgeCases": {
        "docstring": "Test edge cases and error handling.",
        "methods": {
          "test_all_pairs_same_cluster": "Test steering when all pairs belong to same cluster.",
          "test_all_pairs_no_keywords": "Test steering when no pairs have keywords.",
          "test_single_pair": "Test steering with single pair."
        }
      }
    },
    "functions": {
      "sample_embeddings": "Create sample embeddings for testing.\n\nCreates:\n- 10 questions with embeddings\n- 5 keywords with embeddings\n- 3 clusters with centroids\n- 20 pairs with cluster and keyword assignments"
    }
  },
  {
    "path": "tests/test_builder_config.py",
    "file": "test_builder_config.py",
    "module": "test_builder_config",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for BuildConfig dataclass.",
    "classes": {
      "TestBuildConfigInit": {
        "docstring": "Test BuildConfig initialization and validation.",
        "methods": {
          "test_valid_initialization": "Test BuildConfig initializes with valid parameters.",
          "test_default_values": "Test BuildConfig uses correct default values.",
          "test_invalid_split_ratios_sum": "Test BuildConfig raises error when split ratios don't sum to 1.0.",
          "test_invalid_split_ratios_count": "Test BuildConfig raises error when split ratios don't have 3 values.",
          "test_invalid_split_ratios_range": "Test BuildConfig raises error when split ratios are out of range.",
          "test_invalid_steering_probabilities_sum": "Test BuildConfig raises error when steering probabilities don't sum to 1.0.",
          "test_invalid_steering_probabilities_keys": "Test BuildConfig raises error when steering probabilities have wrong keys.",
          "test_invalid_storage_format": "Test BuildConfig raises error for invalid storage format.",
          "test_invalid_curriculum_mode": "Test BuildConfig raises error for invalid curriculum mode.",
          "test_missing_curriculum_mode": "Test BuildConfig raises error when curriculum mode is missing.",
          "test_invalid_embedding_dim": "Test BuildConfig raises error for non-positive embedding dimension.",
          "test_invalid_n_neg": "Test BuildConfig raises error for non-positive n_neg."
        }
      },
      "TestBuildConfigSerialization": {
        "docstring": "Test BuildConfig save/load functionality.",
        "methods": {
          "test_save_and_load": "Test BuildConfig can be saved and loaded correctly.",
          "test_save_creates_directories": "Test save() creates parent directories if they don't exist.",
          "test_load_nonexistent_file": "Test load() raises error for non-existent file.",
          "test_to_dict": "Test to_dict() returns correct dictionary representation.",
          "test_json_format": "Test saved JSON has correct format and indentation."
        }
      },
      "TestBuildConfigUpdate": {
        "docstring": "Test BuildConfig update_post_build functionality.",
        "methods": {
          "test_update_post_build": "Test update_post_build() sets computed statistics correctly.",
          "test_update_persists_after_save": "Test updated values persist after save/load cycle."
        }
      },
      "TestBuildConfigEdgeCases": {
        "docstring": "Test BuildConfig edge cases and boundary conditions.",
        "methods": {
          "test_zero_split_ratio_allowed": "Test BuildConfig allows zero split ratio (e.g., no test set).",
          "test_hdf5_storage_format": "Test BuildConfig accepts 'hdf5' storage format.",
          "test_different_curriculum_modes": "Test BuildConfig accepts all valid curriculum modes.",
          "test_large_embedding_dim": "Test BuildConfig accepts large embedding dimensions.",
          "test_many_negatives": "Test BuildConfig accepts large number of negatives."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "tests/test_dataset_build.py",
    "file": "test_dataset_build.py",
    "module": "test_dataset_build",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for Task 9 dataset build orchestrator.",
    "classes": {
      "MockEmbeddingModel": {
        "docstring": "Lightweight deterministic embedding model for tests.",
        "methods": {
          "__init__": null,
          "encode": "Return deterministic embeddings for provided text input."
        }
      },
      "TestDatasetBuildOrchestrator": {
        "docstring": "Task 9 orchestrator coverage.",
        "methods": {
          "test_build_dataset_end_to_end": "Should run Tasks 1-8 and persist final PT dataset artifacts.",
          "test_build_dataset_rejects_non_pt_storage": "Should raise clear error for unsupported storage format in Task 9.",
          "test_build_dataset_smoke_with_buildconfig": "Should support BuildConfig input and produce a usable config.json in a smoke run."
        }
      }
    },
    "functions": {
      "cluster_json_file": "Create minimal KeywordClusterer-compatible JSON file.",
      "csv_paths": "Create two CSV inputs with 9 unique pairs across 3 clusters."
    }
  },
  {
    "path": "tests/test_dataset_check_agent.py",
    "file": "test_dataset_check_agent.py",
    "module": "test_dataset_check_agent",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for DatasetCheckAgent.",
    "classes": {
      "TestDatasetCheckAgentInit": {
        "docstring": "Test DatasetCheckAgent initialization.",
        "methods": {
          "test_init_with_llm": "Test initialization with a language model.",
          "test_init_with_custom_prompt": "Test initialization with custom prompt."
        }
      },
      "TestDatasetCheckAgentCompareTextSources": {
        "docstring": "Test compare_text_sources method.",
        "methods": {
          "test_compare_text_sources_source1_better": "Test comparison when source1 is better.",
          "test_compare_text_sources_source2_better": "Test comparison when source2 is better.",
          "test_compare_text_sources_neither": "Test comparison when neither source is good.",
          "test_compare_text_sources_with_messages": "Test that messages are returned when requested.",
          "test_compare_text_sources_error_handling": "Test error handling when LLM fails."
        }
      },
      "TestDatasetCheckAgentProcessDataFrame": {
        "docstring": "Test process_dataframe method.",
        "methods": {
          "test_process_dataframe_basic": "Test basic dataframe processing.",
          "test_process_dataframe_skip_labeled": "Test that already labeled rows are skipped when skip_labeled=True.",
          "test_process_dataframe_start_index": "Test processing with start_index.",
          "test_process_dataframe_invalid_start_index": "Test processing with invalid start_index."
        }
      },
      "TestDatasetCheckAgentIntegration": {
        "docstring": "Integration tests for DatasetCheckAgent.",
        "methods": {
          "test_real_llm_integration": "Test with real LLM (requires API key)."
        }
      }
    },
    "functions": {
      "test_agent_import": "Test that DatasetCheckAgent can be imported.",
      "test_check_agent_state_import": "Test that CheckAgentState can be imported."
    }
  },
  {
    "path": "tests/test_dataset_splitter.py",
    "file": "test_dataset_splitter.py",
    "module": "test_dataset_splitter",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Unit tests for dataset_splitter module.\n\nTests cover:\n- DatasetSplitter initialization\n- Split generation with various ratios\n- Save/load functionality\n- Split validation\n- Integration with ClusterLabeledDataset\n- Consistency across multiple loads",
    "classes": {
      "TestDatasetSplitterInit": {
        "docstring": "Test DatasetSplitter initialization.",
        "methods": {
          "test_init_default": "Test initialization with default parameters.",
          "test_init_with_random_state": "Test initialization with random state."
        }
      },
      "TestDatasetSplitterSplit": {
        "docstring": "Test dataset splitting operations.",
        "methods": {
          "test_split_basic": "Test basic split with default parameters.",
          "test_split_different_ratios": "Test splitting with different validation ratios.",
          "test_split_reproducibility": "Test that splits are reproducible with same random_state.",
          "test_split_different_seeds": "Test that different seeds produce different splits.",
          "test_split_no_shuffle": "Test split without shuffling.",
          "test_split_invalid_val_ratio": "Test split with invalid validation ratio.",
          "test_split_invalid_dataset_size": "Test split with invalid dataset size.",
          "test_get_split": "Test getting split after creation.",
          "test_get_split_before_creation": "Test getting split before creating one raises error."
        }
      },
      "TestDatasetSplitterSaveLoad": {
        "docstring": "Test save/load operations.",
        "methods": {
          "test_save_and_load_basic": "Test basic save and load functionality.",
          "test_save_with_metadata": "Test saving with additional metadata.",
          "test_save_without_split": "Test saving without creating a split raises error.",
          "test_load_nonexistent_file": "Test loading non-existent file raises error.",
          "test_load_invalid_format": "Test loading invalid file format raises error.",
          "test_multiple_loads_consistency": "Test loading same split multiple times produces consistent results."
        }
      },
      "TestDatasetSplitterValidation": {
        "docstring": "Test split validation operations.",
        "methods": {
          "test_validate_split_valid": "Test validation of valid split.",
          "test_validate_split_size_mismatch": "Test validation with size mismatch produces warning.",
          "test_validate_split_indices_out_of_bounds": "Test validation with indices out of bounds raises error.",
          "test_validate_split_before_creation": "Test validation before creating split raises error."
        }
      },
      "TestConvenienceFunction": {
        "docstring": "Test create_train_val_split convenience function.",
        "methods": {
          "test_basic_usage": "Test basic usage of convenience function.",
          "test_with_save": "Test convenience function with save.",
          "test_with_metadata": "Test convenience function with metadata."
        }
      },
      "TestEdgeCases": {
        "docstring": "Test edge cases and boundary conditions.",
        "methods": {
          "test_small_dataset": "Test split with very small dataset.",
          "test_large_dataset": "Test split with large dataset.",
          "test_extreme_ratio": "Test with extreme but valid ratios."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "tests/test_decomposed_predictor.py",
    "file": "test_decomposed_predictor.py",
    "module": "test_decomposed_predictor",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for DecomposedJASPERPredictor.",
    "classes": {
      "TestInit": {
        "docstring": null,
        "methods": {
          "test_sub_modules_present": null,
          "test_num_subspaces": null,
          "test_embedding_dim_property": null,
          "test_hidden_dim_property": null,
          "test_from_dict": null,
          "test_invalid_fine_input_mode_raises": null,
          "test_add_mode_has_coarse_projector": null,
          "test_concat_mode_no_coarse_projector": null,
          "test_get_model_summary": null,
          "test_repr": null
        }
      },
      "TestForward": {
        "docstring": null,
        "methods": {
          "test_prediction_shape": null,
          "test_explanation_dict_keys": null,
          "test_explanation_shapes": null,
          "test_explanation_all_detached": null,
          "test_no_nan_in_prediction": null,
          "test_wrong_centroid_K_raises": null,
          "test_different_batch_sizes": null,
          "test_routing_weights_sum_to_one": null
        }
      },
      "TestArithmetic": {
        "docstring": null,
        "methods": {
          "test_prediction_equals_coarse_plus_fine": "Verify that prediction = coarse + fine numerically.",
          "test_atypicality_equals_fine_norm": null
        }
      },
      "TestFineModes": {
        "docstring": null,
        "methods": {
          "test_add_mode_produces_valid_output": null,
          "test_concat_mode_same_shapes_as_add": null
        }
      },
      "TestGradients": {
        "docstring": null,
        "methods": {
          "test_all_params_receive_gradient": null,
          "test_centroid_embs_receives_gradient": null
        }
      },
      "TestLatents": {
        "docstring": null,
        "methods": {
          "test_cached_after_forward": null,
          "test_latents_are_detached": null,
          "test_latents_on_cpu": null,
          "test_get_routing_weights": null
        }
      }
    },
    "functions": {
      "_make_model": null,
      "_make_inputs": null
    }
  },
  {
    "path": "tests/test_domain_assesment.py",
    "file": "test_domain_assesment.py",
    "module": "test_domain_assesment",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for domain assessment agent.",
    "classes": {},
    "functions": {
      "test_pydantic_models_import": "Test that Pydantic models can be imported.",
      "test_domain_suggestion_validation": "Test DomainSuggestion validation.",
      "test_domain_extraction_result_validation": "Test DomainExtractionResult auto-correction of total_suggestions.",
      "test_question_topic_relevance_result_validation": "Test QuestionTopicRelevanceResult validation.",
      "test_operation_mode_enum": "Test OperationMode enum.",
      "create_mock_llm": "Create a mock LLM for testing.",
      "test_domain_analysis_agent_initialization": "Test DomainAnalysisAgent initialization.",
      "test_parse_topic_descriptors_list_of_strings": "Test parsing topic descriptors from list of strings.",
      "test_parse_topic_descriptors_json_string": "Test parsing topic descriptors from JSON string.",
      "test_parse_topic_descriptors_keywordclusterer_dict": "Test parsing topic descriptors from KeywordClusterer dict format.",
      "test_parse_topic_descriptors_file_path": "Test parsing topic descriptors from file path.",
      "test_parse_topic_descriptors_keywordclusterer_file": "Test parsing topic descriptors from KeywordClusterer JSON file.",
      "test_parse_topic_descriptors_invalid_dict": "Test error handling for invalid dict format.",
      "test_parse_topic_descriptors_invalid_file": "Test handling of non-existent file path (treated as single descriptor).",
      "test_parse_topic_descriptors_single_string": "Test parsing single string as single descriptor.",
      "test_parse_topic_descriptors_fallback_to_clusters": "Test fallback to 'clusters' key when 'cluster_stats' is missing.",
      "test_create_relevance_json_mapping": "Test creating JSON mapping from topic scores.",
      "test_get_column_prefix": "Test _get_column_prefix method.",
      "test_extract_result_dict": "Test _extract_result_dict method.",
      "test_check_openai_llm": "Test _check_openai_llm method.",
      "test_extract_domains_success": "Test extract_domains method with successful result.",
      "test_extract_domains_failure": "Test extract_domains method with failure.",
      "test_guess_domains": "Test guess_domains method.",
      "test_assess_domains": "Test assess_domains method.",
      "test_assess_domains_invalid_json": "Test assess_domains with invalid JSON string.",
      "test_assess_topic_relevance_prob": "Test assess_topic_relevance_prob method.",
      "test_assess_topic_relevance_prob_with_keywordclusterer_dict": "Test assess_topic_relevance_prob with KeywordClusterer dict.",
      "test_batch_processing_extract_domains_sequential": "Test extract_domains_batch with sequential processing.",
      "test_batch_processing_guess_domains_sequential": "Test guess_domains_batch with sequential processing.",
      "test_batch_processing_assess_domains_sequential": "Test assess_domains_batch with sequential processing.",
      "test_batch_processing_topic_relevance_sequential": "Test assess_topic_relevance_prob_batch with sequential processing.",
      "test_get_parser_for_mode": "Test _get_parser_for_mode method.",
      "test_get_template_for_mode": "Test _get_template_for_mode method.",
      "test_include_reason_parameter": "Test that include_reason parameter is stored correctly.",
      "test_max_retries_parameter": "Test that max_retries parameter is stored correctly.",
      "test_batch_size_parameter": "Test that batch_size parameter is stored correctly.",
      "test_agent_state_initialization": "Test AgentState initialization.",
      "test_parse_topic_descriptors_with_duplicate_descriptors": "Test that duplicate topic descriptors are deduplicated."
    }
  },
  {
    "path": "tests/test_ema_encoder.py",
    "file": "test_ema_encoder.py",
    "module": "test_ema_encoder",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for EMAEncoder.",
    "classes": {
      "TestInit": {
        "docstring": null,
        "methods": {
          "test_creates_target_copy": "Target encoder must be a distinct object with equal initial weights.",
          "test_target_requires_no_grad": null,
          "test_online_requires_grad": null,
          "test_invalid_tau_raises": null,
          "test_invalid_schedule_raises": null
        }
      },
      "TestTauSchedule": {
        "docstring": null,
        "methods": {
          "test_tau_at_step_zero": null,
          "test_tau_at_last_step": null,
          "test_tau_monotonically_increasing": null,
          "test_tau_within_bounds": null,
          "test_tau_clamps_beyond_max_steps": null,
          "test_zero_max_steps_returns_tau_max": null,
          "test_tau_info_dict": null
        }
      },
      "TestEMAUpdate": {
        "docstring": null,
        "methods": {
          "test_target_params_change_after_update": null,
          "test_target_drifts_slower_than_online": "Target params should change less than online params per step.",
          "test_update_does_not_restore_grad_to_target": null,
          "test_multiple_updates_converge": "After many updates the target should approach the online encoder."
        }
      },
      "TestForward": {
        "docstring": null,
        "methods": {
          "test_forward_uses_online_encoder": null,
          "test_encode_target_uses_target_encoder": null,
          "test_encode_target_returns_detached": null,
          "test_online_forward_produces_grad": null,
          "test_target_does_not_participate_in_backprop": "No grad should accumulate in target params."
        }
      },
      "TestStateDict": {
        "docstring": null,
        "methods": {
          "test_state_dict_contains_tau_keys": null,
          "test_state_dict_round_trip": null,
          "test_load_restores_frozen_target": null,
          "test_repr": null
        }
      }
    },
    "functions": {
      "small_encoder": null,
      "ema": null,
      "x": null
    }
  },
  {
    "path": "tests/test_embed.py",
    "file": "test_embed.py",
    "module": "test_embed",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for Embedding Generator.",
    "classes": {
      "MockEmbeddingModel": {
        "docstring": "Mock sentence-transformers model for testing.",
        "methods": {
          "__init__": null,
          "encode": "Mock encode method that returns deterministic embeddings."
        }
      },
      "TestEmbeddingGeneratorInit": {
        "docstring": "Test EmbeddingGenerator initialization.",
        "methods": {
          "test_init_without_cluster_parser": "Test initialization without cluster parser.",
          "test_init_with_cluster_parser": "Test initialization with cluster parser.",
          "test_init_custom_batch_size": "Test initialization with custom batch size."
        }
      },
      "TestEmbeddingGeneratorValidation": {
        "docstring": "Test embedding validation methods.",
        "methods": {
          "test_check_valid_embeddings": "Test validation of valid embeddings.",
          "test_check_nan_embeddings": "Test detection of NaN values.",
          "test_check_inf_embeddings": "Test detection of Inf values.",
          "test_check_zero_embeddings": "Test detection of all-zero embeddings."
        }
      },
      "TestEmbeddingGeneratorCentroidValidation": {
        "docstring": "Test centroid similarity validation.",
        "methods": {
          "test_validate_centroid_similarity_valid": "Test validation passes for similar centroids.",
          "test_validate_centroid_similarity_no_parser": "Test validation skips when no cluster parser."
        }
      },
      "TestEmbeddingGeneratorTextEmbeddings": {
        "docstring": "Test text embedding generation.",
        "methods": {
          "test_generate_text_embeddings_basic": "Test generating embeddings for texts.",
          "test_generate_text_embeddings_empty_list": "Test generation fails with empty text list.",
          "test_generate_text_embeddings_with_normalization": "Test generation with L2 normalization.",
          "test_generate_text_embeddings_deterministic": "Test embeddings are deterministic for same text."
        }
      },
      "TestEmbeddingGeneratorKeywordEmbeddings": {
        "docstring": "Test keyword embedding generation.",
        "methods": {
          "test_generate_keyword_embeddings": "Test generating keyword embeddings with mapping."
        }
      },
      "TestEmbeddingGeneratorCentroidEmbeddings": {
        "docstring": "Test centroid embedding generation.",
        "methods": {
          "test_generate_centroid_embeddings": "Test generating centroid embeddings from cluster parser.",
          "test_generate_centroid_embeddings_no_parser": "Test generation fails without cluster parser.",
          "test_generate_centroid_embeddings_with_validation": "Test centroid generation with keyword validation."
        }
      },
      "TestEmbeddingGeneratorAllEmbeddings": {
        "docstring": "Test generating all embeddings from DataFrame.",
        "methods": {
          "test_generate_all_embeddings_basic": "Test generating all embeddings from DataFrame.",
          "test_generate_all_embeddings_shapes": "Test embeddings have correct shapes.",
          "test_generate_all_embeddings_string_keywords": "Test handling keywords as comma-separated strings."
        }
      },
      "TestEmbeddingGeneratorSaveEmbeddings": {
        "docstring": "Test saving embeddings to files.",
        "methods": {
          "test_save_embeddings_basic": "Test saving embeddings to directory.",
          "test_save_embeddings_with_prefix": "Test saving embeddings with filename prefix.",
          "test_save_embeddings_creates_directory": "Test saving creates output directory if not exists.",
          "test_save_and_load_embeddings": "Test saved embeddings can be loaded back."
        }
      },
      "TestGenerateEmbeddingsConvenience": {
        "docstring": "Test convenience function.",
        "methods": {
          "test_generate_embeddings_basic": "Test convenience function generates all embeddings.",
          "test_generate_embeddings_with_save": "Test convenience function saves embeddings to directory.",
          "test_generate_embeddings_custom_columns": "Test convenience function with custom column names."
        }
      },
      "TestEmbeddingGeneratorEdgeCases": {
        "docstring": "Test edge cases.",
        "methods": {
          "test_generate_embeddings_single_text": "Test generating embeddings for single text.",
          "test_generate_embeddings_large_batch": "Test generating embeddings for large batch.",
          "test_generate_embeddings_unicode_text": "Test generating embeddings for unicode text.",
          "test_generate_all_embeddings_no_cluster_parser": "Test generating embeddings without cluster parser (no centroids)."
        }
      }
    },
    "functions": {
      "mock_model": "Create mock embedding model.",
      "sample_clustering_json": "Create sample KeywordClusterer JSON format.",
      "clustering_json_file": "Create temporary clustering JSON file.",
      "cluster_parser": "Create ClusterParser instance."
    }
  },
  {
    "path": "tests/test_finalize.py",
    "file": "test_finalize.py",
    "module": "test_finalize",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for dataset finalization and config writing (Task 8).",
    "classes": {
      "TestDatasetFinalizerImport": {
        "docstring": "Import sanity checks.",
        "methods": {
          "test_import": "Test that finalizer symbols are importable."
        }
      },
      "TestDatasetFinalizerInit": {
        "docstring": "Initialization tests.",
        "methods": {
          "test_init_valid_path": "Test finalizer initialization on valid directory.",
          "test_init_missing_path": "Test initialization fails for missing directory."
        }
      },
      "TestDatasetFinalizerValidation": {
        "docstring": "Validation behavior tests.",
        "methods": {
          "test_finalize_valid_with_config": "Test successful finalization with explicit config.",
          "test_finalize_via_convenience_function": "Test convenience function finalization API.",
          "test_finalize_missing_required_file": "Test error when one required artifact is missing.",
          "test_finalize_detects_pair_shape_mismatch": "Test error when pair_index has invalid shape.",
          "test_finalize_detects_true_source_in_negatives": "Test error when hard negatives include true source.",
          "test_finalize_detects_split_overlap": "Test error when splits overlap.",
          "test_finalize_detects_embedding_dim_mismatch_with_config": "Test error when config embedding_dim differs from artifacts.",
          "test_finalize_detects_n_neg_mismatch_with_config": "Test error when config n_neg differs from artifacts."
        }
      }
    },
    "functions": {
      "_write_valid_pt_dataset": "Create a minimal valid PT dataset artifact set for finalizer tests.",
      "valid_output_dir": "Create a temp output directory with valid PT artifacts."
    }
  },
  {
    "path": "tests/test_jasper_losses.py",
    "file": "test_jasper_losses.py",
    "module": "test_jasper_losses",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for JASPER multi-objective loss functions.",
    "classes": {
      "TestJASPERLoss": {
        "docstring": null,
        "methods": {
          "test_returns_dict": null,
          "test_loss_is_scalar": null,
          "test_loss_is_non_negative": null,
          "test_zero_loss_for_identical_inputs": null,
          "test_gradients_flow": null,
          "test_invalid_reduction_raises": null
        }
      },
      "TestContrastiveLoss": {
        "docstring": null,
        "methods": {
          "test_returns_dict": null,
          "test_output_is_scalar": null,
          "test_non_negative_loss": null,
          "test_lower_temperature_gives_larger_loss_variance": "Lower temperature should generally sharpen predictions.",
          "test_gradients_flow": null,
          "test_invalid_temperature_raises": null,
          "test_single_negative": null
        }
      },
      "TestCentroidLoss": {
        "docstring": null,
        "methods": {
          "test_returns_dict": null,
          "test_loss_is_scalar": null,
          "test_accuracy_in_range": null,
          "test_perfect_prediction_gives_high_accuracy": "If predicted == centroid for each sample, accuracy should be 1.0.",
          "test_gradients_flow": null,
          "test_accuracy_is_detached": null
        }
      },
      "TestVICRegLoss": {
        "docstring": null,
        "methods": {
          "test_returns_dict_with_components": null,
          "test_total_loss_is_scalar": null,
          "test_non_negative_total_loss": null,
          "test_gradients_flow": null,
          "test_vicreg_prevents_collapse": "Constant embeddings (collapsed) should produce high variance loss.",
          "test_sub_components_are_detached": null
        }
      },
      "TestJASPERMultiObjectiveLoss": {
        "docstring": null,
        "methods": {
          "loss_fn": null,
          "batch_inputs": null,
          "test_returns_total_and_dict": null,
          "test_total_is_scalar": null,
          "test_all_components_present": null,
          "test_total_gradients_flow": null,
          "test_sub_components_detached": null,
          "test_zero_weights_produce_zero_loss": null,
          "test_repr": null
        }
      }
    },
    "functions": {
      "_rand": null,
      "_make_batch": "Return a consistent dummy batch."
    }
  },
  {
    "path": "tests/test_jasper_predictor.py",
    "file": "test_jasper_predictor.py",
    "module": "test_jasper_predictor",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for JASPERPredictor model.",
    "classes": {
      "TestInit": {
        "docstring": null,
        "methods": {
          "test_from_config_object": null,
          "test_from_dict": null,
          "test_invalid_num_layers_raises": null,
          "test_invalid_dropout_raises": null,
          "test_invalid_activation_raises": null,
          "test_sub_networks_present": null,
          "test_properties": null,
          "test_has_parameters": null
        }
      },
      "TestForward": {
        "docstring": null,
        "methods": {
          "test_output_shape": null,
          "test_output_dtype_float32": null,
          "test_different_batch_sizes": null,
          "test_no_nan_in_output": null,
          "test_normalize_output_flag": null,
          "test_single_layer_model": null,
          "test_deep_model": null
        }
      },
      "TestGradients": {
        "docstring": null,
        "methods": {
          "test_gradients_flow_to_all_params": null,
          "test_no_grad_context_produces_no_grad": null,
          "test_grad_enabled_produces_grad": null
        }
      },
      "TestLatentRepresentations": {
        "docstring": null,
        "methods": {
          "test_empty_before_forward": null,
          "test_populated_after_forward": null,
          "test_latent_shapes": null,
          "test_latents_are_detached": null
        }
      },
      "TestConfigInstantiation": {
        "docstring": null,
        "methods": {
          "test_from_dict_with_extra_keys": "Extra keys should be silently ignored.",
          "test_config_from_dict_classmethod": null,
          "test_summary_string": null,
          "test_repr": null
        }
      }
    },
    "functions": {
      "default_config": null,
      "model": null,
      "batch": "Small deterministic batch."
    }
  },
  {
    "path": "tests/test_jasper_steering_dataset.py",
    "file": "test_jasper_steering_dataset.py",
    "module": "test_jasper_steering_dataset",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for JASPERSteeringDataset.",
    "classes": {},
    "functions": {
      "mock_dataset_dir": "Create a mock dataset directory with all required files.",
      "test_dataset_init": "Test dataset initialization.",
      "test_dataset_splits": "Test that different splits load correct number of samples.",
      "test_getitem_schema": "Test that __getitem__ returns correct schema and shapes.",
      "test_steering_variant_distribution": "Test that steering variant distribution matches configuration over many samples.",
      "test_set_epoch_changes_probs": "Test that set_epoch changes steering probabilities.",
      "test_force_steering_zero": "Test forcing steering to zero variant.",
      "test_force_steering_centroid": "Test forcing steering to centroid variant.",
      "test_force_steering_restore": "Test restoring stochastic steering after forcing.",
      "test_reload_negatives": "Test hot-reloading hard negatives.",
      "test_no_nan_inf": "Test that dataset never returns NaN or Inf values.",
      "test_relevance_in_range": "Test that relevance scores are in [0, 1] range.",
      "test_centroid_distance_in_range": "Test that centroid distances are in [0, 2] range.",
      "test_cluster_id_valid": "Test that cluster IDs are valid indices.",
      "test_steering_variant_valid": "Test that steering variants are in valid range [0, 3].",
      "test_determinism_with_same_epoch": "Test that same epoch gives same samples (deterministic RNG).",
      "test_different_epochs_different_samples": "Test that different epochs give different samples.",
      "test_invalid_split_raises": "Test that invalid split raises error.",
      "test_missing_config_raises": "Test that missing config.json raises error.",
      "test_missing_dataset_dir_raises": "Test that missing dataset directory raises error.",
      "test_index_out_of_bounds_raises": "Test that out-of-bounds index raises IndexError.",
      "test_context_manager_support": "Test that dataset works as context manager.",
      "test_device_placement_cpu": "Test dataset loads to CPU by default.",
      "test_device_placement_cuda": "Test dataset loads to CUDA when requested.",
      "test_create_combined_splits": "Test loading all splits at once.",
      "test_referential_integrity_invalid_question_idx": "Test that invalid question index in pair_index raises error.",
      "test_referential_integrity_invalid_source_idx": "Test that invalid source index in pair_index raises error.",
      "test_referential_integrity_invalid_negative_idx": "Test that invalid index in hard_negatives raises error.",
      "test_referential_integrity_split_indices": "Test that split indices reference valid pairs.",
      "test_memory_usage_logging": "Test that memory usage is logged during initialization.",
      "test_close_method": "Test explicit close method.",
      "test_storage_format_detection_pt": "Test storage format detection defaults to PT.",
      "test_storage_format_explicit_pt": "Test explicit PT format specification.",
      "test_memory_mapping_disabled_by_default": "Test that memory mapping is disabled for small datasets.",
      "test_memory_mapping_explicit_enable": "Test explicit memory mapping enable.",
      "test_create_combined_splits_with_storage_format": "Test create_combined_splits with storage format parameter.",
      "test_hdf5_conversion_requires_h5py": "Test that HDF5 conversion checks for h5py.",
      "test_hdf5_storage_format_error_without_h5py": "Test that requesting HDF5 format without h5py raises proper error.",
      "test_storage_format_auto_no_files": "Test storage format detection with no valid files.",
      "test_invalid_storage_format": "Test invalid storage format raises error.",
      "test_hdf5_integration": "Test full HDF5 workflow: convert and load.",
      "test_hdf5_auto_detection": "Test that storage format auto-detection prefers HDF5 when available.",
      "test_memory_mapping_auto_enable_large_dataset": "Test that memory mapping auto-enables for large datasets."
    }
  },
  {
    "path": "tests/test_jasper_trainer.py",
    "file": "test_jasper_trainer.py",
    "module": "test_jasper_trainer",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for JASPERTrainer.",
    "classes": {
      "_MockDataset": {
        "docstring": "Tiny in-memory dataset that mimics the JASPER batch structure.",
        "methods": {
          "__init__": null,
          "__len__": null,
          "__getitem__": null,
          "set_epoch": null
        }
      },
      "TestInit": {
        "docstring": null,
        "methods": {
          "test_device_is_cpu": null,
          "test_checkpoint_dir_created": null,
          "test_centroids_loaded_from_dataset": null,
          "test_from_dict_config": null
        }
      },
      "TestTrainStep": {
        "docstring": null,
        "methods": {
          "test_single_step_returns_metrics": null,
          "test_parameters_updated_after_step": null,
          "test_ema_target_updated_after_step": null,
          "test_loss_is_finite": null
        }
      },
      "TestEpoch": {
        "docstring": null,
        "methods": {
          "test_train_epoch_returns_dict": null,
          "test_validate_returns_dict": null,
          "test_global_step_increments": null,
          "test_model_in_train_mode_during_epoch": null,
          "test_validate_model_in_eval_mode": null
        }
      },
      "TestCheckpoint": {
        "docstring": null,
        "methods": {
          "test_save_creates_file": null,
          "test_checkpoint_contains_required_keys": null,
          "test_load_restores_epoch": null,
          "test_load_restores_model_weights": null,
          "test_load_restores_global_step": null,
          "test_checkpoint_rotation": null
        }
      },
      "TestCurriculum": {
        "docstring": null,
        "methods": {
          "test_set_epoch_called_on_dataset": null
        }
      }
    },
    "functions": {
      "_make_batch": null,
      "_make_loader": null,
      "_make_source_encoder": null,
      "trainer_components": null
    }
  },
  {
    "path": "tests/test_keyword_clustering.py",
    "file": "test_keyword_clustering.py",
    "module": "test_keyword_clustering",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Unit tests for keyword_clustering module.\n\nTests cover:\n- KeywordClusterer initialization\n- Clustering operations (fit, predict)\n- Topic descriptor extraction\n- Assignment operations (hard/soft modes)\n- Configuration persistence (save/load)\n- State management",
    "classes": {
      "TestKeywordClustererInit": {
        "docstring": "Test KeywordClusterer initialization.",
        "methods": {
          "test_init_parameters": "Test initialization with default and custom parameters.",
          "test_init_invalid_algorithm": "Test initialization with invalid algorithm."
        }
      },
      "TestKeywordClustererFitting": {
        "docstring": "Test KeywordClusterer fitting operations.",
        "methods": {
          "test_fit": "Test successful fitting and state verification.",
          "test_get_cluster_assignments": "Test getting cluster assignments.",
          "test_get_cluster_assignments_unfitted": "Test getting assignments before fitting raises error.",
          "test_get_clusters_and_centroids": "Test getting clusters and centroids."
        }
      },
      "TestTopicDescriptorExtraction": {
        "docstring": "Test topic descriptor extraction.",
        "methods": {
          "test_extract_topic_descriptors": "Test extracting topic descriptors with different metrics.",
          "test_extract_topic_descriptors_ignore_n_closest": "Test topic descriptor extraction with ignore_n_closest_keywords parameter.",
          "test_extract_topic_descriptors_min_distance": "Test topic descriptor extraction with min_descriptor_distance parameter.",
          "test_extract_topic_descriptors_combined_filters": "Test topic descriptor extraction with both ignore_n_closest and min_distance.",
          "test_extract_topic_descriptors_invalid_metric": "Test topic extraction with invalid metric.",
          "test_extract_topic_descriptors_unfitted": "Test extracting topics before fitting raises error."
        }
      },
      "TestAssignmentConfiguration": {
        "docstring": "Test assignment configuration.",
        "methods": {
          "test_configure_assignment": "Test configuring assignment parameters.",
          "test_configure_assignment_invalid_mode": "Test configuring with invalid mode.",
          "test_configure_assignment_invalid_metric": "Test configuring with invalid metric."
        }
      },
      "TestAssignmentOperations": {
        "docstring": "Test assignment operations.",
        "methods": {
          "test_embedding": "Create a test embedding.",
          "test_assign_modes": "Test assignment with default, hard, and soft modes with thresholds.",
          "test_assign_invalid_mode": "Test assignment with invalid mode.",
          "test_assign_unfitted": "Test assignment before fitting raises error.",
          "test_assign_batch": "Test batch assignment with custom parameters."
        }
      },
      "TestPersistence": {
        "docstring": "Test save/load operations.",
        "methods": {
          "configured_clusterer": "Create a configured clusterer with topics.",
          "test_save_and_load_results": "Test saving results and loading with all configurations restored."
        }
      },
      "TestConvenienceFunction": {
        "docstring": "Test cluster_keywords_from_embeddings convenience function.",
        "methods": {
          "test_cluster_keywords_from_embeddings": "Test convenience function with and without saving.",
          "test_cluster_keywords_with_filtering_options": "Test convenience function with new filtering parameters."
        }
      },
      "TestComputeDistances": {
        "docstring": "Test distance computation methods.",
        "methods": {
          "test_embedding": "Create a test embedding.",
          "test_compute_distances": "Test distance computation with different metrics.",
          "test_compute_distances_invalid_metric": "Test distance computation with invalid metric."
        }
      }
    },
    "functions": {
      "sample_embeddings": "Create sample keyword embeddings for testing.\n\nThis fixture generates 20 synthetic embeddings designed to form 3 distinct clusters:\n- Cluster 1 (keywords 0-6): Base vector [1.0, 0.0, 0.0, ...] + small noise\n- Cluster 2 (keywords 7-13): Base vector [0.0, 1.0, 0.0, ...] + small noise\n- Cluster 3 (keywords 14-19): Base vector [0.0, 0.0, 1.0, ...] + small noise\n\nEach embedding is normalized to unit length after adding Gaussian noise (\u03c3=0.05)\nto simulate realistic embedding variations within clusters.\n\nReturns\n-------\ndict\n    Dictionary mapping keyword names to 10-dimensional normalized embeddings",
      "fitted_clusterer": "Create a fitted clusterer for testing."
    }
  },
  {
    "path": "tests/test_link_sources.py",
    "file": "test_link_sources.py",
    "module": "test_link_sources",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for Source-Cluster Linker.",
    "classes": {
      "TestSourceClusterLinkerInit": {
        "docstring": "Test SourceClusterLinker initialization.",
        "methods": {
          "test_init_valid": "Test initialization with valid cluster parser.",
          "test_init_custom_fallback": "Test initialization with custom fallback strategy.",
          "test_init_invalid_fallback": "Test initialization fails with invalid fallback strategy.",
          "test_compute_cluster_sizes": "Test cluster size computation."
        }
      },
      "TestSourceClusterLinkerFallback": {
        "docstring": "Test fallback cluster assignment.",
        "methods": {
          "test_fallback_largest": "Test fallback to largest cluster.",
          "test_fallback_uniform": "Test fallback with uniform distribution.",
          "test_fallback_random": "Test fallback with random assignment."
        }
      },
      "TestSourceClusterLinkerSinglePair": {
        "docstring": "Test linking single pairs to clusters.",
        "methods": {
          "test_link_single_keyword": "Test linking pair with single keyword.",
          "test_link_multiple_keywords_same_cluster": "Test linking pair with multiple keywords from same cluster.",
          "test_link_multiple_keywords_different_clusters": "Test linking pair with keywords from different clusters (majority voting).",
          "test_link_empty_keywords": "Test linking pair with no keywords uses fallback.",
          "test_link_unmatched_keywords": "Test linking pair with unmatched keywords uses fallback."
        }
      },
      "TestSourceClusterLinkerBatch": {
        "docstring": "Test batch linking of pairs.",
        "methods": {
          "test_link_batch_basic": "Test batch linking of multiple pairs.",
          "test_link_batch_with_pair_ids": "Test batch linking with explicit pair IDs."
        }
      },
      "TestSourceClusterLinkerDataFrame": {
        "docstring": "Test DataFrame linking.",
        "methods": {
          "test_link_dataframe_basic": "Test linking DataFrame with keywords.",
          "test_link_dataframe_custom_columns": "Test linking DataFrame with custom column names.",
          "test_link_dataframe_missing_keywords_col": "Test linking DataFrame fails when keywords column missing.",
          "test_link_dataframe_missing_pair_id_col": "Test linking DataFrame uses index when pair_id column missing.",
          "test_link_dataframe_string_keywords": "Test linking DataFrame with keywords as comma-separated strings."
        }
      },
      "TestSourceClusterLinkerValidation": {
        "docstring": "Test cluster assignment validation.",
        "methods": {
          "test_validate_valid_assignments": "Test validation of valid cluster assignments.",
          "test_validate_invalid_range_negative": "Test validation fails with negative cluster IDs.",
          "test_validate_invalid_range_too_large": "Test validation fails with cluster IDs >= n_clusters.",
          "test_validate_missing_clusters": "Test validation warns when clusters have no assignments.",
          "test_validate_imbalanced_distribution": "Test validation warns about highly imbalanced distribution.",
          "test_validate_torch_tensor": "Test validation works with PyTorch tensors.",
          "test_validate_numpy_array": "Test validation works with numpy arrays."
        }
      },
      "TestLinkSourcesConvenience": {
        "docstring": "Test convenience function.",
        "methods": {
          "test_link_sources_basic": "Test convenience function for linking sources.",
          "test_link_sources_custom_fallback": "Test convenience function with custom fallback strategy."
        }
      },
      "TestSourceClusterLinkerEdgeCases": {
        "docstring": "Test edge cases.",
        "methods": {
          "test_link_pair_case_insensitive": "Test keyword matching is case-insensitive.",
          "test_link_pair_with_none": "Test linking pair handles None keywords gracefully.",
          "test_link_dataframe_large": "Test linking large DataFrame efficiently."
        }
      }
    },
    "functions": {
      "sample_clustering_json": "Create sample KeywordClusterer JSON format.",
      "clustering_json_file": "Create temporary clustering JSON file.",
      "cluster_parser": "Create ClusterParser instance."
    }
  },
  {
    "path": "tests/test_loader.py",
    "file": "test_loader.py",
    "module": "test_loader",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for JASPER Steering DataLoader.",
    "classes": {},
    "functions": {
      "mock_dataset_dir": "Create a mock dataset directory with all required files.",
      "test_create_loader_basic": "Test basic DataLoader creation.",
      "test_create_loader_all_splits": "Test DataLoader creation for all splits.",
      "test_loader_iteration": "Test iterating through DataLoader.",
      "test_loader_batch_shapes": "Test that DataLoader produces correct batch shapes.",
      "test_validate_first_batch_passes": "Test that validate_first_batch passes for valid DataLoader.",
      "test_validate_first_batch_all_keys": "Test that validate_first_batch checks all expected keys.",
      "test_set_epoch_on_loader": "Test set_epoch function on DataLoader.",
      "test_drop_last_train_vs_val": "Test that drop_last defaults to True for train, False for val.",
      "test_drop_last_override": "Test that drop_last can be overridden.",
      "test_pin_memory_enabled": "Test that pin_memory is enabled by default.",
      "test_pin_memory_disabled": "Test that pin_memory can be disabled.",
      "test_num_workers_configurable": "Test that num_workers can be configured.",
      "test_loader_shuffle_train": "Test that train loader shuffles by default.",
      "test_loader_no_shuffle_val": "Test that val loader does not shuffle.",
      "test_distributed_sampler_not_used_by_default": "Test that DistributedSampler is not used by default.",
      "test_loader_one_epoch_iteration_count": "Test that one epoch iterates correct number of batches.",
      "test_loader_with_epoch_curriculum": "Test DataLoader with curriculum learning across epochs.",
      "test_loader_full_pipeline": "Test full DataLoader pipeline: create, iterate, validate.",
      "test_loader_different_batch_sizes": "Test DataLoader with different batch sizes.",
      "test_loader_test_split": "Test DataLoader for test split.",
      "test_loader_force_steering_in_validation": "Test forcing steering variant in validation mode."
    }
  },
  {
    "path": "tests/test_merge_csv.py",
    "file": "test_merge_csv.py",
    "module": "test_merge_csv",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for CSV merger.",
    "classes": {
      "TestCSVMergerInit": {
        "docstring": "Test CSVMerger initialization.",
        "methods": {
          "test_default_aliases": "Test CSVMerger initializes with default aliases.",
          "test_custom_aliases": "Test CSVMerger accepts custom aliases."
        }
      },
      "TestCSVMergerColumnFinding": {
        "docstring": "Test column finding with aliases.",
        "methods": {
          "test_find_column_exact_match": "Test _find_column() finds exact match.",
          "test_find_column_alias_match": "Test _find_column() finds alias match.",
          "test_find_column_no_match": "Test _find_column() returns None when no match."
        }
      },
      "TestCSVMergerNormalization": {
        "docstring": "Test DataFrame normalization.",
        "methods": {
          "test_normalize_minimal_columns": "Test normalization with only required columns.",
          "test_normalize_all_columns": "Test normalization with all columns present.",
          "test_normalize_missing_required_column": "Test normalization raises error when required column is missing.",
          "test_normalize_with_aliases": "Test normalization works with column aliases.",
          "test_normalize_clips_scores": "Test normalization clips relevance scores to [0, 1]."
        }
      },
      "TestCSVMergerKeywordParsing": {
        "docstring": "Test keyword parsing from various formats.",
        "methods": {
          "test_parse_keywords_comma_separated": "Test parsing comma-separated keywords.",
          "test_parse_keywords_json_list": "Test parsing JSON list of keywords.",
          "test_parse_keywords_single_string": "Test parsing single keyword string.",
          "test_parse_keywords_empty": "Test parsing empty keywords.",
          "test_parse_keywords_nan": "Test parsing NaN keywords.",
          "test_parse_keywords_list": "Test parsing list of keywords."
        }
      },
      "TestCSVMergerDeduplication": {
        "docstring": "Test duplicate merging logic.",
        "methods": {
          "test_merge_duplicates_no_duplicates": "Test merge_duplicates() preserves unique pairs.",
          "test_merge_duplicates_many_to_many_preserved": "Test merge_duplicates() preserves many-to-many relationships.",
          "test_merge_duplicates_exact_duplicates": "Test merge_duplicates() removes exact duplicates.",
          "test_merge_duplicates_max_score": "Test merge_duplicates() keeps max relevance score.",
          "test_merge_duplicates_union_keywords": "Test merge_duplicates() unions keywords.",
          "test_merge_duplicates_longest_answer": "Test merge_duplicates() keeps longest answer."
        }
      },
      "TestCSVMergerIDAssignment": {
        "docstring": "Test ID assignment logic.",
        "methods": {
          "test_assign_ids_unique_questions": "Test _assign_ids() creates unique question IDs.",
          "test_assign_ids_unique_sources": "Test _assign_ids() creates unique source IDs.",
          "test_assign_ids_sequential_pairs": "Test _assign_ids() creates sequential pair IDs."
        }
      },
      "TestCSVMergerIntegration": {
        "docstring": "Test full merge workflow.",
        "methods": {
          "test_merge_single_file": "Test merging a single CSV file.",
          "test_merge_multiple_files": "Test merging multiple CSV files.",
          "test_merge_many_to_many_workflow": "Test complete merge workflow preserves many-to-many relationships.",
          "test_merge_with_exact_duplicates_in_many_to_many": "Test merge removes exact duplicates while preserving many-to-many.",
          "test_merge_with_output_path": "Test merging saves to output path.",
          "test_merge_removes_empty_rows": "Test merging removes rows with empty question or source."
        }
      },
      "TestCSVMergerConvenienceFunction": {
        "docstring": "Test convenience function.",
        "methods": {
          "test_merge_csv_files_function": "Test merge_csv_files() convenience function works."
        }
      },
      "TestCSVMergerInspectionMetadata": {
        "docstring": "Test inspection metadata creation.",
        "methods": {
          "test_create_inspection_metadata": "Test create_inspection_metadata() generates correct structure."
        }
      },
      "TestCSVMergerEdgeCases": {
        "docstring": "Test edge cases and error handling.",
        "methods": {
          "test_merge_empty_file_list": "Test merge raises error for empty file list.",
          "test_merge_nonexistent_file": "Test merge raises error for non-existent file."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "tests/test_mine_negatives.py",
    "file": "test_mine_negatives.py",
    "module": "test_mine_negatives",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for Hard Negative Miner.",
    "classes": {
      "TestNegativeMinerInit": {
        "docstring": "Test NegativeMiner initialization.",
        "methods": {
          "test_init_valid": "Test initialization with valid inputs.",
          "test_init_with_tier_proportions": "Test initialization with custom tier proportions.",
          "test_init_auto_tier_proportions": "Test automatic tier proportion distribution.",
          "test_init_invalid_source_embeddings_type": "Test initialization with invalid source_embeddings type.",
          "test_init_invalid_dimension_mismatch": "Test initialization with dimension mismatch.",
          "test_init_invalid_n_neg": "Test initialization with invalid n_neg.",
          "test_init_invalid_tier_proportions_sum": "Test initialization with tier proportions that don't sum to n_neg.",
          "test_init_invalid_tier_proportions_length": "Test initialization with wrong number of tier proportions.",
          "test_init_cluster_structures_built": "Test that cluster structures are built during initialization."
        }
      },
      "TestNegativeMinerMining": {
        "docstring": "Test negative mining functionality.",
        "methods": {
          "test_mine_all_negatives_shape": "Test that mine_all_negatives returns correct shapes.",
          "test_mine_all_negatives_indices_valid": "Test that all negative indices are valid.",
          "test_mine_all_negatives_tiers_valid": "Test that all tier labels are valid.",
          "test_mine_all_negatives_no_true_source": "Test that true source is never in negatives.",
          "test_mine_all_negatives_tier_distribution": "Test that tier distribution matches proportions.",
          "test_mine_single_pair": "Test mining negatives for single pair.",
          "test_mine_with_small_n_neg": "Test mining with small number of negatives.",
          "test_mine_with_large_n_neg": "Test mining with large number of negatives."
        }
      },
      "TestNegativeMinerEdgeCases": {
        "docstring": "Test edge cases for negative mining.",
        "methods": {
          "test_small_cluster": "Test mining with very small clusters.",
          "test_single_cluster": "Test mining when all sources in single cluster.",
          "test_deterministic_with_seed": "Test that results are deterministic with same seed."
        }
      },
      "TestNegativeMinerSaveLoad": {
        "docstring": "Test saving functionality.",
        "methods": {
          "test_save": "Test saving negatives to disk."
        }
      },
      "TestMineNegativesFunction": {
        "docstring": "Test convenience function.",
        "methods": {
          "test_mine_negatives_function": "Test mine_negatives convenience function.",
          "test_mine_negatives_function_returns_correct_shape": "Test that convenience function returns correct shapes."
        }
      }
    },
    "functions": {
      "sample_data": "Create sample data for testing.\n\nCreates:\n- 100 sources with embeddings\n- 20 questions with embeddings\n- 5 clusters with centroids\n- 50 pairs with cluster assignments"
    }
  },
  {
    "path": "tests/test_monitoring.py",
    "file": "test_monitoring.py",
    "module": "test_monitoring",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for TrainingMonitor.\n\nCovers:\n- log_metrics: history accumulation, W&B payload filtering (bool vs int/float)\n- plot_losses: guard on empty history, guard on no recognised loss keys,\n               successful PNG creation, key detection logic\n- plot_steering_distribution: guard on missing steering keys, PNG creation\n- export_history: CSV + JSON sidecar creation, row count, bug regression\n  (empty history with pandas unavailable must not report false success)\n- num_epochs_logged property",
    "classes": {
      "TestInit": {
        "docstring": null,
        "methods": {
          "test_output_dir_created": null,
          "test_initial_epoch_count_is_zero": null,
          "test_history_initially_empty": null,
          "test_repr_contains_expected_info": null,
          "test_no_wandb_by_default": null
        }
      },
      "TestLogMetrics": {
        "docstring": null,
        "methods": {
          "test_history_grows_with_each_call": null,
          "test_epoch_key_stored_in_record": null,
          "test_all_metric_keys_stored": null,
          "test_bool_excluded_from_wandb_payload": "bool is a subclass of int \u2014 must be excluded from W&B float payload.",
          "test_wandb_failure_does_not_raise": null
        }
      },
      "TestPlotLosses": {
        "docstring": null,
        "methods": {
          "test_returns_none_when_history_empty": null,
          "test_returns_none_when_no_recognised_loss_keys": null,
          "test_creates_png_file": null,
          "test_default_filename_used_when_no_save_path": null,
          "test_detects_both_train_and_val_keys": null,
          "test_only_val_keys_also_works": null,
          "test_handles_none_values_in_history": "None values should be converted to NaN without raising."
        }
      },
      "TestPlotSteeringDistribution": {
        "docstring": null,
        "methods": {
          "test_returns_none_when_no_steering_keys": null,
          "test_returns_none_when_history_empty": null,
          "test_creates_png_when_steering_keys_present": null,
          "test_default_filename_used_when_no_save_path": null
        }
      },
      "TestExportHistory": {
        "docstring": null,
        "methods": {
          "test_csv_and_json_sidecar_created": null,
          "test_csv_has_correct_row_count": null,
          "test_json_sidecar_is_valid_json": null,
          "test_default_path_used_when_not_specified": null,
          "test_empty_history_still_creates_json": "JSON sidecar should always be written (even for empty history).",
          "test_csv_contains_epoch_column": null,
          "test_fallback_csv_without_pandas": "Manual CSV fallback (no pandas) should produce a valid file.",
          "test_empty_history_no_pandas_does_not_create_csv": "Regression test: empty history + no pandas skips CSV but must not false-report success.\n\nThe current implementation skips writing CSV silently.  This test\ndocuments the behaviour so any accidental change is caught."
        }
      },
      "TestGetSummaryTable": {
        "docstring": null,
        "methods": {
          "test_returns_data_matching_history": null,
          "test_empty_history_returns_empty_structure": null
        }
      },
      "TestFinish": {
        "docstring": null,
        "methods": {
          "test_finish_without_wandb_does_not_raise": null,
          "test_finish_closes_wandb_run": null,
          "test_finish_clears_run_even_if_finish_raises": null
        }
      }
    },
    "functions": {
      "_make_metrics": "Return a realistic metrics dict for a single epoch."
    }
  },
  {
    "path": "tests/test_parse_clusters.py",
    "file": "test_parse_clusters.py",
    "module": "test_parse_clusters",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for Cluster Parser.",
    "classes": {
      "TestClusterParserInit": {
        "docstring": "Test ClusterParser initialization.",
        "methods": {
          "test_init_valid_file": "Test initialization with valid JSON file.",
          "test_init_file_not_found": "Test initialization with non-existent file.",
          "test_init_loads_embeddings": "Test initialization loads embeddings if available.",
          "test_init_without_embeddings": "Test initialization without embeddings in JSON.",
          "test_init_custom_cosine_threshold": "Test initialization with custom cosine threshold."
        }
      },
      "TestClusterParserKeywordMatching": {
        "docstring": "Test keyword matching functionality.",
        "methods": {
          "test_match_keyword_exact_match": "Test exact keyword matching.",
          "test_match_keyword_case_insensitive": "Test case-insensitive keyword matching.",
          "test_match_keyword_with_whitespace": "Test keyword matching with extra whitespace.",
          "test_match_keyword_no_match": "Test keyword matching when no match found.",
          "test_match_keyword_all_keywords": "Test matching all keywords in dataset."
        }
      },
      "TestClusterParserBatchMatching": {
        "docstring": "Test batch keyword matching.",
        "methods": {
          "test_match_keywords_batch": "Test batch keyword matching.",
          "test_match_keywords_batch_empty": "Test batch matching with empty list."
        }
      },
      "TestClusterParserFuzzyMatching": {
        "docstring": "Test fuzzy keyword matching.",
        "methods": {
          "test_match_keyword_fuzzy_exact_match": "Test fuzzy matching with exact match.",
          "test_match_keyword_fuzzy_no_match": "Test fuzzy matching with no match.",
          "test_match_keyword_fuzzy_without_similarity": "Test fuzzy matching without returning similarity."
        }
      },
      "TestClusterParserDescriptors": {
        "docstring": "Test cluster descriptor retrieval.",
        "methods": {
          "test_get_cluster_descriptors": "Test retrieving cluster descriptors.",
          "test_get_cluster_descriptors_all_clusters": "Test retrieving descriptors for all clusters."
        }
      },
      "TestClusterParserCentroids": {
        "docstring": "Test centroid retrieval.",
        "methods": {
          "test_get_centroid": "Test retrieving cluster centroid.",
          "test_get_centroid_all_clusters": "Test retrieving all centroids."
        }
      },
      "TestClusterParserClusterKeywords": {
        "docstring": "Test cluster keyword retrieval.",
        "methods": {
          "test_get_cluster_keywords": "Test retrieving keywords for a cluster.",
          "test_get_cluster_keywords_all_clusters": "Test retrieving keywords for all clusters."
        }
      },
      "TestClusterParserUtilities": {
        "docstring": "Test utility methods.",
        "methods": {
          "test_get_all_keywords": "Test retrieving all keywords.",
          "test_get_clusters_for_keywords": "Test getting cluster assignments for keywords.",
          "test_get_clusters_for_keywords_ignore_missing": "Test getting clusters with ignore_missing=False.",
          "test_compute_cluster_coverage": "Test computing cluster coverage statistics.",
          "test_compute_cluster_coverage_empty": "Test coverage with empty keyword list."
        }
      },
      "TestClusterParserValidation": {
        "docstring": "Test validation functionality.",
        "methods": {
          "test_validate_valid_data": "Test validation with valid data.",
          "test_validate_invalid_cluster_id": "Test validation catches invalid cluster IDs."
        }
      },
      "TestClusterParserMetadata": {
        "docstring": "Test metadata retrieval.",
        "methods": {
          "test_get_metadata": "Test retrieving metadata for config.json."
        }
      },
      "TestConvenienceFunction": {
        "docstring": "Test parse_clusters convenience function.",
        "methods": {
          "test_parse_clusters_with_validation": "Test parse_clusters() convenience function with validation.",
          "test_parse_clusters_without_validation": "Test parse_clusters() without validation.",
          "test_parse_clusters_custom_threshold": "Test parse_clusters() with custom threshold."
        }
      },
      "TestClusterParserEdgeCases": {
        "docstring": "Test edge cases and error handling.",
        "methods": {
          "test_normalize_keyword_various_formats": "Test keyword normalization with various formats.",
          "test_empty_keyword_list_batch_match": "Test batch matching with empty list.",
          "test_get_centroid_invalid_cluster": "Test getting centroid for invalid cluster ID.",
          "test_get_cluster_keywords_invalid_cluster": "Test getting keywords for invalid cluster ID."
        }
      }
    },
    "functions": {
      "sample_clustering_json": "Create sample KeywordClusterer JSON format.",
      "clustering_json_file": "Create temporary clustering JSON file."
    }
  },
  {
    "path": "tests/test_question_augmentation_agent.py",
    "file": "test_question_augmentation_agent.py",
    "module": "test_question_augmentation_agent",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for QuestionAugmentationAgent.",
    "classes": {
      "TestQuestionAugmentationAgentInit": {
        "docstring": "Test QuestionAugmentationAgent initialization.",
        "methods": {
          "test_init_with_llm": "Test initialization with a language model.",
          "test_init_with_custom_params": "Test initialization with custom parameters.",
          "test_init_none_llm_raises_error": "Test that None LLM raises ValueError.",
          "test_init_invalid_llm_type_raises_error": "Test that invalid LLM type raises TypeError."
        }
      },
      "TestQuestionAugmentationAgentRephraseWithSource": {
        "docstring": "Test rephrase_question_with_source method.",
        "methods": {
          "test_rephrase_question_with_source_basic": "Test basic question rephrasing with source.",
          "test_rephrase_question_with_source_returns_none_on_error": "Test that None is returned when LLM fails."
        }
      },
      "TestQuestionAugmentationAgentRephraseWithDomain": {
        "docstring": "Test rephrase_question_with_domain method.",
        "methods": {
          "test_rephrase_question_with_domain_basic": "Test basic question rephrasing with domain.",
          "test_rephrase_question_with_domain_returns_none_on_error": "Test that None is returned when LLM fails."
        }
      },
      "TestQuestionAugmentationAgentGenerateAlternatives": {
        "docstring": "Test generate_alternative_questions method.",
        "methods": {
          "test_generate_alternative_questions_basic": "Test basic alternative question generation.",
          "test_generate_alternative_questions_with_allow_vague": "Test alternative question generation with allow_vague parameter.",
          "test_generate_alternative_questions_returns_none_on_error": "Test that None is returned when LLM fails."
        }
      },
      "TestQuestionAugmentationAgentProcessDataFrame": {
        "docstring": "Test process_dataframe_rephrasing method.",
        "methods": {
          "test_process_dataframe_rephrasing_basic": "Test basic dataframe processing for rephrasing."
        }
      },
      "TestQuestionAugmentationAgentProcessDataFrameGeneration": {
        "docstring": "Test process_dataframe_generation method.",
        "methods": {
          "test_process_dataframe_generation_basic": "Test basic dataframe processing for question generation.",
          "test_process_dataframe_generation_with_custom_columns": "Test question generation with custom column names."
        }
      },
      "TestQuestionAugmentationAgentIntegration": {
        "docstring": "Integration tests for QuestionAugmentationAgent.",
        "methods": {
          "test_real_llm_integration": "Test with real LLM (requires API key)."
        }
      }
    },
    "functions": {
      "test_agent_import": "Test that QuestionAugmentationAgent can be imported."
    }
  },
  {
    "path": "tests/test_routing_losses.py",
    "file": "test_routing_losses.py",
    "module": "test_routing_losses",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for routing loss functions.",
    "classes": {
      "TestRoutingLoss": {
        "docstring": null,
        "methods": {
          "test_returns_required_keys": null,
          "test_loss_is_scalar": null,
          "test_loss_is_non_negative": null,
          "test_accuracy_in_range": null,
          "test_accuracy_is_detached": null,
          "test_gradients_flow": null,
          "test_perfect_logits_give_perfect_accuracy": null,
          "test_invalid_label_smoothing_raises": null,
          "test_weight_applied": null
        }
      },
      "TestEntropyRegularization": {
        "docstring": null,
        "methods": {
          "test_returns_required_keys": null,
          "test_loss_is_scalar": null,
          "test_entropy_is_detached": null,
          "test_uniform_weights_give_max_entropy": "Uniform weights should give entropy \u2248 log(K).",
          "test_target_decreases_over_epochs": null,
          "test_target_after_anneal_epochs_stays_at_low": null,
          "test_entropy_high_defaults_to_log_k": null,
          "test_gradients_flow": null,
          "test_invalid_entropy_low_raises": null,
          "test_invalid_anneal_epochs_raises": null
        }
      },
      "TestResidualPenalty": {
        "docstring": null,
        "methods": {
          "test_returns_required_keys": null,
          "test_loss_is_scalar": null,
          "test_zero_loss_below_margin": "If all norms are below margin, penalty should be exactly 0.",
          "test_positive_loss_above_margin": "Large fine vectors should incur a positive penalty.",
          "test_hinge_behaviour": "Only samples exceeding margin should contribute to the loss.",
          "test_norm_mean_is_detached": null,
          "test_gradients_flow": null,
          "test_invalid_margin_raises": null,
          "test_weight_applied": null
        }
      },
      "TestDisentanglementLoss": {
        "docstring": null,
        "methods": {
          "test_returns_required_key": null,
          "test_loss_is_scalar": null,
          "test_correlated_inputs_give_higher_loss": "Identical rows (fully correlated) should give higher loss than random rows.",
          "test_gradients_flow": null,
          "test_single_sample_returns_zero": "Covariance undefined for B=1; should return zero tensor.",
          "test_weight_applied": null
        }
      }
    },
    "functions": {
      "_make_inputs": null
    }
  },
  {
    "path": "tests/test_source_evaluation_agent.py",
    "file": "test_source_evaluation_agent.py",
    "module": "test_source_evaluation_agent",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for SourceEvaluationAgent.",
    "classes": {
      "TestSourceEvaluationAgentInit": {
        "docstring": "Test SourceEvaluationAgent initialization.",
        "methods": {
          "test_init_with_llm": "Test initialization with a language model.",
          "test_init_with_custom_retries": "Test initialization with custom max_retries."
        }
      },
      "TestSourceEvaluationAgentEvaluate": {
        "docstring": "Test evaluate method.",
        "methods": {
          "test_evaluate_basic": "Test basic source evaluation.",
          "test_evaluate_with_error": "Test source evaluation with LLM error."
        }
      },
      "TestSourceEvaluationAgentProcessDataFrame": {
        "docstring": "Test process_dataframe method.",
        "methods": {
          "test_process_dataframe_basic": "Test basic dataframe processing.",
          "test_process_dataframe_with_include_reasoning": "Test dataframe processing with reasoning included.",
          "test_process_dataframe_skip_existing": "Test that already evaluated rows are skipped."
        }
      },
      "TestSourceEvaluationAgentScoreExtraction": {
        "docstring": "Test score extraction from evaluation results.",
        "methods": {
          "test_score_extraction_with_valid_evaluation": "Test extracting scores from valid evaluation.",
          "test_score_validation": "Test that invalid scores are rejected."
        }
      },
      "TestSourceEvaluationAgentIntegration": {
        "docstring": "Integration tests for SourceEvaluationAgent.",
        "methods": {
          "test_real_llm_integration": "Test with real LLM (requires API key)."
        }
      }
    },
    "functions": {
      "test_agent_import": "Test that SourceEvaluationAgent can be imported.",
      "test_pydantic_models_import": "Test that Pydantic models can be imported."
    }
  },
  {
    "path": "tests/test_split.py",
    "file": "test_split.py",
    "module": "test_split",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for Dataset Splitter.",
    "classes": {
      "TestDatasetSplitterInit": {
        "docstring": "Test DatasetSplitter initialization.",
        "methods": {
          "test_init_valid": "Test initialization with valid inputs.",
          "test_init_custom_ratios": "Test initialization with custom ratios.",
          "test_init_invalid_pair_indices_type": "Test initialization with invalid pair indices type.",
          "test_init_invalid_pair_cluster_ids_type": "Test initialization with invalid cluster IDs type.",
          "test_init_invalid_pair_indices_shape": "Test initialization with invalid pair indices shape.",
          "test_init_invalid_pair_cluster_ids_shape": "Test initialization with invalid cluster IDs shape.",
          "test_init_length_mismatch": "Test initialization with mismatched tensor lengths.",
          "test_init_empty_tensors": "Test initialization with empty tensors.",
          "test_init_invalid_train_ratio": "Test initialization with invalid train ratio.",
          "test_init_invalid_val_ratio": "Test initialization with invalid val ratio.",
          "test_init_ratios_not_sum_to_one": "Test initialization with ratios that don't sum to 1."
        }
      },
      "TestDatasetSplitterSplit": {
        "docstring": "Test split functionality.",
        "methods": {
          "test_split_basic": "Test basic split operation.",
          "test_split_covers_all_pairs": "Test that split covers all pairs exactly once.",
          "test_split_no_question_leakage": "Test that no questions appear in multiple splits.",
          "test_split_non_empty_splits": "Test that all splits are non-empty.",
          "test_split_approximate_ratios": "Test that split sizes approximately match ratios.",
          "test_split_deterministic": "Test that split is deterministic with same seed.",
          "test_split_different_seeds": "Test that different seeds produce different splits.",
          "test_split_small_dataset": "Test split on small dataset.",
          "test_split_single_cluster": "Test split when all pairs belong to single cluster."
        }
      },
      "TestSplitDatasetFunction": {
        "docstring": "Test split_dataset convenience function.",
        "methods": {
          "test_split_dataset_creates_files": "Test that split_dataset creates output files.",
          "test_split_dataset_files_loadable": "Test that saved files can be loaded.",
          "test_split_dataset_custom_ratios": "Test split_dataset with custom ratios."
        }
      },
      "TestEdgeCases": {
        "docstring": "Test edge cases and error conditions.",
        "methods": {
          "test_many_sources_per_question": "Test with questions having many sources.",
          "test_unbalanced_cluster_distribution": "Test with highly unbalanced cluster distribution."
        }
      }
    },
    "functions": {
      "sample_data": "Create sample data for testing.\n\nCreates:\n- 100 questions\n- 500 pairs distributed across questions\n- 5 clusters with varying distributions",
      "small_data": "Create small dataset for edge case testing.\n\nCreates:\n- 10 questions\n- 30 pairs\n- 3 clusters"
    }
  },
  {
    "path": "tests/test_subspace_router.py",
    "file": "test_subspace_router.py",
    "module": "test_subspace_router",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for SubspaceRouter.",
    "classes": {
      "TestInit": {
        "docstring": null,
        "methods": {
          "test_from_config_object": null,
          "test_from_dict": null,
          "test_router_mlp_present": null,
          "test_invalid_temperature_raises": null,
          "test_invalid_num_subspaces_raises": null,
          "test_invalid_num_layers_raises": null,
          "test_invalid_activation_raises": null,
          "test_get_model_summary": null,
          "test_repr": null
        }
      },
      "TestForward": {
        "docstring": null,
        "methods": {
          "test_output_shapes": null,
          "test_routing_weights_sum_to_one": null,
          "test_routing_weights_non_negative": null,
          "test_no_nan_in_output": null,
          "test_no_nan_in_training_output": null,
          "test_different_batch_sizes": null,
          "test_training_weights_sum_to_one": null
        }
      },
      "TestGumbel": {
        "docstring": null,
        "methods": {
          "test_training_mode_is_stochastic": "Different Gumbel noise seeds should produce different routing weights.",
          "test_inference_mode_is_deterministic": "Without Gumbel noise, same inputs \u2192 same outputs.",
          "test_hard_gumbel_produces_one_hot": "With gumbel_hard=True each routing row should have a single 1.0."
        }
      },
      "TestRoutingCollapse": {
        "docstring": null,
        "methods": {
          "test_routing_not_always_same_subspace": "Fresh initialisation should not immediately collapse to one subspace."
        }
      },
      "TestGetPrimarySubspace": {
        "docstring": null,
        "methods": {
          "test_output_shapes": null,
          "test_cluster_ids_in_range": null,
          "test_confidence_in_range": null,
          "test_no_grad_on_output": null
        }
      },
      "TestExplain": {
        "docstring": null,
        "methods": {
          "test_required_keys_present": null,
          "test_primary_subspace_in_names": null,
          "test_routing_weights_length": null,
          "test_wrong_cluster_names_length_raises": null,
          "test_single_sample_input": null
        }
      },
      "TestGradients": {
        "docstring": null,
        "methods": {
          "test_gradients_flow_to_router_mlp": null,
          "test_no_gradients_without_training": "get_primary_subspace uses torch.no_grad \u2014 returned tensors have no grad."
        }
      }
    },
    "functions": {
      "_make_router": null,
      "_make_inputs": null
    }
  },
  {
    "path": "tests/test_tensor_utils.py",
    "file": "test_tensor_utils.py",
    "module": "test_tensor_utils",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for tensor_utils module.",
    "classes": {
      "TestLoadTensorArtifact": {
        "docstring": "Tests for load_tensor_artifact function.",
        "methods": {
          "test_load_existing_tensor": "Test loading existing tensor succeeds.",
          "test_load_existing_tensor_path_object": "Test loading with Path object works.",
          "test_load_missing_required_file": "Test loading missing required file raises FileNotFoundError.",
          "test_load_missing_optional_file": "Test loading missing optional file returns None.",
          "test_load_with_weights_only_true": "Test loading with weights_only=True.",
          "test_load_with_weights_only_false": "Test loading with weights_only=False for non-tensor data.",
          "test_shape_validation_correct_2d": "Test shape validation with correct 2D shape.",
          "test_shape_validation_correct_1d": "Test shape validation with correct 1D shape.",
          "test_shape_validation_exact_match": "Test shape validation with exact dimensions.",
          "test_shape_validation_wrong_ndim": "Test shape validation with wrong number of dimensions raises ValueError.",
          "test_shape_validation_wrong_columns": "Test shape validation with wrong column count raises ValueError.",
          "test_shape_validation_wrong_rows": "Test shape validation with wrong row count raises ValueError.",
          "test_no_shape_validation": "Test that no validation is performed when expected_shape is None."
        }
      },
      "TestLoadMultipleTensors": {
        "docstring": "Tests for load_multiple_tensors function.",
        "methods": {
          "test_batch_loading_two_tensors": "Test loading multiple tensors at once.",
          "test_batch_loading_multiple_tensors": "Test loading many tensors efficiently.",
          "test_batch_loading_with_non_tensor": "Test loading mix of tensors and non-tensors.",
          "test_empty_specs_list": "Test loading with empty specs returns empty dict.",
          "test_missing_file_in_batch": "Test missing file in batch raises FileNotFoundError.",
          "test_shape_validation_failure_in_batch": "Test shape validation failure in batch raises ValueError."
        }
      },
      "TestSaveTensorArtifact": {
        "docstring": "Tests for save_tensor_artifact function.",
        "methods": {
          "test_save_valid_tensor_2d": "Test saving valid 2D tensor succeeds.",
          "test_save_valid_tensor_1d": "Test saving valid 1D tensor succeeds.",
          "test_save_creates_directory": "Test saving creates nested directories if needed.",
          "test_save_tensor_with_nan_validation_enabled": "Test saving tensor with NaN raises ValueError when validation enabled.",
          "test_save_tensor_with_inf_validation_enabled": "Test saving tensor with Inf raises ValueError when validation enabled.",
          "test_save_tensor_with_negative_inf": "Test saving tensor with -Inf raises ValueError.",
          "test_save_without_validation_allows_nan": "Test saving with validation disabled allows NaN.",
          "test_save_without_validation_allows_inf": "Test saving with validation disabled allows Inf.",
          "test_save_with_validation_default_enabled": "Test validation is enabled by default.",
          "test_save_overwrites_existing_file": "Test saving overwrites existing file.",
          "test_save_path_object": "Test saving with Path object works.",
          "test_save_int_tensor": "Test saving integer tensor.",
          "test_save_long_tensor": "Test saving long tensor."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "tests/test_text_augmentation_agent.py",
    "file": "test_text_augmentation_agent.py",
    "module": "test_text_augmentation_agent",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for TextAugmentationAgent.",
    "classes": {
      "TestTextAugmentationAgentInit": {
        "docstring": "Test TextAugmentationAgent initialization.",
        "methods": {
          "test_init_with_llm": "Test initialization with a language model.",
          "test_init_with_custom_params": "Test initialization with custom parameters."
        }
      },
      "TestTextAugmentationAgentRephraseText": {
        "docstring": "Test rephrase_text method.",
        "methods": {
          "test_rephrase_text_full_mode": "Test text rephrasing in full mode.",
          "test_rephrase_text_sentence_mode": "Test text rephrasing in sentence mode.",
          "test_rephrase_text_random_mode": "Test text rephrasing in random mode.",
          "test_rephrase_text_returns_none_on_error": "Test that None is returned when LLM fails."
        }
      },
      "TestTextAugmentationAgentAugmentDataFrame": {
        "docstring": "Test augment_dataframe method.",
        "methods": {
          "test_augment_dataframe_questions_only": "Test augmenting only questions in dataframe.",
          "test_augment_dataframe_sources_only": "Test augmenting only sources in dataframe.",
          "test_augment_dataframe_both": "Test augmenting both questions and sources.",
          "test_augment_dataframe_with_probability": "Test augmentation with probability < 1.",
          "test_augment_dataframe_custom_columns": "Test augmentation with custom column names."
        }
      },
      "TestTextAugmentationAgentVerifyMeaning": {
        "docstring": "Test meaning verification functionality.",
        "methods": {
          "test_verify_meaning_preserved": "Test that meaning verification works when enabled."
        }
      },
      "TestTextAugmentationAgentIntegration": {
        "docstring": "Integration tests for TextAugmentationAgent.",
        "methods": {
          "test_real_llm_integration": "Test with real LLM (requires API key)."
        }
      }
    },
    "functions": {
      "test_agent_import": "Test that TextAugmentationAgent can be imported."
    }
  },
  {
    "path": "tests/test_topic_distance_calculator.py",
    "file": "test_topic_distance_calculator.py",
    "module": "test_topic_distance_calculator",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for topic distance calculator utility.",
    "classes": {},
    "functions": {
      "test_topic_distance_calculator_basic": "Test basic functionality of TopicDistanceCalculator.",
      "test_topic_distance_calculator_with_csv": "Test CSV processing with topic distance calculator.",
      "test_compute_distances": "Test distance computation.",
      "test_get_embeddings_batch": "Test batch embedding generation with caching.",
      "test_get_embeddings_batch_no_cache": "Test batch embedding without caching.",
      "test_invalid_metric": "Test that invalid metric raises ValueError.",
      "test_missing_centroids": "Test that missing centroids in JSON raises ValueError.",
      "test_missing_required_columns": "Test that missing required columns in CSV raises ValueError.",
      "test_embedder_none_when_needed": "Test that missing embedder raises ValueError when text embedding is needed.",
      "test_database_none_when_using_ids": "Test that missing database raises ValueError when using ID columns.",
      "test_unsupported_embedder_interface": "Test that embedder with unsupported interface raises ValueError.",
      "test_create_distance_json_mapping": "Test creation of JSON mapping from distances to topic descriptors.",
      "test_create_distance_json_mapping_duplicate_topics": "Test JSON mapping when topic descriptor appears in multiple clusters (should use minimum).",
      "test_empty_text_handling": "Test handling of empty or None text values.",
      "test_embedding_dimension_mismatch": "Test error when embedding dimension doesn't match centroids.",
      "test_database_embedding_not_found": "Test handling when database doesn't return embedding.",
      "test_embedder_wrapping": "Test that non-KeywordEmbedder embedders are automatically wrapped.",
      "test_cache_functionality": "Test embedding cache functionality.",
      "test_cache_disabled": "Test that cache can be disabled.",
      "test_cosine_vs_euclidean_metrics": "Test that different metrics produce different results.",
      "test_distance_json_mapping_invalid_dimensions": "Test error when distance array dimensions don't match centroids.",
      "test_keyboard_interrupt_handling_with_save": "Test that keyboard interrupt saves partial results when enabled.",
      "test_keyboard_interrupt_handling_no_save": "Test that keyboard interrupt doesn't save when disabled.",
      "test_database_interface_chromadb_style": "Test database interface using ChromaDB-style get method."
    }
  },
  {
    "path": "tests/test_validation_utils.py",
    "file": "test_validation_utils.py",
    "module": "test_validation_utils",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for validation_utils module.",
    "classes": {
      "TestValidateTensor2D": {
        "docstring": "Tests for validate_tensor_2d function.",
        "methods": {
          "test_valid_tensor": "Test valid 2D tensor passes validation.",
          "test_valid_tensor_with_column_check": "Test valid 2D tensor with correct column count.",
          "test_invalid_type_list": "Test non-tensor list raises TypeError.",
          "test_invalid_type_numpy": "Test numpy array raises TypeError.",
          "test_wrong_dimensions_1d": "Test 1D tensor raises ValueError.",
          "test_wrong_dimensions_3d": "Test 3D tensor raises ValueError.",
          "test_column_count_mismatch": "Test wrong column count raises ValueError.",
          "test_too_few_rows": "Test insufficient rows raises ValueError.",
          "test_minimum_one_row_default": "Test default minimum 1 row requirement.",
          "test_edge_case_single_row": "Test single row tensor is valid."
        }
      },
      "TestValidateTensor1D": {
        "docstring": "Tests for validate_tensor_1d function.",
        "methods": {
          "test_valid_tensor": "Test valid 1D tensor passes validation.",
          "test_valid_tensor_with_length_check": "Test valid 1D tensor with correct length.",
          "test_invalid_type": "Test non-tensor raises TypeError.",
          "test_wrong_dimensions_2d": "Test 2D tensor raises ValueError.",
          "test_wrong_dimensions_0d": "Test scalar tensor raises ValueError.",
          "test_length_mismatch": "Test wrong length raises ValueError.",
          "test_too_short": "Test tensor shorter than minimum raises ValueError.",
          "test_empty_tensor_fails_default": "Test empty tensor fails with default min_length=1."
        }
      },
      "TestValidateEmbeddingDimensions": {
        "docstring": "Tests for validate_embedding_dimensions function.",
        "methods": {
          "test_consistent_dimensions_two_tensors": "Test two tensors with same dimensions pass.",
          "test_consistent_dimensions_three_tensors": "Test three tensors with same dimensions pass.",
          "test_inconsistent_dimensions_two_tensors": "Test tensors with different dimensions raise ValueError.",
          "test_inconsistent_dimensions_three_tensors": "Test three tensors with inconsistent dimensions raise ValueError.",
          "test_no_tensors_provided": "Test providing no tensors raises ValueError.",
          "test_single_tensor": "Test single tensor returns its dimension.",
          "test_validates_2d_requirement": "Test that non-2D tensors are rejected."
        }
      },
      "TestValidatePairIndicesBounds": {
        "docstring": "Tests for validate_pair_indices_bounds function.",
        "methods": {
          "test_valid_indices": "Test valid indices pass validation.",
          "test_question_index_out_of_bounds_high": "Test out-of-bounds question index raises ValueError.",
          "test_source_index_out_of_bounds_high": "Test out-of-bounds source index raises ValueError.",
          "test_question_index_negative": "Test negative question index is caught by max check.",
          "test_edge_case_max_valid_index": "Test maximum valid indices are accepted.",
          "test_validates_2d_requirement": "Test that non-2D tensor is rejected.",
          "test_validates_column_count": "Test that tensor without exactly 2 columns is rejected.",
          "test_custom_name_in_error": "Test custom name appears in error messages."
        }
      },
      "TestValidateClusterIdsBounds": {
        "docstring": "Tests for validate_cluster_ids_bounds function.",
        "methods": {
          "test_valid_cluster_ids": "Test valid cluster IDs pass validation.",
          "test_cluster_id_out_of_bounds_high": "Test out-of-bounds cluster ID raises ValueError.",
          "test_edge_case_max_valid_cluster": "Test maximum valid cluster ID is accepted.",
          "test_single_cluster": "Test single cluster case.",
          "test_validates_1d_requirement": "Test that non-1D tensor is rejected.",
          "test_custom_name_in_error": "Test custom name appears in error messages."
        }
      },
      "TestValidateLengthConsistency": {
        "docstring": "Tests for validate_length_consistency function.",
        "methods": {
          "test_consistent_tensor_lengths": "Test tensors with consistent lengths pass.",
          "test_consistent_list_length": "Test list with correct length passes.",
          "test_mixed_tensor_and_list": "Test mix of tensors and lists with consistent lengths.",
          "test_tensor_length_mismatch": "Test tensor length mismatch raises ValueError.",
          "test_list_length_mismatch": "Test list length mismatch raises ValueError.",
          "test_invalid_type": "Test invalid type raises TypeError.",
          "test_multiple_mismatches": "Test first mismatch is reported."
        }
      },
      "TestValidateSplitRatios": {
        "docstring": "Tests for validate_split_ratios function.",
        "methods": {
          "test_valid_ratios_standard": "Test valid standard split ratios.",
          "test_valid_ratios_80_10_10": "Test valid 80/10/10 split.",
          "test_train_ratio_too_low": "Test train ratio <= 0 raises ValueError.",
          "test_train_ratio_too_high": "Test train ratio >= 1 raises ValueError.",
          "test_val_ratio_too_low": "Test val ratio <= 0 raises ValueError.",
          "test_test_ratio_too_low": "Test test ratio <= 0 raises ValueError.",
          "test_ratios_sum_too_low": "Test ratios summing to less than 1.0 raises ValueError.",
          "test_ratios_sum_too_high": "Test ratios summing to more than 1.0 raises ValueError.",
          "test_ratios_sum_within_tolerance": "Test ratios that sum to 1.0 within tolerance pass."
        }
      },
      "TestValidateKeywordIdsList": {
        "docstring": "Tests for validate_keyword_ids_list function.",
        "methods": {
          "test_valid_keyword_ids_list": "Test valid list of keyword ID lists passes.",
          "test_valid_with_empty_lists": "Test valid list with some empty keyword lists.",
          "test_not_a_list": "Test non-list raises TypeError.",
          "test_wrong_length": "Test wrong number of pairs raises ValueError.",
          "test_inner_not_list": "Test non-list inner element raises TypeError.",
          "test_keyword_id_not_int": "Test non-integer keyword ID raises TypeError.",
          "test_keyword_id_out_of_bounds_high": "Test keyword ID >= n_keywords raises ValueError.",
          "test_keyword_id_negative": "Test negative keyword ID raises ValueError.",
          "test_custom_name_in_error": "Test custom name appears in error messages."
        }
      },
      "TestValidateNoNanInf": {
        "docstring": "Tests for validate_no_nan_inf function.",
        "methods": {
          "test_valid_tensor_float": "Test valid float tensor passes validation.",
          "test_valid_tensor_int": "Test valid integer tensor passes validation.",
          "test_tensor_with_nan": "Test tensor with NaN raises ValueError.",
          "test_tensor_with_inf": "Test tensor with Inf raises ValueError.",
          "test_tensor_with_negative_inf": "Test tensor with -Inf raises ValueError.",
          "test_tensor_with_both_nan_and_inf": "Test tensor with both NaN and Inf raises ValueError for NaN first.",
          "test_custom_name_in_error_nan": "Test custom name appears in NaN error message.",
          "test_custom_name_in_error_inf": "Test custom name appears in Inf error message.",
          "test_1d_tensor": "Test validation works with 1D tensors.",
          "test_3d_tensor": "Test validation works with 3D tensors."
        }
      },
      "TestValidateValuesInRange": {
        "docstring": "Tests for validate_values_in_range function.",
        "methods": {
          "test_valid_values_inclusive": "Test valid values within inclusive range pass.",
          "test_valid_values_exclusive": "Test valid values within exclusive range pass.",
          "test_edge_values_inclusive": "Test edge values are accepted in inclusive range.",
          "test_edge_values_exclusive_fails": "Test edge values are rejected in exclusive range.",
          "test_value_below_min_inclusive": "Test value below minimum raises ValueError in inclusive range.",
          "test_value_above_max_inclusive": "Test value above maximum raises ValueError in inclusive range.",
          "test_value_at_min_exclusive_fails": "Test value at minimum fails in exclusive range.",
          "test_value_at_max_exclusive_fails": "Test value at maximum fails in exclusive range.",
          "test_integer_range": "Test integer range validation.",
          "test_integer_out_of_range": "Test integer out of range raises ValueError.",
          "test_negative_range": "Test negative value ranges.",
          "test_custom_name_in_error": "Test custom name appears in error messages.",
          "test_single_value_tensor": "Test validation works with single value tensor.",
          "test_multidimensional_tensor": "Test validation works with multidimensional tensors."
        }
      }
    },
    "functions": {}
  },
  {
    "path": "tests/test_xai_interface.py",
    "file": "test_xai_interface.py",
    "module": "test_xai_interface",
    "parent_module": "tests",
    "package": "tests",
    "module_docstring": "Tests for XAIInterface.",
    "classes": {
      "TestInit": {
        "docstring": null,
        "methods": {
          "test_is_decomposed_flag_true_for_decomposed": null,
          "test_is_decomposed_flag_false_for_jasper": null,
          "test_model_in_eval_mode": null,
          "test_wrong_cluster_names_length_raises": null,
          "test_training_pairs_stored": null,
          "test_repr": null
        }
      },
      "TestExplainPrediction": {
        "docstring": null,
        "methods": {
          "test_all_required_keys_present": null,
          "test_primary_subspace_in_cluster_names": null,
          "test_primary_confidence_in_range": null,
          "test_routing_distribution_sums_to_one": null,
          "test_routing_distribution_has_all_clusters": null,
          "test_atypicality_non_negative": null,
          "test_prediction_is_list": null,
          "test_coarse_and_fine_vectors_present_for_decomposed": null,
          "test_coarse_and_fine_none_for_jasper": null,
          "test_works_with_1D_input": "Accepts [D] shaped (non-batched) inputs.",
          "test_works_with_2D_input": "Accepts [1, D] shaped inputs.",
          "test_works_with_jasper_predictor": null,
          "test_actionable_signal_is_string": null,
          "test_similar_pairs_empty_without_training_pairs": null,
          "test_similar_pairs_with_training_data": null
        }
      },
      "TestCompareSteeringInfluence": {
        "docstring": null,
        "methods": {
          "test_returns_correct_list_lengths": null,
          "test_single_steering_has_empty_kl": null,
          "test_custom_labels": null,
          "test_routing_matrices_sum_to_one": null
        }
      },
      "TestVisualization": {
        "docstring": null,
        "methods": {
          "_make_xai_dict": null,
          "test_returns_figure_when_matplotlib_available": null,
          "test_returns_none_when_matplotlib_unavailable": null,
          "test_saves_to_file_when_save_path_given": null
        }
      },
      "TestSaveXAIOutputs": {
        "docstring": null,
        "methods": {
          "test_saves_valid_json": null,
          "test_saved_json_has_required_keys": null,
          "test_saves_multiple_results": null
        }
      }
    },
    "functions": {
      "_make_decomposed_interface": null,
      "_make_jasper_interface": null,
      "_make_query": null
    }
  }
]